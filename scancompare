#!/usr/bin/env python3
# scancompare version 1.0.0

import os
import sys
import json
import subprocess
from datetime import datetime, timezone
from urllib.request import Request, urlopen, urlretrieve
from urllib.parse import urlparse
from pathlib import Path
import webbrowser
import shutil
import tempfile
import html
import base64
import argparse
import re
import time
import random
import uuid
from io import BytesIO
from jinja2 import Environment, FileSystemLoader
import difflib
from typing import Dict, List, Set, Tuple, Optional
import sys
import webbrowser

# Check if 'requests' is installed
try:
    import requests
except ImportError:
    # Use `python3` if available, redirecting both stdout and stderr to DEVNULL
    subprocess.check_call([sys.executable, "-m", "pip", "install", "requests"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, stdin=subprocess.DEVNULL)
    import requests
from urllib.parse import urlparse

SCRIPT_NAME = "scancompare"
SCRIPT_URL = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scancompare"
VERSION = "1.0.0"

# Constants for directory structure
USER_ROOT = os.path.expanduser("~/ScanCompare")
SCAN_REPORTS_DIR = os.path.join(USER_ROOT, "scan_reports")
TEMP_DIR = os.path.join(USER_ROOT, "temp")
BACKUP_DIR = os.path.join(USER_ROOT, "backups")
GITHUB_PAGES_DIR = os.path.join(TEMP_DIR, "gh-pages")
DOCKER_TEMP_DIR = os.path.join(TEMP_DIR, "docker")
LOG_FILE = os.path.join(USER_ROOT, "scancompare.log")

def handle_cli_args():
    parser = argparse.ArgumentParser(
        description=(
            "Scan and compare Docker image vulnerabilities using Trivy and Grype.\n\n"
            "Supports scanning from local Docker images or GitHub repositories containing Dockerfiles.\n"
            "Optional features:\n"
            "  ‚Ä¢ Upload results to GitHub Advanced Security (GHAS)\n"
            "  ‚Ä¢ Publish HTML report to GitHub Pages\n"
        ),
        formatter_class=argparse.RawTextHelpFormatter
    )

    parser.add_argument("image", nargs="?", help="Docker image to scan")
    parser.add_argument("--repo-url", help="GitHub repo URL containing Dockerfile for image build and scan")
    parser.add_argument("--ghas", action="store_true", help="Upload scan results to GitHub Advanced Security (SARIF upload)")
    parser.add_argument("--gh-pages", action="store_true", help="Publish HTML report to GitHub Pages (requires git & gh CLI)")
    parser.add_argument("--keep-data", action="store_true", help="Keep all output and temp data (HTML, SARIF, JSON, image, repo)")
    parser.add_argument("--version", action="store_true", help="Show scancompare version")
    parser.add_argument("--update", action="store_true", help="Check and apply latest scancompare update")
    parser.add_argument("--verbose", action="store_true", help="Show detailed output (clone, build, scans)")
    parser.add_argument("--uninstall", action="store_true", help="Uninstall scancompare CLI")
    parser.add_argument("--auto", action="store_true", help="Automatically answer all prompts")
    parser.add_argument("--mock-yes", action="store_true", help="Automatically answer yes prompts")
    parser.add_argument("--mock-no", action="store_true", help="Automatically answer no prompts")

    args = parser.parse_args()

    automation_flags = [args.auto, args.mock_yes, args.mock_no]
    if sum(flag for flag in automation_flags) > 1:
        print("‚ö†Ô∏è Only one automation flag (--auto, --mock-yes, --mock-no) can be used at a time.")
        sys.exit(1)
        
    return args

def get_automation_mode(args):
    """Determine the automation mode based on CLI args."""
    if args.auto:
        return "auto"
    elif args.mock_yes:
        return "yes"
    elif args.mock_no:
        return "no"
    return None

def tool_progress(ACTION, TOOL_NAME):
    print(f"{ACTION} {TOOL_NAME}...", end=" ")

def tool_done():
    print(" \033[32m‚úî\033[0m")

def ensure_directory_structure():
    """Ensure all required directories exist ‚Äî only create if missing."""
    directories = [
        USER_ROOT,
        SCAN_REPORTS_DIR,
        TEMP_DIR,
        BACKUP_DIR,
        GITHUB_PAGES_DIR,
        DOCKER_TEMP_DIR
    ]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)
            print(f"üìÅ Created directory: {directory}")
    
    if not os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'w') as f:
            f.write(f"ScanCompare Log File created on {datetime.now().isoformat()}\n")
            f.write("=" * 60 + "\n")

def get_current_version():
    return VERSION

def check_latest_version(only_check=False):
    already_updated = os.environ.get("SCANCOMPARE_UPDATED") == "1"
    if already_updated:
        return

    try:
        script_path = get_script_path()
        tmp_path = script_path + ".tmp"

        # Download the latest version of the script
        urlretrieve(SCRIPT_URL, tmp_path)

        with open(tmp_path, 'r') as f:
            new_code = f.read()

        latest_match = re.search(r'VERSION\s*=\s*["\']([\d\.]+)["\']', new_code)
        if not latest_match:
            raise ValueError("Could not determine latest version from downloaded script.")

        latest = latest_match.group(1)
        current = get_current_version()

        os.remove(tmp_path)

        if latest == current:
            if only_check and not already_updated:
                print(f"üì¶ scancompare version {current} (latest)")
            elif not only_check:
                print(f"‚úÖ You are already running the latest version: {current}")
            return

        if only_check:
            print(f"üì¶ scancompare version {current} ‚Äî üîÑ New version available: {current} ‚Üí {latest}")
            return

        print(f"üîÑ Updating scancompare from {current} ‚Üí {latest}...")
        update_script(new_code)

        print("‚ôªÔ∏è Restarting with updated version...")
        os.execve(
            sys.executable,
            [sys.executable, script_path] + sys.argv[1:],
            {**os.environ, "SCANCOMPARE_UPDATED": "1"}
        )

    except Exception as e:
        try:
            os.remove(tmp_path)
        except Exception:
            pass

        if not only_check:
            print(f"‚ùå Update check failed: {e}")

def update_script(new_code):
    try:
        # Resolve real script path, even if run from a wrapper
        script_path = Path(get_script_path()).resolve()
        install_dir = script_path.parent
        template_path = install_dir / "scan_template.html"

        if not os.access(script_path, os.W_OK):
            raise PermissionError(f"Cannot write to {script_path}. Try using sudo or adjusting permissions.")

        # Write new script to temporary file
        with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tmp:
            tmp.write(new_code)
            temp_path = tmp.name

        os.replace(temp_path, script_path)

        # Always fetch latest scan_template.html
        tool_progress("Getting","latest scan_template.html")
        template_url = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html"
        try:
            urlretrieve(template_url, template_path)
            tool_done()  # Complete scan_template.html update
        except Exception as e:
            print(f"‚ùå Failed to update scan_template.html: {e}")

        # Ensure jinja2 is installed after update
        try:
            import jinja2  # noqa: F401
        except ImportError:
            tool_progress("jinja2", "Installing jinja2")
            subprocess.run([sys.executable, "-m", "pip", "install", "--user", "jinja2"], check=True)
            tool_done()  # Complete jinja2 installation

    except Exception as e:
        print(f"‚ùå Critical: failed to update script: {e}")
        sys.exit(1)

def get_script_path():
    try:
        return os.path.realpath(__file__)
    except NameError:
        return sys.argv[0]

from pathlib import Path
import shutil
import sys

# Define all relevant paths (must match installer layout)
USER_ROOT = Path.home() / "ScanCompare"
MAIN_DIR = USER_ROOT / "main"
INSTALL_BIN = MAIN_DIR / "bin"
INSTALL_LIB = MAIN_DIR / "lib"
SCAN_REPORTS_DIR = USER_ROOT / "scan_reports"
TEMP_DIR = USER_ROOT / "temp"
BACKUP_DIR = USER_ROOT / "backups"
LOG_FILE = USER_ROOT / "scancompare.log"

def uninstall_scancompare():
    print("üßπ Uninstalling scancompare...")

    paths_removed = False
    script_path = INSTALL_LIB / "scancompare"
    wrapper_path = Path.home() / ".local" / "bin" / "scancompare"
    venv_path = INSTALL_LIB / "venv"
    template_path = INSTALL_LIB / "scan_template.html"

    # Removing script, wrapper, and template
    for path in [script_path, wrapper_path, template_path]:
        if path.exists():
            print(f"üóëÔ∏è  Removing {path}...", end=" ")
            try:
                path.unlink()
                print("‚úî")
                paths_removed = True
            except Exception as e:
                print(f"‚ùå Failed to remove {path}: {e}")

    
    if wrapper_path.is_symlink():
        try:
            wrapper_path.unlink()
            print("üîó Removed symlink from ~/.local/bin ‚úî")
            paths_removed = True
        except Exception as e:
            print(f"‚ùå Failed to remove symlink: {e}")

    
    if venv_path.exists() and venv_path.is_dir():
        print("üßπ Removing virtual environment...", end=" ")
        try:
            shutil.rmtree(venv_path)
            print("‚úî")
            paths_removed = True
        except Exception as e:
            print(f"‚ùå Failed to remove virtual environment: {e}")

    
    for path in [SCAN_REPORTS_DIR, TEMP_DIR, BACKUP_DIR]:
        if path.exists() and path.is_dir():
            print(f"üóëÔ∏è  Removing {path.name} directory...", end=" ")
            try:
                shutil.rmtree(path)
                print("‚úî")
                paths_removed = True
            except Exception as e:
                print(f"‚ùå Failed to remove {path.name}: {e}")

    
    if LOG_FILE.exists():
        print("üóëÔ∏è  Removing log file...", end=" ")
        try:
            LOG_FILE.unlink()
            print("‚úî")
            paths_removed = True
        except Exception as e:
            print(f"‚ùå Failed to remove log file: {e}")

    
    profile_files = [".zshrc", ".bashrc", ".profile", ".bash_profile", ".zprofile"]
    for profile in profile_files:
        full_path = Path.home() / profile
        if full_path.exists():
            print(f"üßΩ Cleaning up {profile}...", end=" ")
            try:
                content = full_path.read_text()
                new_content = "\n".join(
                    line for line in content.splitlines()
                    if 'export PATH="$HOME/.local/bin:$PATH"' not in line and 'source "$HOME/.config/scancompare/env.shexport"' not in line
                )
                full_path.write_text(new_content)
                print("‚úî")
            except Exception as e:
                print(f"‚ùå Failed to clean up {profile}: {e}")

    
    for path in [INSTALL_BIN, INSTALL_LIB, MAIN_DIR]:
        if path.exists() and path.is_dir():
            try:
                if not any(path.iterdir()):
                    path.rmdir()
                    print(f"üóëÔ∏è  Removing empty {path.name} directory... ‚úî")
            except Exception as e:
                print(f"‚ùå Failed to remove {path.name} directory: {e}")

    
    if USER_ROOT.exists() and USER_ROOT.is_dir():
        try:
            if not any(USER_ROOT.iterdir()):
                USER_ROOT.rmdir()
                print("üóëÔ∏è  Removing empty ScanCompare directory... ‚úî")
        except Exception as e:
            print(f"‚ùå Failed to remove ScanCompare directory: {e}")

    if paths_removed:
        print("\n‚úÖ scancompare successfully uninstalled.")
        print("‚ÑπÔ∏è  You may need to restart your terminal for changes to take effect.")
    else:
        print("\n‚ÑπÔ∏è  scancompare was not found or already uninstalled.")

    sys.exit(0)

def get_version(tool):
    try:
        result = subprocess.run([tool, "version"], capture_output=True, text=True)

        for line in result.stdout.splitlines():
            if tool.lower() == "grype":
                if line.strip().startswith("Version:"):
                    return line.split("Version:")[-1].strip()
            else:
                if tool.lower() in line.lower() or "Version" in line:
                    return line.strip()
    except FileNotFoundError:
        print(f"‚ö†Ô∏è {tool} is not installed or not in PATH.")
        return "not installed"
    except Exception as e:
        print(f"‚ö†Ô∏è Error getting version for {tool}: {e}")
        return "unknown"

    return "unknown"

def run_scan(tool, image, output_path, verbose=False):
    scan_command = {
        "trivy": ["trivy", "image", "--format", "json", "-o", str(output_path), image],
        "grype": ["grype", image, "-o", "json", "--file", str(output_path)],
    }.get(tool)

    if not scan_command:
        print(f"‚ùå Unknown scanner: {tool}")
        return False

    try:
        # Ensure the directory exists for the output path
        output_dir = os.path.dirname(output_path)
        os.makedirs(output_dir, exist_ok=True)
        
        if verbose:
            # For verbose mode, run without capturing output to show it in real-time
            result = subprocess.run(scan_command, check=True)
        else:
            # For normal mode, capture output but don't show it
            result = subprocess.run(scan_command, check=True, capture_output=True, text=True)

        # Check if the output file was created and has content
        output_path = Path(output_path) if not isinstance(output_path, Path) else output_path
        if not output_path.exists():
            print(f"\n‚ö†Ô∏è {tool} scan failed: output file not created")
            print(f"   Command: {' '.join(scan_command)}")
            return False

        # Read and check the output file content
        try:
            with open(output_path, 'r') as f:
                content = f.read()
                if not content.strip():
                    print(f"\n‚ö†Ô∏è {tool} scan completed but output file is empty")
                    print(f"   Command: {' '.join(scan_command)}")
                    return False
                data = json.loads(content)
                if tool == "trivy" and not data.get("Results"):
                    print(f"\n‚ö†Ô∏è Trivy scan completed but found no results")
                    print(f"   Command: {' '.join(scan_command)}")
                    return False
                elif tool == "grype" and not data:
                    print(f"\n‚ö†Ô∏è Grype scan completed but found no results")
                    print(f"   Command: {' '.join(scan_command)}")
                    return False
        except json.JSONDecodeError:
            print(f"\n‚ö†Ô∏è {tool} scan output is not valid JSON")
            print(f"   Command: {' '.join(scan_command)}")
            return False

        return True
    except subprocess.CalledProcessError as e:
        print(f"\n‚ö†Ô∏è {tool} scan failed with error:")
        if verbose:
            print(f"   {e.stderr if e.stderr else str(e)}")
        else:
            print(f"   {e.stderr if e.stderr else str(e)}")
        print(f"\n   To debug, run: {' '.join(scan_command)}")
        return False
    
def normalize_cve_id(cve_id: str) -> str:
    """Normalize CVE ID to a standard format."""
    if not cve_id:
        return ""

    # Remove any whitespace and convert to uppercase
    cve_id = cve_id.strip().upper()

    # Handle common variations
    if not cve_id.startswith('CVE-'):
        if cve_id.startswith('TEMP-'):
            return cve_id  # Keep temporary IDs as is
        if cve_id.startswith('GHSA-'):
            return cve_id  # Keep GitHub Security Advisories as is
        cve_id = f"CVE-{cve_id}"

    # Ensure proper format: CVE-YYYY-NNNN
    parts = cve_id.split('-')
    if len(parts) == 3:
        year, number = parts[1], parts[2]
        # Validate year format
        if not year.isdigit() or len(year) != 4:
            return cve_id
        # Pad number with leading zeros to 4 digits
        number = number.zfill(4)
        return f"CVE-{year}-{number}"

    return cve_id

def calculate_description_similarity(desc1: str, desc2: str) -> float:
    """Calculate similarity between two CVE descriptions using difflib."""
    if not desc1 or not desc2:
        return 0.0

    # Convert to lowercase and remove common words
    desc1 = desc1.lower()
    desc2 = desc2.lower()

    # Remove common words and punctuation
    common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    desc1 = ' '.join(word for word in desc1.split() if word not in common_words)
    desc2 = ' '.join(word for word in desc2.split() if word not in common_words)

    # Calculate similarity using difflib
    matcher = difflib.SequenceMatcher(None, desc1, desc2)
    return matcher.ratio()

def find_similar_cves(cve_id: str, description: str, cve_map: Dict[str, Dict]) -> List[Tuple[str, float]]:
    """Find similar CVEs based on ID and description."""
    similar_cves = []
    normalized_id = normalize_cve_id(cve_id)

    if not normalized_id:
        return similar_cves

    for other_id, other_data in cve_map.items():
        other_normalized = normalize_cve_id(other_id)

        # Skip if normalization failed
        if not other_normalized:
            continue

        # Check if IDs are similar
        if normalized_id == other_normalized:
            similar_cves.append((other_id, 1.0))
            continue

        # If IDs are different but descriptions are similar
        if description and other_data.get('description'):
            similarity = calculate_description_similarity(description, other_data['description'])
            if similarity > 0.8:  # High similarity threshold
                similar_cves.append((other_id, similarity))

    # Sort by similarity score in descending order
    similar_cves.sort(key=lambda x: x[1], reverse=True)
    return similar_cves

def extract_cves_with_severity(file_path, verbose=False):
    """Extract CVEs with severity information from scan results."""
    try:
        # Convert string path to Path object if it's not already
        if not isinstance(file_path, Path):
            file_path = Path(file_path)
            
        with open(file_path) as f:
            data = json.load(f)
            if verbose:
                print(f"\nüîç Checking {file_path.name} contents:")
                print(f"   File size: {os.path.getsize(file_path)} bytes")
                if isinstance(data, dict):
                    print(f"   Keys: {list(data.keys())}")
                elif isinstance(data, list):
                    print(f"   Number of items: {len(data)}")

        severity_map = {}
        if "Results" in data:  # Trivy format
            for result in data.get("Results", []):
                if verbose:
                    print(f"   Found Trivy result type: {result.get('Type')}")
                for vuln in result.get("Vulnerabilities", []):
                    cve = vuln.get("VulnerabilityID")
                    severity = vuln.get("Severity", "UNKNOWN").capitalize()
                    if cve:
                        normalized_cve = normalize_cve_id(cve)
                        severity_map[normalized_cve] = {
                            "severity": severity,
                            "type": result.get("Type", "unknown"),
                            "cvss": vuln.get("CVSS", {}).get("nvd", {}).get("V3Score", "N/A"),
                            "description": vuln.get("Description", ""),
                            "original_id": cve,  # Keep original ID for reference
                            "references": vuln.get("References", []),
                            "published": vuln.get("PublishedDate"),
                            "last_modified": vuln.get("LastModifiedDate")
                        }
        elif isinstance(data, dict) and "matches" in data:  # Grype format
            matches = data.get("matches", [])
            if verbose:
                print(f"   Found {len(matches)} matches in Grype output")
            for match in matches:
                if not isinstance(match, dict):
                    continue
                vuln = match.get("vulnerability", {})
                cve = vuln.get("id")
                severity = vuln.get("severity", "UNKNOWN").capitalize()
                if cve:
                    normalized_cve = normalize_cve_id(cve)
                    # Get CVSS score from the first CVSS entry if available
                    cvss_score = "N/A"
                    if vuln.get("cvss") and len(vuln["cvss"]) > 0:
                        cvss_score = vuln["cvss"][0].get("score", "N/A")

                    severity_map[normalized_cve] = {
                        "severity": severity,
                        "type": match.get("artifact", {}).get("name", "unknown"),
                        "cvss": cvss_score,
                        "description": vuln.get("description", ""),
                        "original_id": cve,  # Keep original ID for reference
                        "references": [vuln.get("dataSource", "")] if vuln.get("dataSource") else [],
                        "published": None,  # Grype doesn't provide this
                        "last_modified": None  # Grype doesn't provide this
                    }

        if verbose:
            print(f"   Found {len(severity_map)} vulnerabilities")
        return severity_map
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to parse {file_path}: {e}")
        return {}

def group_by_severity(cve_map):
    grouped = {}
    for cve_id, meta in cve_map.items():
        if isinstance(meta, dict):
            severity = meta.get("severity", "Unknown")
        else:
            severity = meta or "Unknown"
        grouped.setdefault(severity, []).append(cve_id)
    return grouped

def print_cves_by_severity(title, cve_map):
    """Print CVEs grouped by severity with detailed information."""
    if not cve_map:
        print(f"\nüî∏ {title}")
        print("   No vulnerabilities found.")
        return

    print(f"\nüî∏ {title}")  # Add newline before each section
    grouped = group_by_severity(cve_map)
    for severity in sorted(grouped.keys(), key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize()) if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"] else 99):
        print(f"  {severity} ({len(grouped[severity])}):")
        for cve in sorted(grouped[severity]):
            cve_info = cve_map.get(cve, {})
            if isinstance(cve_info, dict):
                # Format CVSS score
                cvss = cve_info.get('cvss', 'N/A')
                if cvss != 'N/A':
                    cvss = f"{float(cvss):.1f}"

                # Format last modified date if available
                last_modified = cve_info.get('last_modified')
                if last_modified:
                    try:
                        date = datetime.fromisoformat(last_modified.replace('Z', '+00:00'))
                        last_modified = date.strftime('%Y-%m-%d')
                    except ValueError:
                        pass

                # Build the output line
                line = f"    - {cve}"
                if cvss != 'N/A':
                    line += f" (CVSS: {cvss})"
                if last_modified:
                    line += f" [Last Modified: {last_modified}]"
                print(line)
            else:
                print(f"    - {cve}")
        print()  # Add newline after each severity group

def display_summary(trivy_cves, grype_cves, image=None, trivy_data=None, grype_data=None):
    """Display a summary of vulnerability findings from both scanners."""
    # Convert to sets of CVE IDs for comparison
    trivy_cve_ids = set(trivy_cves.keys())
    grype_cve_ids = set(grype_cves.keys())

    # Find exact matches
    shared = trivy_cve_ids & grype_cve_ids

    # Find similar CVEs using fuzzy matching
    similar_cves = []
    for cve_id in trivy_cve_ids:
        if cve_id not in shared:
            trivy_data_item = trivy_cves[cve_id]
            similar = find_similar_cves(cve_id, trivy_data_item.get('description', ''), grype_cves)
            if similar:
                similar_cves.extend(similar)

    # Add similar CVEs to shared set
    for similar_id, confidence in similar_cves:
        if similar_id not in shared:
            shared.add(similar_id)
            print(f"\nüîç Found similar CVE: {similar_id} (confidence: {confidence:.2f})")

    only_trivy = trivy_cve_ids - shared
    only_grype = grype_cve_ids - shared
    
    # Always print summary before checking conditions
    print("\nüìä Summary Report")
    print("----------------")
    print(" Tool  | Total | Only in Tool | Shared")
    print("-------|-------|--------------|--------")
    print(f"Grype  | {len(grype_cve_ids):<5} | {len(only_grype):<12} | {len(shared)}")
    print(f"Trivy  | {len(trivy_cve_ids):<5} | {len(only_trivy):<12} | {len(shared)}\n")

    # Check if both scans produced zero results
    if not trivy_cve_ids and not grype_cve_ids:
        print(f"\n‚ö†Ô∏è No vulnerabilities found in {image if image else 'the image'} by either scanner.")
        print("\nüìã Analysis:")
        print("   - Both scanners completed successfully")
        
        # Get actual package counts if data is available
        trivy_packages_count = 0
        grype_artifacts_count = 0
        os_info = "unknown base"
        
        if trivy_data:
            # Try to extract package counts from Trivy data
            for result in trivy_data.get("Results", []):
                if "Packages" in result:
                    trivy_packages_count += len(result.get("Packages", []))
                # Try to get OS info
                if result.get("Type") == "os" and result.get("Target"):
                    os_info = result.get("Target")
        
        if grype_data:
            # Try to extract artifact counts from Grype data
            if isinstance(grype_data, dict) and "matches" in grype_data:
                artifacts = set()
                for match in grype_data.get("matches", []):
                    if match.get("artifact", {}).get("name"):
                        artifacts.add(match.get("artifact", {}).get("name"))
                grype_artifacts_count = len(artifacts)
        
        print(f"   - Trivy analyzed {trivy_packages_count} packages" if trivy_packages_count else "   - Trivy completed analysis")
        print(f"   - Grype identified {grype_artifacts_count} artifacts" if grype_artifacts_count else "   - Grype completed analysis")
        
        print(f"\nüîç Why no vulnerabilities were found in {os_info} image:")
        print("   - This may be a minimal or hardened container image")
        print("   - All packages may be up-to-date with no known vulnerabilities")
        print("   - The image may have limited installed packages")
        
        print("\nüí° Recommendations:")
        print("   1. For more meaningful results, try scanning:")
        print("      - A more complex application image (e.g., wordpress:latest)")
        print("      - An image with known vulnerabilities (e.g., node:14)")
        print("      - A more complex Dockerfile that installs additional packages")
        
        print(f"\n   2. To scan with more detail, try verbose mode:")
        print(f"      scancompare {image if image else '<image-name>'} --verbose")
        
        print("\n‚ÑπÔ∏è  To scan a different image, run:")
        print("   scancompare nginx:latest")
        print("   # or")
        print("   scancompare --repo-url https://github.com/your-repo/your-dockerfile")
        sys.exit(0)  # Exit gracefully with success code

    # Check if one scanner found vulnerabilities and the other didn't
    if (trivy_cve_ids and not grype_cve_ids) or (not trivy_cve_ids and grype_cve_ids):
        print(f"\n‚ö†Ô∏è Scanner Discrepancy Detected for {image if image else 'the image'}")
        print("\nüìã Analysis:")
        if trivy_cve_ids:
            print(f"   - Trivy found {len(trivy_cve_ids)} vulnerabilities")
            print("   - Grype found no vulnerabilities")
            
            # Get severity distribution for Trivy findings
            severity_counts = get_severity_distribution(trivy_cves)
            severity_info = ", ".join([f"{count} {sev}" for sev, count in severity_counts.items()])
            if severity_info:
                print(f"   - Severity distribution: {severity_info}")
        else:
            print("   - Trivy found no vulnerabilities")
            print(f"   - Grype found {len(grype_cve_ids)} vulnerabilities")
            
            # Get severity distribution for Grype findings
            severity_counts = get_severity_distribution(grype_cves)
            severity_info = ", ".join([f"{count} {sev}" for sev, count in severity_counts.items()])
            if severity_info:
                print(f"   - Severity distribution: {severity_info}")

        # Try to determine image type for more specific advice
        image_type = "container"
        if image:
            image_type = determine_image_type(image)

        print(f"\nüîç Possible Reasons for discrepancy with {image_type} image:")
        print("   - Different vulnerability databases with varying update frequencies")
        print("   - Different scanning depths and detection methodologies")
        print(f"   - Different package recognition mechanisms for {image_type} images")
        print("   - One scanner may have false positives or the other false negatives")
        
        print("\nüí° Recommendations:")
        print(f"   1. Review the detailed scan results in the HTML report for {image if image else 'this image'}")
        print(f"   2. Run the scan with --verbose flag for more details:")
        print(f"      scancompare {image if image else '<image-name>'} --verbose")
        print("   3. Consider using both scanners for comprehensive coverage")
        print(f"   4. For {image_type} images, check scanner documentation for known limitations")
    
    # When both scanners find vulnerabilities (which seems to be your case)
    elif trivy_cve_ids and grype_cve_ids:
        # Add a new section for the case where both scanners find vulnerabilities
        image_type = "container"
        if image:
            image_type = determine_image_type(image)
        
        trivy_severity = get_severity_distribution(trivy_cves)
        grype_severity = get_severity_distribution(grype_cves)
        
        print(f"\nüìã Vulnerability Analysis for {image if image else 'the image'} ({image_type}):")
        
        # Print combined severity information
        if trivy_severity or grype_severity:
            print("\nüî¥ Severity Breakdown:")
            for severity in ["Critical", "High", "Medium", "Low", "Unknown"]:
                trivy_count = trivy_severity.get(severity, 0)
                grype_count = grype_severity.get(severity, 0)
                max_count = max(trivy_count, grype_count)
                
                if max_count > 0:
                    print(f"   - {severity}: {max_count} vulnerabilities")
        
        # Print agreement percentage
        agreement = (len(shared) / len(trivy_cve_ids | grype_cve_ids)) * 100
        print(f"\nü§ù Scanner Agreement: {agreement:.1f}% ({len(shared)} shared findings)")
        
        # Extract package info if available
        if trivy_data:
            os_info = "unknown base"
            pkg_types = set()
            for result in trivy_data.get("Results", []):
                if result.get("Type"):
                    pkg_types.add(result.get("Type"))
                if result.get("Type") == "os" and result.get("Target"):
                    os_info = result.get("Target")
            
            if os_info != "unknown base":
                print(f"\nüì¶ Image OS: {os_info}")
            
            if pkg_types:
                print(f"üì¶ Found packages: {', '.join(pkg_types)}")

    # Create dictionaries for shared CVEs with metadata from both scanners
    shared_cves = {}
    for cve in shared:
        trivy_data_item = trivy_cves.get(cve, {})
        grype_data_item = grype_cves.get(cve, {})

        # Calculate confidence score based on metadata matching
        confidence = 1.0
        if trivy_data_item.get('description') and grype_data_item.get('description'):
            confidence = calculate_description_similarity(
                trivy_data_item['description'],
                grype_data_item['description']
            )

        # Prefer Trivy's severity if available, otherwise use Grype's
        severity = trivy_data_item.get("severity", grype_data_item.get("severity", "Unknown"))
        # Prefer Trivy's CVSS score if available, otherwise use Grype's
        cvss = trivy_data_item.get("cvss", grype_data_item.get("cvss", "N/A"))
        # Combine package types
        types = set()
        if trivy_data_item.get("type"):
            types.add(trivy_data_item["type"])
        if grype_data_item.get("type"):
            types.add(grype_data_item["type"])

        # Combine references and remove duplicates
        references = set()
        if trivy_data_item.get("references"):
            references.update(trivy_data_item["references"])
        if grype_data_item.get("references"):
            references.update(grype_data_item["references"])

        # Use the most recent last_modified date
        last_modified = max(
            filter(None, [
                trivy_data_item.get("last_modified"),
                grype_data_item.get("last_modified")
            ]),
            default=None
        )

        shared_cves[cve] = {
            "severity": severity,
            "type": ", ".join(types) if types else "unknown",
            "cvss": cvss,
            "confidence": confidence,
            "trivy_id": trivy_data_item.get("original_id", cve),
            "grype_id": grype_data_item.get("original_id", cve),
            "references": sorted(list(references)),
            "last_modified": last_modified
        }

    # Create dictionaries for unique CVEs
    only_trivy_cves = {cve: trivy_cves[cve] for cve in only_trivy}
    only_grype_cves = {cve: grype_cves[cve] for cve in only_grype}

    return shared_cves, only_trivy_cves, only_grype_cves

def get_severity_distribution(cves_dict):
    """Get distribution of severities from a CVE dictionary."""
    severity_counts = {}
    for cve_id, data in cves_dict.items():
        if isinstance(data, dict) and "severity" in data:
            severity = data["severity"]
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
    
    # Sort by severity importance
    severity_order = ["Critical", "High", "Medium", "Low", "Unknown"]
    return {sev: severity_counts.get(sev, 0) for sev in severity_order if sev in severity_counts}

def determine_image_type(image):
    """Determine the likely type of image from its name."""
    image_name = image.lower()
    
    if ":" in image_name:
        base_image = image_name.split(":")[0]
    else:
        base_image = image_name
    
    # Common base images
    if any(name in base_image for name in ["nginx", "apache", "httpd"]):
        return "web server"
    elif any(name in base_image for name in ["mysql", "postgres", "mariadb", "mongo", "redis"]):
        return "database"
    elif any(name in base_image for name in ["node", "python", "ruby", "php", "java", "golang"]):
        return "language runtime"
    elif any(name in base_image for name in ["ubuntu", "debian", "alpine", "centos", "fedora"]):
        return "operating system"
    elif any(name in base_image for name in ["wordpress", "drupal", "joomla"]):
        return "CMS"
    else:
        return "application"

def get_upstream_url(repo_url):
    """
    Get the upstream URL of a GitHub repository. 
    If the repository is a fork, return the URL of the parent repository.
    """
    # Parse the GitHub repository URL to get the user and repo
    parsed = urlparse(repo_url)
    repo_path = parsed.path.strip("/")
    if repo_path.endswith(".git"):
        repo_path = repo_path[:-4]
    
    # Get the user and repo from the parsed URL
    user, repo = repo_path.split("/")[-2:]

    # Use GitHub API to fetch repo details
    api_url = f"https://api.github.com/repos/{user}/{repo}"
    response = requests.get(api_url)

    if response.status_code != 200:
        print("‚ö†Ô∏è Failed to fetch repository details from GitHub.")
        return None

    repo_data = response.json()

    # If it's a fork, get the parent repo (upstream)
    if repo_data.get("fork", False):
        upstream_user = repo_data["parent"]["owner"]["login"]
        upstream_repo = repo_data["parent"]["name"]
        upstream_url = f"https://github.com/{upstream_user}/{upstream_repo}"
        return upstream_url

    # If it's not a fork, return the original repo URL
    return f"https://github.com/{user}/{repo}"

def get_repo_from_url(repo_url):
    match = re.match(r"https?://github.com/([^/]+/[^/]+)(?:\\.git)?", repo_url)
    if match:
        return match.group(1)
    print("‚ùå Invalid GitHub repository URL format.")
    sys.exit(1)

def ensure_gh_authenticated():
    automation_mode = get_automation_mode(args)
    
    try:
        # Check if already authenticated
        subprocess.run(["gh", "auth", "status", "--hostname", "github.com"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # Confirm token can access the user
        result = subprocess.run(["gh", "api", "user"], capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError("Authenticated, but cannot access user profile.")

        # Try a harmless call that requires correct scopes
        probe = subprocess.run(
            ["gh", "api", "repos/octocat/Hello-World/code-scanning/alerts"],
            capture_output=True,
            text=True
        )

        if probe.returncode == 403 or "insufficient" in probe.stderr.lower():
            if automation_mode in ["auto", "yes"]:
                print("\nüîê GitHub token needs additional permissions but in automation mode.")
                print("   Skipping token refresh in automated mode.")
                return
                
            print("\nüîê GitHub token needs additional permissions.")
            print("   Attempting to refresh token with required scopes...")
            try:
                subprocess.run([
                    "gh", "auth", "refresh",
                    "-h", "github.com",
                    "-s", "security_events,admin:repo_hook"
                ], check=True)
                print("‚úÖ Token refreshed successfully.")
                return
            except subprocess.CalledProcessError:
                if automation_mode in ["auto", "yes"]:
                    print("\n‚ö†Ô∏è Token refresh failed. Skipping GitHub operations in automation mode.")
                    return
                    
                print("\n‚ö†Ô∏è Token refresh failed. Please re-authenticate with GitHub.")
                print("   This will open your browser to complete the process.")
                subprocess.run([
                    "gh", "auth", "login",
                    "-h", "github.com",
                    "-s", "security_events,admin:repo_hook",
                    "--web"
                ], check=True)
                return

    except subprocess.CalledProcessError:
        if automation_mode in ["auto", "yes"]:
            print("\n‚ö†Ô∏è GitHub authentication required for auto mode but not available.")
            print("   Skipping GitHub operations in automated mode.")
            return
            
        print("\nüîê GitHub authentication required.")
        print("   This will open your browser to complete the process.")
        subprocess.run([
            "gh", "auth", "login",
            "-h", "github.com",
            "-s", "security_events,admin:repo_hook",
            "--web"
        ], check=True)
        return

    except Exception as e:
        print(f"‚ùå Failed to validate GitHub auth: {e}")
        if automation_mode in ["auto", "yes"]:
            print("   Skipping GitHub operations in automated mode.")
            return
        sys.exit(1)

def resolve_fork(repo, action_label="upload", remember_choice=True, cache_key="fork-choice"):
    """
    Handles forked GitHub repos and lets the user choose upstream or fork.
    Preserves fork choice temporarily for multi-step flows, unless repo context changes.
    """
    import os
    import json
    from urllib.parse import urlparse

    os.makedirs(TEMP_DIR, exist_ok=True)
    CHOICE_FILE = os.path.join(TEMP_DIR, f"{cache_key}.json")

    repo_url = f"https://github.com/{repo}"
    upstream_url = get_upstream_url(repo_url)

    # If upstream URL is missing or the same, skip fork logic entirely
    if not upstream_url or upstream_url == repo_url:
        if os.path.exists(CHOICE_FILE):
            try:
                os.remove(CHOICE_FILE)
            except Exception:
                pass
        return repo_url, get_repo_from_url(repo_url)

    # Load previous choice if it exists
    previous_choice = None
    if os.path.exists(CHOICE_FILE):
        try:
            with open(CHOICE_FILE, "r") as f:
                data = json.load(f)
                previous_choice = data.get("repo_url")
        except Exception:
            previous_choice = None

    if isinstance(previous_choice, str) and previous_choice.startswith("https://github.com/"):
        print(f"\nüìå Using previously selected repository for {action_label}: {previous_choice}")
        return previous_choice, get_repo_from_url(previous_choice)
    else:
        if args.verbose:
            print(f"‚ö†Ô∏è Ignoring invalid cached fork choice: {previous_choice}")
        try:
            os.remove(CHOICE_FILE)
        except Exception:
            pass

    automation_mode = get_automation_mode(args)

    if automation_mode == "auto" or automation_mode == "yes":
        print(f"\n‚ö†Ô∏è The repository you selected appears to be a fork:\n")
        print(f"  Forked repo:\n    {repo_url}")
        print(f"\n  Upstream repo:\n    {upstream_url}")
        print(f"\n‚ÑπÔ∏è Automatically selecting upstream repository in {automation_mode} mode.\n")
        final_url = upstream_url
    elif automation_mode == "no":
        print(f"\n‚ö†Ô∏è The repository you selected appears to be a fork:\n")
        print(f"  Forked repo:\n    {repo_url}")
        print(f"\n  Upstream repo:\n    {upstream_url}")
        print(f"\n‚ÑπÔ∏è Automatically selecting forked repository in no-automation mode.\n")
        final_url = repo_url
    else:
        print(f"\n‚ö†Ô∏è The repository you selected appears to be a fork:\n")
        print(f"  Forked repo:\n    {repo_url}")
        print(f"\n  Upstream repo:\n    {upstream_url}")
        choice = input(f"\nDo you want to {action_label} to the upstream repository instead? (y/n): ").strip().lower()

        if choice == "y":
            print(f"üîÑ Changing to upstream repository URL:\n{upstream_url}")
            final_url = upstream_url
        else:
            print(f"üîÑ Keeping the forked repository URL:\n{repo_url}")
            final_url = repo_url

    if remember_choice:
        try:
            with open(CHOICE_FILE, "w") as f:
                json.dump({"repo_url": final_url}, f)
        except Exception:
            pass

    return final_url, get_repo_from_url(final_url)

def publish_to_github_pages(report_path, image, repo_url, remember_choice=False):
    import os
    from pathlib import Path
    import shutil
    import subprocess
    from datetime import datetime
    from urllib.parse import urlparse

    if not repo_url:
        return None

    repo_obj = get_repo_from_url(repo_url)
    repo_url, repo = resolve_fork(repo_obj, action_label="publish", remember_choice=remember_choice, cache_key="fork-choice")

    parsed = urlparse(repo_url)
    repo_path = parsed.path.strip("/")
    if repo_path.endswith(".git"):
        repo_path = repo_path[:-4]
    user, repo = repo_path.split("/")[-2:]

    # Clone the repo to a temporary directory
    repo_clone_dir = os.path.join(TEMP_DIR, f"gh-pages-{repo}")
    
    # Remove existing clone if it exists
    if os.path.exists(repo_clone_dir):
        try:
            shutil.rmtree(repo_clone_dir)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to clean up existing directory: {e}")
    
    # Clone the repository for GitHub Pages publishing
    try:
        tool_progress("üì§", f"Preparing GitHub Pages for {repo}")
        subprocess.run(["git", "clone", repo_url, repo_clone_dir], 
                       check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        tool_done()
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to prepare repository: {e}")
        return None

    # Prepare the report path and content
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    report_filename = f"scan_report_{image.replace(':', '_')}_{timestamp}.html"
    
    # Check if we should use docs/ or gh-pages branch
    docs_dir = os.path.join(repo_clone_dir, "docs")
    use_docs = os.path.exists(docs_dir)
    
    if use_docs:
        target_dir = docs_dir
        branch = "main"  # Usually docs/ is on main branch
    else:
        target_dir = os.path.join(repo_clone_dir, "gh-pages")
        os.makedirs(target_dir, exist_ok=True)
        branch = "gh-pages"

    # Copy report to target directory
    target_path = os.path.join(target_dir, report_filename)
    shutil.copy2(report_path, target_path)
    
    # Create an index.html that redirects to the latest report
    index_html = os.path.join(target_dir, "index.html")
    with open(index_html, 'w') as f:
        f.write(f"""<html>
<head><meta http-equiv='refresh' content='0; URL={report_filename}'></head>
<body>
<p>Redirecting to <a href='{report_filename}'>{report_filename}</a></p>
</body>
</html>""")

    # Set up Git and push changes
    try:
        current_dir = os.getcwd()
        os.chdir(repo_clone_dir)
        
        # More robust branch handling
        if not use_docs:
            # Try to handle gh-pages branch with better error handling
            try:
                # First, fetch all branches to make sure we have the latest info
                subprocess.run(["git", "fetch", "origin"], 
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                
                # Check if branch exists remotely
                remote_exists = subprocess.run(
                    ["git", "ls-remote", "--heads", "origin", "gh-pages"],
                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
                ).stdout.strip() != ""
                
                if remote_exists:
                    print("üìÑ Setting up GitHub Pages branch...")
                    # Create a new local branch that tracks the remote
                    subprocess.run(["git", "branch", "-D", "gh-pages"], 
                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    subprocess.run(["git", "checkout", "-b", "gh-pages", "--no-track"], 
                                check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    subprocess.run(["git", "reset", "--hard", "origin/gh-pages"], 
                                check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                else:
                    print("üìÑ Creating new GitHub Pages branch...")
                    subprocess.run(["git", "checkout", "--orphan", "gh-pages"], 
                                check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    # Remove all existing files in this new branch
                    for item in os.listdir(repo_clone_dir):
                        if item != ".git" and item != "gh-pages":
                            path = os.path.join(repo_clone_dir, item)
                            if os.path.isdir(path):
                                shutil.rmtree(path)
                            else:
                                os.remove(path)
            except Exception as e:
                print(f"‚ö†Ô∏è Error setting up gh-pages branch, falling back to docs folder: {e}")
                # Fall back to using the main branch and docs folder
                subprocess.run(["git", "checkout", "main"], 
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                os.makedirs(os.path.join(repo_clone_dir, "docs"), exist_ok=True)
                target_dir = os.path.join(repo_clone_dir, "docs")
                branch = "main"
                use_docs = True
        
        # Add files
        if use_docs:
            subprocess.run(["git", "add", "docs"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        else:
            # In gh-pages branch, all files are at the root
            subprocess.run(["git", "add", "."], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
        # Commit
        tool_progress("üì§", "Publishing report to GitHub Pages")
        try:
            subprocess.run(
                ["git", "commit", "-m", f"üìä Add security scan report for {image}"],
                check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
            )
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è No changes to commit")
            os.chdir(current_dir)
            return None
            
        # Push (with force to handle any conflicts)
        try:
            subprocess.run(
                ["git", "push", "-u", "origin", branch, "--force"],
                check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
            )
            tool_done()
        except subprocess.CalledProcessError as e:
            print(f"‚ö†Ô∏è Failed to push to GitHub Pages: {e}")
            os.chdir(current_dir)
            return None
            
        os.chdir(current_dir)
        
    except Exception as e:
        print(f"‚ö†Ô∏è GitHub Pages publishing failed: {e}")
        if current_dir != os.getcwd():
            os.chdir(current_dir)
        return None
        
    # Clean up
    if not args.keep_data:
        try:
            shutil.rmtree(repo_clone_dir)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to clean up: {e}")
            
    # Return the GitHub Pages URL
    pages_url = f"https://{user}.github.io/{repo}/{report_filename}"
    return pages_url

def generate_html_report(image, trivy_cves, grype_cves, shared, only_trivy, only_grype, trivy_version, grype_version):
    """Generate a detailed HTML report of vulnerability findings."""
    from jinja2 import Environment, FileSystemLoader
    from pathlib import Path
    import html
    import json
    from datetime import datetime
    from urllib.request import urlretrieve

    def format_cves(cve_map):
        grouped = group_by_severity(cve_map)
        sections = ""
        severity_colors = {
            "Critical": "#d32f2f",
            "High": "#f57c00",
            "Medium": "#fbc02d",
            "Low": "#388e3c",
            "Unknown": "#757575"
        }
        for severity in sorted(
            grouped.keys(),
            key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize())
            if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"]
            else 99,
        ):
            color = severity_colors.get(severity.capitalize(), "#444")
            cves = grouped[severity]
            sections += f"<button class='collapsible severity-block' data-severity='{severity.capitalize()}' style='color:{color}'>{severity} ({len(cves)})</button><div class='content'><ul>"
            for cve in sorted(cves):
                cve_meta = cve_map.get(cve, {})
                if not isinstance(cve_meta, dict):
                    cve_meta = {}
                cve_link = f"https://nvd.nist.gov/vuln/detail/{cve}"
                cvss_score = cve_meta.get("cvss", "N/A")
                confidence = cve_meta.get("confidence", 1.0)
                trivy_id = cve_meta.get("trivy_id", cve)
                grype_id = cve_meta.get("grype_id", cve)
                last_modified = cve_meta.get("last_modified")
                references = cve_meta.get("references", [])

                if cvss_score != "N/A":
                    try:
                        cvss_score = f"{float(cvss_score):.1f}"
                    except ValueError:
                        pass

                if last_modified:
                    try:
                        date = datetime.fromisoformat(last_modified.replace('Z', '+00:00'))
                        last_modified = date.strftime('%Y-%m-%d')
                    except ValueError:
                        pass

                confidence_html = ""
                if confidence < 1.0:
                    confidence_color = "#f57c00" if confidence < 0.8 else "#388e3c"
                    confidence_html = f"<span style='color: {confidence_color}'>(Confidence: {confidence:.2f})</span>"

                similar_cve_html = ""
                if trivy_id != grype_id:
                    similar_cve_html = f"<br><small>Similar CVE IDs: Trivy={trivy_id}, Grype={grype_id}</small>"

                references_html = ""
                if references:
                    references_html = "<br><small>References:<br>"
                    for ref in references:
                        references_html += f"<a href='{ref}' target='_blank'>{ref}</a><br>"
                    references_html += "</small>"

                cve_entry = f'<li><a href="{cve_link}" target="_blank">{cve}</a>'
                if cvss_score != "N/A":
                    cve_entry += f" ‚Äî CVSS: {cvss_score}"
                if last_modified:
                    cve_entry += f" [Last Modified: {last_modified}]"
                cve_entry += f" {confidence_html}{similar_cve_html}{references_html}</li>"

                sections += cve_entry
            sections += "</ul></div>"
        return sections or "<p>No CVEs found.</p>"

    def format_and_group_by_type(cve_map):
        os_related, lang_related, unknown = {}, {}, {}
        for cve, meta in cve_map.items():
            if not isinstance(meta, dict):
                unknown[cve] = meta
                continue
            pkg_type = meta.get("pkg_type") or meta.get("type") or "unknown"
            if pkg_type.lower() in ["os", "debian", "alpine", "rpm", "apk"]:
                os_related[cve] = meta
            elif pkg_type.lower() in ["python", "java", "nodejs", "golang"]:
                lang_related[cve] = meta
            else:
                unknown[cve] = meta

        result = ""
        if os_related:
            result += "<h3>üßπ OS Package CVEs</h3>" + format_cves(os_related)
        if lang_related:
            result += "<h3>üì¶ Language Package CVEs</h3>" + format_cves(lang_related)
        if unknown:
            result += (
                "<h3>‚ùì Unclassified CVEs</h3>"
                "<p><em>These CVEs couldn't be categorized as OS or language packages. This often includes:</em></p>"
                "<ul><li>Uncommon sources or tools</li><li>Temporary CVE IDs (like TEMP-1234)</li><li>Missing or malformed metadata</li></ul>"
                + format_cves(unknown)
            )
        return result

    timestamp = datetime.now().strftime("%Y-%m-%d")
    report_name = f"scan_report_{image.replace(':', '_')}_{timestamp}.html"
    report_path = Path(SCAN_REPORTS_DIR) / report_name

    # Extract just the repository name from the image tag
    repo_name = None
    if "-" in image:
        # Get just the repository name without any other parts
        repo_name = image.split("-")[0]
        
    if args.verbose:
        print(f"üîç Generate HTML report - Using repo name: '{repo_name}'")

    # Important: We need to use the exact same file paths as in the main function
    if repo_name and not ":" in image:  # For Docker images built from a repo
        trivy_path = Path(SCAN_REPORTS_DIR) / f"original_trivy_{repo_name}.json"
        grype_path = Path(SCAN_REPORTS_DIR) / f"original_grype_{repo_name}.json"
        if args.verbose:
            print(f"üîç Looking for scan files at: {trivy_path}")
    else:  # For directly scanned images
        trivy_path = Path(SCAN_REPORTS_DIR) / "original_trivy.json"
        grype_path = Path(SCAN_REPORTS_DIR) / "original_grype.json"
        if args.verbose:
            print(f"üîç Using default scan file paths")

    # Try to load the scan data, with better error handling
    try:
        with open(trivy_path, 'r') as f:
            trivy_raw = html.escape(json.dumps(json.load(f), indent=2))
        with open(grype_path, 'r') as f:
            grype_raw = html.escape(json.dumps(json.load(f), indent=2))
    except FileNotFoundError as e:
        # This is the critical error we want to avoid, so let's be more graceful
        if args.verbose:
            print(f"‚ö†Ô∏è Could not find scan result files: {e}")
        trivy_raw = "{}"
        grype_raw = "{}"
    except json.JSONDecodeError as e:
        if args.verbose:
            print(f"‚ö†Ô∏è Invalid JSON in scan result files: {e}")
        trivy_raw = "{}"
        grype_raw = "{}"

    shared_cves_detailed = {cve: trivy_cves.get(cve) if isinstance(trivy_cves.get(cve), dict) else grype_cves.get(cve) or {"type": "unknown"} for cve in shared}
    trivy_unique_detailed = {cve: trivy_cves.get(cve) or {"type": "unknown"} for cve in only_trivy}
    grype_unique_detailed = {cve: grype_cves.get(cve) or {"type": "unknown"} for cve in only_grype}
    total_vulns = len(trivy_cves) + len(grype_cves) - len(shared)

    template_dir = Path(__file__).parent
    template_path = template_dir / "scan_template.html"

    if not template_path.exists():
        tool_progress("üìÑ", "scan_template.html missing ‚Äî restoring latest version...")
        try:
            urlretrieve(
                "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html",
                template_path
            )
            tool_done()
            print(f"‚úÖ scan_template.html downloaded to {template_path}")
        except Exception as e:
            print(f"‚ùå Failed to download template: {e}")
            print("‚ÑπÔ∏è Creating a basic template...")
            template_path.write_text("<html><body><h1>Vulnerability Report</h1><p>{{ image }}</p></body></html>")
            tool_done()

    env = Environment(loader=FileSystemLoader(template_dir))
    template = env.get_template("scan_template.html")

    rendered = template.render(
        image=image,
        trivy_version=trivy_version,
        grype_version=grype_version,
        total_vulns=total_vulns,
        shared_count=len(shared),
        trivy_count=len(only_trivy),
        grype_count=len(only_grype),
        shared_section=format_and_group_by_type(shared_cves_detailed),
        trivy_section=format_and_group_by_type(trivy_unique_detailed),
        grype_section=format_and_group_by_type(grype_unique_detailed),
        trivy_raw=trivy_raw,
        grype_raw=grype_raw,
        scan_date=timestamp,
        ghas_link=None  # Only shown if you inject it later
    )

    report_path.write_text(rendered)
    tool_progress("üìÑ", "Generating report")
    tool_done()
    
    return report_path

def upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, temp_dir, repo_url=None, verbose=False):
    from pathlib import Path
    import json, base64, tempfile, subprocess, gzip
    from datetime import datetime, timezone

    if not repo_url:
        return None

    repo_obj = get_repo_from_url(repo_url)
    repo_url, repo = resolve_fork(repo, action_label="upload", remember_choice=True, cache_key="fork-choice")

    def is_valid_sarif(sarif_path):
        try:
            with open(sarif_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return all(key in data for key in ["version", "runs"])
        except Exception:
            return False

    def check_repo_access(repo):
        try:
            result = subprocess.run(
                ["gh", "api", f"repos/{repo}"],
                capture_output=True,
                text=True
            )
            if result.returncode != 0:
                print(f"‚ö†Ô∏è Cannot access repository {repo}. Skipping GHAS upload.")
                return False
            return True
        except subprocess.CalledProcessError:
            print(f"‚ö†Ô∏è Failed to check repository access. Skipping GHAS upload.")
            return False

    def upload_sarif(sarif_path, tool_name):
        if not sarif_path.exists() or not is_valid_sarif(sarif_path):
            print(f"‚ö†Ô∏è Skipping {tool_name} SARIF upload: file not found or invalid.")
            return False

        def run_git(cmd):
            try:
                return subprocess.check_output(cmd, cwd=temp_dir, text=True).strip()
            except subprocess.CalledProcessError:
                return None

        if not (Path(temp_dir) / ".git").exists():
            print(f"‚ö†Ô∏è Not in a git repository. Skipping {tool_name} SARIF upload.")
            return False

        commit_sha = run_git(["git", "rev-parse", "HEAD"]) or "0000000000000000000000000000000000000000"
        ref = run_git(["git", "symbolic-ref", "-q", "HEAD"]) or "refs/heads/main"

        if commit_sha.startswith("fatal") or len(commit_sha) != 40:
            print(f"‚ö†Ô∏è Unable to retrieve a valid commit SHA. Skipping {tool_name} SARIF upload.")
            return False

        if not ref.startswith("refs/"):
            print(f"‚ö†Ô∏è Invalid or missing Git ref. Skipping {tool_name} SARIF upload.")
            return False

        with open(sarif_path, "rb") as f_in:
            sarif_bytes = f_in.read()

        with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=".sarif.gz") as gz_file:
            with gzip.GzipFile(fileobj=gz_file, mode="wb") as gzip_out:
                gzip_out.write(sarif_bytes)
            gzipped_path = gz_file.name

        with open(gzipped_path, "rb") as gz_file:
            encoded_sarif = base64.b64encode(gz_file.read()).decode("utf-8")

        payload = {
            "commit_sha": commit_sha,
            "ref": ref,
            "sarif": encoded_sarif,
            "tool_name": tool_name,
            "checkout_uri": ".",
            "started_at": datetime.now(timezone.utc).isoformat()
        }

        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".json") as tmp:
            json.dump(payload, tmp)
            tmp_path = tmp.name

        try:
            result = subprocess.run([
                "gh", "api", f"repos/{repo}/code-scanning/sarifs",
                "--method", "POST",
                "--input", tmp_path
            ], capture_output=True, text=True)

            if result.returncode != 0:
                error_msg = result.stderr.strip()
                if "insufficient_scope" in error_msg.lower() or "not authorized" in error_msg.lower():
                    print(f"\n‚ö†Ô∏è GitHub token lacks required permissions for SARIF uploads.")
                    print("   Required scopes: security_events, admin:repo_hook")
                    print("\n   To fix this, run:")
                    print("   gh auth refresh -h github.com -s security_events,admin:repo_hook")
                    print("\n   Or visit: https://github.com/settings/tokens")
                    return "insufficient_scope"
                elif "not found" in error_msg.lower():
                    print(f"‚ö†Ô∏è Repository {repo} not found or not accessible.")
                else:
                    print(f"‚ö†Ô∏è Failed to upload {tool_name} SARIF: {error_msg}")
                return False
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ö†Ô∏è Failed to upload {tool_name} SARIF: {e.stderr.strip() if e.stderr else str(e)}")
            return False

    if not check_repo_access(repo):
        return

    scan_dir = Path(SCAN_REPORTS_DIR)
    grype_sarif_path = scan_dir / "ghas_upload_grype.sarif"
    trivy_sarif_path = scan_dir / "ghas_upload_trivy.sarif"
    diff_sarif_path = scan_dir / "ghas_diff.sarif"

    print("\nüì¶ Generating SARIF files for GHAS upload...")

    uploads = []
    permission_error = False

    try:
        tool_progress("üîç", "Generating SARIF for Grype")
        subprocess.run(["grype", image, "-o", "sarif", "--file", str(grype_sarif_path)], check=True,
                       stdout=None if verbose else subprocess.DEVNULL,
                       stderr=None if verbose else subprocess.DEVNULL)
        result = upload_sarif(grype_sarif_path, "grype")
        if result == "insufficient_scope":
            permission_error = True
            print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
            return
        elif result:
            uploads.append("grype")
        tool_done()
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate SARIF with grype: {e}")
        tool_done()

    if not permission_error:
        try:
            tool_progress("üîç", "Generating SARIF for Trivy")
            subprocess.run(["trivy", "image", "-f", "sarif", "-o", str(trivy_sarif_path), image], check=True,
                          stdout=None if verbose else subprocess.DEVNULL,
                          stderr=None if verbose else subprocess.DEVNULL)
            result = upload_sarif(trivy_sarif_path, "trivy")
            if result == "insufficient_scope":
                permission_error = True
                print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
                return
            elif result:
                uploads.append("trivy")
            tool_done()
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to generate SARIF with trivy: {e}")
            tool_done()

    if not permission_error and diff_sarif_path.exists():
        tool_progress("üîç", "Generating diff SARIF")
        result = upload_sarif(diff_sarif_path, "scancompare")
        if result == "insufficient_scope":
            permission_error = True
            print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
            return
        elif result:
            uploads.append("diff (trivy vs grype)")
        tool_done()

    success = False
    if uploads:
        print(f"\n‚úÖ Uploaded SARIF to GHAS: {', '.join(uploads)}")
        gh_link = f"https://github.com/{repo}/security/code-scanning"
        success = True
    elif not permission_error:
        print("\n‚ö†Ô∏è No SARIF files were uploaded to GHAS.")
    
    return repo_url, repo, success

def explain_exit(msg):
    print(f"‚ö†Ô∏è {msg}")
    sys.exit(1)

def validate_image_exists(image):
    try:
        # Try checking locally first
        local_result = subprocess.run(["docker", "image", "inspect", image], capture_output=True)
        if local_result.returncode == 0:
            return True

        # If not found locally, try to pull the image
        print(f"üîÑ Image not found locally. Attempting to pull {image}...")
        pull_result = subprocess.run(["docker", "pull", image], capture_output=True)
        if pull_result.returncode == 0:
            return True

        return False

    except FileNotFoundError:
        print("‚ùå Docker is not installed or not in your PATH.")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Unexpected error while validating image: {e}")
        return False

def handle_post_scan_prompts(image_tag, html_path, repo_url=None, trivy_path=None, grype_path=None, shared=None, only_trivy=None, only_grype=None, temp_dir=None, scanned_from_repo=False):
    """Handle post-scan operations including GitHub uploads and report viewing with support for automation."""
    import webbrowser
    from urllib.parse import urlparse
    from pathlib import Path

    uploaded_ghas = False
    uploaded_pages = False
    ghas_url = None
    pages_url = None
    chosen_repo_url = repo_url

    urls_to_open = {
        "local": f"file://{Path(html_path).absolute()}"
    }

    repo = get_repo_from_url(repo_url) if repo_url else None

    # Determine automation mode from args
    automation_mode = get_automation_mode(args)

    # Handle GitHub repository integration
    if repo and scanned_from_repo and not (args.ghas or args.gh_pages):
        if automation_mode == "auto":
            print("\nüìã Automatically uploading results to both GHAS and GitHub Pages in auto mode.")
            upload_options = "b"
        elif automation_mode == "yes":
            print("\nüìã Automatically selecting 'yes' to upload to both GHAS and GitHub Pages.")
            upload_options = "b"
        elif automation_mode == "no":
            print("\nüìã Automatically selecting 'no' to GitHub uploads in no-automation mode.")
            upload_options = "n"
        else:
            # Interactive mode
            print("\nüìã GitHub Repository Integration Options")
            print("Since you've scanned using a GitHub repository, you can publish your results:")
            upload_options = input("Would you like to upload results to GitHub? (n = no, g = GHAS only, p = Pages only, b = both): ").strip().lower()

        # Process selected upload options
        if upload_options in ("g", "b"):
            # Handle GHAS upload
            try:
                ensure_gh_authenticated()
                chosen_repo_url, modified_repo, success = upload_to_ghas(
                    image_tag, trivy_path, grype_path, shared, only_trivy,
                    only_grype, repo, temp_dir or ".", repo_url=repo_url,
                    verbose=args.verbose
                )
                if success:
                    uploaded_ghas = True
                    ghas_url = f"https://github.com/{modified_repo}/security/code-scanning"
                    urls_to_open["ghas"] = ghas_url
            except Exception as e:
                print(f"\n‚ö†Ô∏è GHAS upload failed: {e}")
                if automation_mode == "auto":
                    print("Continuing with auto mode despite GHAS upload failure.")

        if upload_options in ("p", "b"):
            # Handle GitHub Pages upload
            try:
                pages_url = publish_to_github_pages(html_path, image_tag, chosen_repo_url, remember_choice=uploaded_ghas)
                if pages_url:
                    uploaded_pages = True
                    urls_to_open["pages"] = pages_url
            except Exception as e:
                print(f"\n‚ö†Ô∏è GitHub Pages publishing failed: {e}")
                if automation_mode == "auto":
                    print("Continuing with auto mode despite GitHub Pages failure.")

    # Handle explicit command line flags for GHAS and GitHub Pages
    else:
        if repo:
            if args.ghas:
                try:
                    ensure_gh_authenticated()
                    chosen_repo_url, modified_repo, success = upload_to_ghas(
                        image_tag, trivy_path, grype_path, shared, only_trivy,
                        only_grype, repo, temp_dir or ".", repo_url=repo_url,
                        verbose=args.verbose
                    )
                    if success:
                        uploaded_ghas = True
                        ghas_url = f"https://github.com/{modified_repo}/security/code-scanning"
                        urls_to_open["ghas"] = ghas_url
                except Exception as e:
                    print(f"\n‚ö†Ô∏è GHAS upload failed: {e}")
                    if automation_mode == "auto":
                        print("Continuing with auto mode despite GHAS upload failure.")

            if args.gh_pages:
                try:
                    pages_url = publish_to_github_pages(html_path, image_tag, chosen_repo_url, remember_choice=uploaded_ghas)
                    if pages_url:
                        uploaded_pages = True
                        urls_to_open["pages"] = pages_url
                except Exception as e:
                    print(f"\n‚ö†Ô∏è GitHub Pages publishing failed: {e}")
                    if automation_mode == "auto":
                        print("Continuing with auto mode despite GitHub Pages failure.")

    # Output all URLs
    if uploaded_ghas or uploaded_pages:
        print("\n‚úÖ Publishing complete!")
        if uploaded_ghas:
            print("üîó GHAS results:")
            print(ghas_url)
        if uploaded_pages:
            print("üîó GitHub Pages report:")
            print(pages_url)

            parsed = urlparse(pages_url)
            path_parts = parsed.path.strip("/").split("/")
            if len(path_parts) >= 1:
                repo_root = f"{parsed.scheme}://{parsed.netloc}/{path_parts[0]}"
                index_fallback_url = f"{repo_root}/index.html"
                print("\n‚ö†Ô∏è GitHub Pages may take a minute or two to render your report.")
                print("   If it doesn't load immediately, try visiting the root index.html directly:")
                print("   " + index_fallback_url)
                print("   Need help? GitHub Pages docs:")
                print("   https://docs.github.com/en/pages")

    # Handle browser opening based on automation mode
    if automation_mode == "auto":
        print("\nüñ•Ô∏è Automatically opening reports in browser (auto mode)...")
        # In auto mode, we open all available reports
        if not uploaded_pages:
            webbrowser.open(urls_to_open["local"])
        if "ghas" in urls_to_open:
            webbrowser.open(urls_to_open["ghas"])
        if "pages" in urls_to_open:
            webbrowser.open(urls_to_open["pages"])
    elif automation_mode == "yes":
        print("\nüñ•Ô∏è Automatically selecting 'yes' to open reports in browser.")
        # In mock-yes mode, we also open all available reports
        if not uploaded_pages:
            webbrowser.open(urls_to_open["local"])
        if "ghas" in urls_to_open:
            webbrowser.open(urls_to_open["ghas"])
        if "pages" in urls_to_open:
            webbrowser.open(urls_to_open["pages"])
    elif automation_mode == "no":
        print("\nüñ•Ô∏è Automatically selecting 'no' to opening reports in browser.")
        # In mock-no mode, we don't open any reports
        pass
    else:
        # Interactive mode - ask the user
        if uploaded_ghas and uploaded_pages:
            prompt = "\nüñ•Ô∏è Open GitHub-hosted reports in browser? (GHAS + GitHub Pages) (y/n): "
        elif uploaded_ghas:
            prompt = "\nüñ•Ô∏è Open GHAS results and local HTML report in browser? (y/n): "
        elif uploaded_pages:
            prompt = "\nüñ•Ô∏è Open GitHub Pages report in browser? (y/n): "
        else:
            prompt = "\nüñ•Ô∏è Open local HTML report in browser? (y/n): "

        open_reports = input(prompt).strip().lower()
        
        if open_reports == "y":
            if not uploaded_pages:
                webbrowser.open(urls_to_open["local"])
            if "ghas" in urls_to_open:
                webbrowser.open(urls_to_open["ghas"])
            if "pages" in urls_to_open:
                webbrowser.open(urls_to_open["pages"])

def main():
    global args
    fork_choice_cache = {}
    global cloned_repo_dir

    # Capture the command-line arguments
    args = handle_cli_args()

    # ‚úÖ Always check for updates *before* anything else
    if args.update:
        check_latest_version(only_check=False)
        return
    else:
        check_latest_version(only_check=True)

    # Now continue with directory setup and command handling
    ensure_directory_structure()

    if args.uninstall:
        uninstall_scancompare()
        return

    if args.version:
        return  # Version info already printed

    # If neither image nor repo URL is provided, show usage and exit
    if not args.image and not args.repo_url:
        print("Usage: scancompare <image> [--repo-url <repo>, --ghas, --gh-pages, --keep-data, --verbose, --version, --update, --uninstall]")
        sys.exit(1)

    # Check if the provided image exists
    if args.image and not validate_image_exists(args.image):
        explain_exit(f"Docker image '{args.image}' could not be found locally or remotely.")

    # Prepare for scanning and logging
    summary_lines = ["\n" + "="*40, f"Scan started: {datetime.now().isoformat()}"]
    summary_lines.append(f"üî¢ scancompare version: {get_current_version()}")

    temp_dir = None
    image_tag = None

    try:
        # Handle scanning from a repository
        if args.repo_url and not args.image:
            repo_name = args.repo_url.split('/')[-1].replace('.git', '')
            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
            short_timestamp = timestamp[-6:]
            temp_dir = os.path.join(DOCKER_TEMP_DIR, f"{repo_name}_{timestamp}")
            os.makedirs(temp_dir, exist_ok=True)
            print(f"üìÅ Created temporary directory: {temp_dir}")

            tool_progress("üì•", f"Cloning {args.repo_url}")
            git_clone_cmd = ["git", "clone", args.repo_url, temp_dir]
            if args.verbose:
                subprocess.run(git_clone_cmd, check=True)
            else:
                subprocess.run(git_clone_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            tool_done()

            cloned_repo_dir = temp_dir

            dockerfile_paths = []
            for root, _, files in os.walk(temp_dir):
                for file in files:
                    if file.lower().startswith("dockerfile"):
                        dockerfile_paths.append(os.path.join(root, file))

            if not dockerfile_paths:
                print("‚ùå No Dockerfile found in the provided repository.")
                if not args.keep_data:
                    shutil.rmtree(temp_dir)
                sys.exit(1)

            built_images = []
            for dockerfile_path in dockerfile_paths:
                try:
                    dockerfile_dir = os.path.dirname(dockerfile_path)
                    image_tag = f"{repo_name}-{short_timestamp}"

                    tool_progress("üê≥", f"Building Docker image '{image_tag}'")
                    docker_build_cmd = ["docker", "build", "-t", image_tag, dockerfile_dir]
                    if args.verbose:
                        subprocess.run(docker_build_cmd, check=True)
                    else:
                        subprocess.run(docker_build_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    tool_done()

                    built_images.append(image_tag)
                    print(f"üõ°Ô∏è Scanning image: {image_tag}")
                    summary_lines.append(f"üõ°Ô∏è Scanning built image: {image_tag}")

                    trivy_path = os.path.join(SCAN_REPORTS_DIR, f"original_trivy_{repo_name}.json")
                    grype_path = os.path.join(SCAN_REPORTS_DIR, f"original_grype_{repo_name}.json")

                    tool_progress("üîπ", "Running Trivy scan")
                    if not run_scan("trivy", image_tag, trivy_path, verbose=args.verbose):
                        print(f"‚ö†Ô∏è Trivy scan failed for {image_tag}")
                        continue
                    tool_done()

                    tool_progress("üîπ", "Running Grype scan")
                    if not run_scan("grype", image_tag, grype_path, verbose=args.verbose):
                        print(f"‚ö†Ô∏è Grype scan failed for {image_tag}")
                        continue
                    tool_done()

                    trivy_data = extract_cves_with_severity(trivy_path, verbose=args.verbose)
                    grype_data = extract_cves_with_severity(grype_path, verbose=args.verbose)

                    try:
                        with open(trivy_path, 'r') as f:
                            raw_trivy_data = json.load(f)
                        with open(grype_path, 'r') as f:
                            raw_grype_data = json.load(f)
                    except (json.JSONDecodeError, FileNotFoundError) as e:
                        raw_trivy_data = None
                        raw_grype_data = None
                        if args.verbose:
                            print(f"‚ö†Ô∏è Could not load raw scan data: {e}")

                    shared, only_trivy, only_grype = display_summary(
                        trivy_data, grype_data, image=image_tag,
                        trivy_data=raw_trivy_data, grype_data=raw_grype_data
                    )

                    print_cves_by_severity(f"Unique to Grype ({image_tag})", {c: grype_data[c] for c in only_grype})
                    print_cves_by_severity(f"Unique to Trivy ({image_tag})", {c: trivy_data[c] for c in only_trivy})
                    print_cves_by_severity(f"Shared CVEs ({image_tag})", {c: trivy_data.get(c, grype_data.get(c, 'Unknown')) for c in shared})

                    html_path = generate_html_report(image_tag, trivy_data, grype_data, shared, only_trivy, only_grype,
                                                    get_version("trivy"), get_version("grype"))
                    print(f"\n‚úÖ Local HTML report saved: {html_path}")

                    handle_post_scan_prompts(
                        image_tag=image_tag,
                        html_path=html_path,
                        repo_url=args.repo_url,
                        trivy_path=trivy_path,
                        grype_path=grype_path,
                        shared=shared,
                        only_trivy=only_trivy,
                        only_grype=only_grype,
                        temp_dir=temp_dir,
                        scanned_from_repo=True
                    )

                except Exception as e:
                    print(f"‚ö†Ô∏è Unexpected error: {e}")
                    if args.verbose:
                        import traceback
                        traceback.print_exc()

            if not built_images:
                print("‚ùå No images were successfully built and scanned.")
                if not args.keep_data:
                    shutil.rmtree(temp_dir)
                sys.exit(1)
        else:
            image = args.image
            print(f"üõ°Ô∏è Scanning image: {image}")
            summary_lines.append(f"üõ°Ô∏è Scanning image: {image}")

            trivy_path = os.path.join(SCAN_REPORTS_DIR, "original_trivy.json")
            grype_path = os.path.join(SCAN_REPORTS_DIR, "original_grype.json")

            tool_progress("üîπ", "Running Trivy scan")
            if not run_scan("trivy", image, trivy_path, verbose=args.verbose):
                explain_exit("Trivy scan failed or image not found.")
            tool_done()

            tool_progress("üîπ", "Running Grype scan")
            if not run_scan("grype", image, grype_path, verbose=args.verbose):
                explain_exit("Grype scan failed or image not found.")
            tool_done()

            trivy_data = extract_cves_with_severity(trivy_path, verbose=args.verbose)
            grype_data = extract_cves_with_severity(grype_path, verbose=args.verbose)

            try:
                with open(trivy_path, 'r') as f:
                    raw_trivy_data = json.load(f)
                with open(grype_path, 'r') as f:
                    raw_grype_data = json.load(f)
            except (json.JSONDecodeError, FileNotFoundError) as e:
                raw_trivy_data = None
                raw_grype_data = None
                if args.verbose:
                    print(f"‚ö†Ô∏è Could not load raw scan data: {e}")

            shared, only_trivy, only_grype = display_summary(
                trivy_data, grype_data, image=image,
                trivy_data=raw_trivy_data, grype_data=raw_grype_data
            )

            print_cves_by_severity("Unique to Grype", {c: grype_data[c] for c in only_grype})
            print_cves_by_severity("Unique to Trivy", {c: trivy_data[c] for c in only_trivy})
            print_cves_by_severity("Shared CVEs", {c: trivy_data.get(c, grype_data.get(c, 'Unknown')) for c in shared})

            html_path = generate_html_report(image, trivy_data, grype_data, shared, only_trivy, only_grype,
                                            get_version("trivy"), get_version("grype"))
            print(f"\n‚úÖ Local HTML report saved: {html_path}")

            handle_post_scan_prompts(
                image_tag=image,
                html_path=html_path,
                repo_url=args.repo_url,
                trivy_path=trivy_path,
                grype_path=grype_path,
                shared=shared,
                only_trivy=only_trivy,
                only_grype=only_grype,
                temp_dir=temp_dir,
                scanned_from_repo=bool(args.repo_url)
            )
    finally:
        if temp_dir and not args.keep_data:
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to clean up temporary directory: {e}")

    summary_lines.extend([f"Scan completed: {datetime.now().isoformat()}", "="*40 + "\n"])
    with open(LOG_FILE, 'a') as f:
        f.write("\n".join(summary_lines))

if __name__ == "__main__":
    main()