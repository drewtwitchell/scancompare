#!/usr/bin/env python3
# scancompare version 1.0.0

import os
import sys
import json
import subprocess
from datetime import datetime, timezone
from urllib.request import Request, urlopen
from pathlib import Path
import webbrowser
import shutil
import tempfile
import html
import base64
import argparse
import re
import time
import random
import uuid
from io import BytesIO  
from jinja2 import Environment, FileSystemLoader

SCRIPT_NAME = "scancompare"
SCRIPT_URL = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scancompare"
VERSION = "1.0.0"

args = None  # global placeholder
cloned_repo_dir = None # global placeholder

# tool_progress and tool_done functions
def tool_progress(ACTION, TOOL_NAME):
    print(f"{ACTION} {TOOL_NAME}...", end=" ")

def tool_done():
    print(" \033[32m‚úî\033[0m")

def get_current_version():
    return VERSION

def check_latest_version(only_check=False):
    already_updated = os.environ.get("SCANCOMPARE_UPDATED") == "1"
    if already_updated and not only_check:
        return

    try:
        api_url = "https://api.github.com/repos/drewtwitchell/scancompare/contents/scancompare"
        req = Request(api_url, headers={"User-Agent": "scancompare-updater"})
        with urlopen(req) as response:
            metadata = json.load(response)

        if "content" not in metadata or "encoding" not in metadata or metadata["encoding"] != "base64":
            raise ValueError("Could not decode GitHub API content")

        new_code = base64.b64decode(metadata["content"]).decode("utf-8")
        latest_match = re.search(r'VERSION\s*=\s*[\'"]([^\'"]+)[\'"]', new_code)
        if not latest_match:
            raise ValueError("Could not determine latest version from script.")

        latest = latest_match.group(1)
        current = get_current_version()

        if latest == current:
            if only_check:
                print(f"üì¶ scancompare version {current}")
                sys.exit(0)
            else:
                print(f"üì¶ scancompare version {current}")
                return

        print(f"üîÑ Updating scancompare from {current} to {latest}...")
        update_script(new_code)  # This will handle the update, with its own progress steps

        print(f"‚úÖ scancompare updated to version {latest}")
        print("‚ôªÔ∏è Restarting with updated version...")

        os.execve(
            sys.executable,
            [sys.executable, str(get_script_path())] + sys.argv[1:],
            {**os.environ, "SCANCOMPARE_UPDATED": "1"}
        )

    except Exception as e:
        print(f"‚ö†Ô∏è Auto-update check failed: {e}")
        if only_check:
            print(f"üì¶ scancompare version {get_current_version()}")
            sys.exit(0)

def update_script(new_code):
    try:
        import urllib.request

        # Resolve real script path, even if run from a wrapper
        script_path = Path(get_script_path()).resolve()
        install_dir = script_path.parent
        template_path = install_dir / "scan_template.html"

        if not os.access(script_path, os.W_OK):
            raise PermissionError(f"Cannot write to {script_path}. Try using sudo or adjusting permissions.")

        # Write new script to temporary file
        with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tmp:
            tmp.write(new_code)
            temp_path = tmp.name

        os.replace(temp_path, script_path)

        # Always fetch latest scan_template.html
        tool_progress("updates"," Getting latest scan_template.html")
        template_url = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html"
        try:
            with urllib.request.urlopen(template_url) as response:
                template_html = response.read().decode("utf-8")
                template_path.write_text(template_html)
            tool_done()  # Complete scan_template.html update
        except Exception as e:
            print(f"‚ùå Failed to update scan_template.html: {e}")

        # Ensure jinja2 is installed after update
        try:
            import jinja2  # noqa: F401
        except ImportError:
            tool_progress("jinja2", "Installing jinja2")
            subprocess.run([sys.executable, "-m", "pip", "install", "--user", "jinja2"], check=True)
            tool_done()  # Complete jinja2 installation

    except Exception as e:
        print(f"‚ùå Critical: failed to update script: {e}")
        sys.exit(1)

def get_script_path():
    try:
        return os.path.realpath(__file__)
    except NameError:
        return sys.argv[0]
    
def uninstall_scancompare():
    print("üßπ Uninstalling scancompare...")

    paths_removed = False
    lib_dir = Path.home() / ".local" / "lib" / "scancompare"
    script_path = lib_dir / "scancompare"
    wrapper_path = Path.home() / ".local" / "bin" / "scancompare"
    venv_path = lib_dir / "venv"

    # Removing script and wrapper paths
    for path in [script_path, wrapper_path]:
        if path.exists():
            print(f"üóëÔ∏è  Removing {path}...", end=" ")
            path.unlink()
            print("‚úî")
            paths_removed = True

    # Removing virtual environment
    if venv_path.exists() and venv_path.is_dir():
        print("üßπ Removing virtual environment...", end=" ")
        shutil.rmtree(venv_path)
        print("‚úî")

    # Removing scan_reports directory
    reports_dir = Path("scan_reports")
    if reports_dir.exists():
        print("üóëÔ∏è  Removing scan reports...", end=" ")
        shutil.rmtree(reports_dir)
        print("‚úî")

    # Cleaning up shell config in profile files
    profile_files = [".zshrc", ".bashrc", ".profile", ".bash_profile", ".zprofile"]
    for profile in profile_files:
        full_path = Path.home() / profile
        if full_path.exists():
            print(f"üßΩ Cleaning up {profile}...", end=" ")
            content = full_path.read_text()
            new_content = "\n".join(
                line for line in content.splitlines()
                if 'export PATH="$HOME/.local/bin:$PATH"' not in line and 'source "$HOME/.config/scancompare/env.shexport"' not in line
            )
            full_path.write_text(new_content)
            print("‚úî")

    if paths_removed:
        print("\n‚úÖ scancompare successfully uninstalled.")
    else:
        print("\n‚ÑπÔ∏è  scancompare was not found or already uninstalled.")
    
    sys.exit(0)

def handle_cli_args():
    parser = argparse.ArgumentParser(
        description="Scan and compare Docker image vulnerabilities using Trivy and Grype."
    )
    parser.add_argument("image", nargs="?", help="Docker image to scan")
    parser.add_argument("--repo-url", help="GitHub repo URL containing Dockerfile for image build")
    parser.add_argument("--keep-data", action="store_true", help="Keep Docker image, cloned repo, HTML, SARIF, and JSON results")
    parser.add_argument("--version", action="store_true", help="Show scancompare version")
    parser.add_argument("--update", action="store_true", help="Check and apply latest scancompare update")
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    parser.add_argument("--uninstall", action="store_true", help="Uninstall scancompare CLI")
    return parser.parse_args()

def get_version(tool):
    try:
        result = subprocess.run([tool, "version"], capture_output=True, text=True)

        for line in result.stdout.splitlines():
            if tool.lower() == "grype":
                if line.strip().startswith("Version:"):
                    return line.split("Version:")[-1].strip()
            else:
                if tool.lower() in line.lower() or "Version" in line:
                    return line.strip()
    except FileNotFoundError:
        print(f"‚ö†Ô∏è {tool} is not installed or not in PATH.")
        return "not installed"
    except Exception as e:
        print(f"‚ö†Ô∏è Error getting version for {tool}: {e}")
        return "unknown"

    return "unknown"

def run_scan(tool, image, output_path, verbose=False):
    scan_command = {
        "trivy": ["trivy", "image", "--format", "json", "-o", str(output_path), image],
        "grype": ["grype", image, "-o", "json", "--file", str(output_path)],
    }.get(tool)

    if not scan_command:
        print(f"‚ùå Unknown scanner: {tool}")
        return False

    try:
        if verbose:
            subprocess.run(scan_command, check=True)
        else:
            subprocess.run(scan_command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        return True
    except subprocess.CalledProcessError:
        return False

def extract_cves_with_severity(file_path):
    try:
        with open(file_path) as f:
            data = json.load(f)

        severity_map = {}
        if "Results" in data:  # Trivy format
            for result in data.get("Results", []):
                for vuln in result.get("Vulnerabilities", []):
                    cve = vuln.get("VulnerabilityID")
                    severity = vuln.get("Severity", "UNKNOWN").capitalize()
                    if cve:
                        severity_map[cve] = {
                            "severity": severity,
                            "type": result.get("Type", "unknown"),
                            "cvss": vuln.get("CVSS", {}).get("nvd", {}).get("V3Score", "N/A")
                        }
        elif isinstance(data, list):  # Grype format
            for match in data:
                vuln = match.get("vulnerability", {})
                cve = vuln.get("id")
                severity = vuln.get("severity", "UNKNOWN").capitalize()
                if cve:
                    severity_map[cve] = {
                        "severity": severity,
                        "type": match.get("artifact", {}).get("type", "unknown"),
                        "cvss": vuln.get("cvss", {}).get("nvd", {}).get("v3", {}).get("score", "N/A")
                    }

        return severity_map
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to parse {file_path}: {e}")
        return {}

def group_by_severity(cve_map):
    grouped = {}
    for cve_id, meta in cve_map.items():
        if isinstance(meta, dict):
            severity = meta.get("severity", "Unknown")
        else:
            severity = meta or "Unknown"
        grouped.setdefault(severity, []).append(cve_id)
    return grouped

def print_cves_by_severity(title, cve_map):
    print(f"\nüî∏ {title}")  # Add newline before each section
    grouped = group_by_severity(cve_map)
    for severity in sorted(grouped.keys(), key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize()) if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"] else 99):
        print(f"  {severity} ({len(grouped[severity])}):")
        for cve in sorted(grouped[severity]):
            cve_info = cve_map.get(cve, {})
            if isinstance(cve_info, dict):
                print(f"    - {cve} (CVSS: {cve_info.get('cvss', 'N/A')})")
            else:
                print(f"    - {cve}")
        print()  # Add newline after each severity group

def display_summary(trivy_cves, grype_cves):
    shared = set(trivy_cves) & set(grype_cves)
    only_trivy = set(trivy_cves) - set(grype_cves)
    only_grype = set(grype_cves) - set(trivy_cves)

    print("\nüìä Summary Report")
    print("----------------")
    print(" Tool  | Total | Only in Tool | Shared")
    print("-------|-------|--------------|--------")
    print(f"Grype  | {len(grype_cves):<5} | {len(only_grype):<12} | {len(shared)}")
    print(f"Trivy  | {len(trivy_cves):<5} | {len(only_trivy):<12} | {len(shared)}\n")

    return shared, only_trivy, only_grype

def generate_html_report(image, trivy_cves, grype_cves, shared, only_trivy, only_grype, trivy_version, grype_version):
    def format_cves(cve_map):
        grouped = group_by_severity(cve_map)
        sections = ""
        severity_colors = {
            "Critical": "#d32f2f",
            "High": "#f57c00",
            "Medium": "#fbc02d",
            "Low": "#388e3c",
            "Unknown": "#757575"
        }
        for severity in sorted(
            grouped.keys(),
            key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize())
            if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"]
            else 99,
        ):
            color = severity_colors.get(severity.capitalize(), "#444")
            cves = grouped[severity]
            sections += f"<button class='collapsible severity-block' data-severity='{severity.capitalize()}' style='color:{color}'>{severity} ({len(cves)})</button><div class='content'><ul>"
            for cve in sorted(cves):
                cve_meta = cve_map.get(cve, {})
                if not isinstance(cve_meta, dict):
                    cve_meta = {}
                cve_link = f"https://nvd.nist.gov/vuln/detail/{cve}"
                cvss_score = cve_meta.get("cvss", "N/A")
                sections += f'<li><a href="{cve_link}" target="_blank">{cve}</a> ‚Äî CVSS: {cvss_score}</li>'
            sections += "</ul></div>"
        return sections or "<p>No CVEs found.</p>"

    def format_and_group_by_type(cve_map):
        os_related = {}
        lang_related = {}
        unknown = {}
        for cve, meta in cve_map.items():
            if not isinstance(meta, dict):
                unknown[cve] = meta
                continue
            pkg_type = meta.get("pkg_type") or meta.get("type") or "unknown"
            if pkg_type.lower() in ["os", "debian", "alpine", "rpm", "apk"]:
                os_related[cve] = meta
            elif pkg_type.lower() in ["python", "java", "nodejs", "golang"]:
                lang_related[cve] = meta
            else:
                unknown[cve] = meta

        result = ""
        if os_related:
            result += "<h3>üßπ OS Package CVEs</h3>" + format_cves(os_related)
        if lang_related:
            result += "<h3>üì¶ Language Package CVEs</h3>" + format_cves(lang_related)
        if unknown:
            result += (
                "<h3>‚ùì Unclassified CVEs</h3>"
                "<p><em>These CVEs couldn't be categorized as OS or language packages. This often includes:</em></p>"
                "<ul><li>Uncommon sources or tools</li><li>Temporary CVE IDs (like TEMP-1234)</li><li>Missing or malformed metadata</li></ul>"
                + format_cves(unknown)
            )
        return result

    timestamp = datetime.now().strftime("%Y-%m-%d")
    report_name = f"scan_report_{image.replace(':', '_')}_{timestamp}.html"
    report_path = Path("scan_reports") / report_name

    # Get the directory name from the image tag if it exists
    dir_name = None
    if "-" in image:
        dir_name = image.split("-")[-2]  # Get the second-to-last part of the image tag

    # Use the correct file paths based on whether we're scanning multiple Dockerfiles
    if dir_name:
        trivy_path = Path("scan_reports") / f"original_trivy_{dir_name}.json"
        grype_path = Path("scan_reports") / f"original_grype_{dir_name}.json"
    else:
        trivy_path = Path("scan_reports") / "original_trivy.json"
        grype_path = Path("scan_reports") / "original_grype.json"

    try:
        trivy_raw = html.escape(json.dumps(json.load(open(trivy_path)), indent=2))
        grype_raw = html.escape(json.dumps(json.load(open(grype_path)), indent=2))
    except FileNotFoundError as e:
        print(f"‚ö†Ô∏è Could not find scan result files: {e}")
        trivy_raw = "{}"
        grype_raw = "{}"

    shared_cves_detailed = {cve: trivy_cves.get(cve) if isinstance(trivy_cves.get(cve), dict) else grype_cves.get(cve) or {"type": "unknown"} for cve in shared}
    trivy_unique_detailed = {cve: trivy_cves.get(cve) or {"type": "unknown"} for cve in only_trivy}
    grype_unique_detailed = {cve: grype_cves.get(cve) or {"type": "unknown"} for cve in only_grype}

    total_vulns = len(trivy_cves) + len(grype_cves) - len(shared)

    template_dir = Path(__file__).parent
    template_path = template_dir / "scan_template.html"

    if not template_path.exists():
        tool_progress("updates_html","üìÑ scan_template.html missing ‚Äî restoring latest version...")
        try:
            urllib.request.urlretrieve(
                "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html",
                template_path
            )
            tool_done()
            print(f"‚úÖ scan_template.html downloaded to {template_path}")
        except Exception as e:
            print(f"‚ùå Failed to download template: {e}")
    env = Environment(loader=FileSystemLoader(template_dir))
    template = env.get_template("scan_template.html")

    rendered = template.render(
        image=image,
        trivy_version=trivy_version,
        grype_version=grype_version,
        total_vulns=total_vulns,
        shared_count=len(shared),
        trivy_count=len(only_trivy),
        grype_count=len(only_grype),
        shared_section=format_and_group_by_type(shared_cves_detailed),
        trivy_section=format_and_group_by_type(trivy_unique_detailed),
        grype_section=format_and_group_by_type(grype_unique_detailed),
        trivy_raw=trivy_raw,
        grype_raw=grype_raw
    )

    report_path.write_text(rendered)
    tool_progress("üìÑ", "Generating local HTML report")
    tool_done()
    print(f"‚úÖ Local HTML report saved: {report_path}")

    open_html = input("üìÅ Open local HTML report in browser? (y/n): ").strip().lower()
    if open_html == "y":
        webbrowser.open(f"file://{report_path.absolute()}")

def explain_exit(msg):
    print(f"‚ö†Ô∏è {msg}")
    sys.exit(1)

def validate_image_exists(image):
    try:
        # Try checking locally first
        local_result = subprocess.run(["docker", "image", "inspect", image], capture_output=True)
        if local_result.returncode == 0:
            return True

        # If not found locally, try to pull the image
        print(f"üîÑ Image not found locally. Attempting to pull {image}...")
        pull_result = subprocess.run(["docker", "pull", image], capture_output=True)
        if pull_result.returncode == 0:
            return True

        return False

    except FileNotFoundError:
        print("‚ùå Docker is not installed or not in your PATH.")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Unexpected error while validating image: {e}")
        return False

def get_repo_from_url(repo_url):
    match = re.match(r"https?://github.com/([^/]+/[^/]+)(?:\\.git)?", repo_url)
    if match:
        return match.group(1)
    print("‚ùå Invalid GitHub repository URL format.")
    sys.exit(1)

def ensure_gh_authenticated():
    try:
        # Check if already authenticated
        subprocess.run(["gh", "auth", "status", "--hostname", "github.com"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # Confirm token can access the user
        result = subprocess.run(["gh", "api", "user"], capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError("Authenticated, but cannot access user profile.")

        # Try a harmless call that requires correct scopes
        probe = subprocess.run(
            ["gh", "api", "repos/octocat/Hello-World/code-scanning/alerts"],
            capture_output=True,
            text=True
        )

        if probe.returncode == 403 or "insufficient" in probe.stderr.lower():
            print("\nüîê GitHub token needs additional permissions.")
            print("   Attempting to refresh token with required scopes...")
            try:
                subprocess.run([
                    "gh", "auth", "refresh",
                    "-h", "github.com",
                    "-s", "security_events,admin:repo_hook"
                ], check=True)
                print("‚úÖ Token refreshed successfully.")
                return
            except subprocess.CalledProcessError:
                print("\n‚ö†Ô∏è Token refresh failed. Please re-authenticate with GitHub.")
                print("   This will open your browser to complete the process.")
                subprocess.run([
                    "gh", "auth", "login",
                    "-h", "github.com",
                    "-s", "security_events,admin:repo_hook",
                    "--web"
                ], check=True)
                return

    except subprocess.CalledProcessError:
        print("\nüîê GitHub authentication required.")
        print("   This will open your browser to complete the process.")
        subprocess.run([
            "gh", "auth", "login",
            "-h", "github.com",
            "-s", "security_events,admin:repo_hook",
            "--web"
        ], check=True)
        return

    except Exception as e:
        print(f"‚ùå Failed to validate GitHub auth: {e}")
        sys.exit(1)

def upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, temp_dir, verbose=False):
    from pathlib import Path
    import json, base64, tempfile, subprocess, gzip
    from datetime import datetime, timezone

    def is_valid_sarif(sarif_path):
        try:
            with open(sarif_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return all(key in data for key in ["version", "runs"])
        except Exception:
            return False

    def check_repo_access(repo):
        try:
            # Check if we can access the repository
            result = subprocess.run(
                ["gh", "api", f"repos/{repo}"],
                capture_output=True,
                text=True
            )
            if result.returncode != 0:
                print(f"‚ö†Ô∏è Cannot access repository {repo}. Skipping GHAS upload.")
                return False
            return True
        except subprocess.CalledProcessError:
            print(f"‚ö†Ô∏è Failed to check repository access. Skipping GHAS upload.")
            return False

    def upload_sarif(sarif_path, tool_name):
        if not sarif_path.exists() or not is_valid_sarif(sarif_path):
            print(f"‚ö†Ô∏è Skipping {tool_name} SARIF upload: file not found or invalid.")
            return False

        def run_git(cmd):
            try:
                return subprocess.check_output(cmd, cwd=temp_dir, text=True).strip()
            except subprocess.CalledProcessError:
                return None

        # Check if we're in a git repository
        if not Path(temp_dir) / ".git" / "config":
            print(f"‚ö†Ô∏è Not in a git repository. Skipping {tool_name} SARIF upload.")
            return False

        commit_sha = run_git(["git", "rev-parse", "HEAD"]) or "0000000000000000000000000000000000000000"
        ref = run_git(["git", "symbolic-ref", "-q", "HEAD"]) or "refs/heads/main"

        if commit_sha.startswith("fatal") or len(commit_sha) != 40:
            print(f"‚ö†Ô∏è Unable to retrieve a valid commit SHA from repo. Skipping {tool_name} SARIF upload.")
            return False

        if not ref.startswith("refs/"):
            print(f"‚ö†Ô∏è Invalid or missing Git ref. Skipping {tool_name} SARIF upload.")
            return False

        with open(sarif_path, "rb") as f_in:
            sarif_bytes = f_in.read()

        with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=".sarif.gz") as gz_file:
            with gzip.GzipFile(fileobj=gz_file, mode="wb") as gzip_out:
                gzip_out.write(sarif_bytes)
            gzipped_path = gz_file.name

        with open(gzipped_path, "rb") as gz_file:
            encoded_sarif = base64.b64encode(gz_file.read()).decode("utf-8")

        payload = {
            "commit_sha": commit_sha,
            "ref": ref,
            "sarif": encoded_sarif,
            "tool_name": tool_name,
            "checkout_uri": ".",
            "started_at": datetime.now(timezone.utc).isoformat()
        }

        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".json") as tmp:
            json.dump(payload, tmp)
            tmp_path = tmp.name

        try:
            result = subprocess.run([
                "gh", "api", f"repos/{repo}/code-scanning/sarifs",
                "--method", "POST",
                "--input", tmp_path
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                error_msg = result.stderr.strip()
                if "insufficient_scope" in error_msg.lower() or "not authorized" in error_msg.lower():
                    print(f"\n‚ö†Ô∏è GitHub token lacks required permissions for SARIF uploads.")
                    print("   Required scopes: security_events, admin:repo_hook")
                    print("\n   To fix this, run:")
                    print("   gh auth refresh -h github.com -s security_events,admin:repo_hook")
                    print("\n   Or visit: https://github.com/settings/tokens")
                    print("   to create a new token with the required scopes.")
                    return "insufficient_scope"
                elif "not found" in error_msg.lower():
                    print(f"‚ö†Ô∏è Repository {repo} not found or not accessible.")
                else:
                    print(f"‚ö†Ô∏è Failed to upload {tool_name} SARIF: {error_msg}")
                return False
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ö†Ô∏è Failed to upload {tool_name} SARIF: {e.stderr.strip() if e.stderr else str(e)}")
            return False

    # First check if we can access the repository
    if not check_repo_access(repo):
        return

    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)

    grype_sarif_path = scan_dir / "ghas_upload_grype.sarif"
    trivy_sarif_path = scan_dir / "ghas_upload_trivy.sarif"
    diff_sarif_path = scan_dir / "ghas_diff.sarif"

    print("\nüì¶ Generating SARIF files for GHAS upload...")

    uploads = []
    permission_error = False

    try:
        tool_progress("üîç", "Generating SARIF for Grype")
        subprocess.run(["grype", image, "-o", "sarif", "--file", str(grype_sarif_path)], check=True,
                       stdout=None if verbose else subprocess.DEVNULL,
                       stderr=None if verbose else subprocess.DEVNULL)
        result = upload_sarif(grype_sarif_path, "grype")
        if result == "insufficient_scope":
            permission_error = True
            print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
            return
        elif result:
            uploads.append("grype")
        tool_done()
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate SARIF with grype: {e}")
        tool_done()

    if not permission_error:
        try:
            tool_progress("üîç", "Generating SARIF for Trivy")
            subprocess.run(["trivy", "image", "-f", "sarif", "-o", str(trivy_sarif_path), image], check=True,
                           stdout=None if verbose else subprocess.DEVNULL,
                           stderr=None if verbose else subprocess.DEVNULL)
            result = upload_sarif(trivy_sarif_path, "trivy")
            if result == "insufficient_scope":
                permission_error = True
                print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
                return
            elif result:
                uploads.append("trivy")
            tool_done()
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to generate SARIF with trivy: {e}")
            tool_done()

    if not permission_error and diff_sarif_path.exists():
        tool_progress("üîç", "Generating diff SARIF")
        result = upload_sarif(diff_sarif_path, "scancompare")
        if result == "insufficient_scope":
            permission_error = True
            print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
            return
        elif result:
            uploads.append("diff (trivy vs grype)")
        tool_done()

    if uploads:
        print(f"\n‚úÖ Uploaded SARIF to GHAS: {', '.join(uploads)}")
        gh_link = f"https://github.com/{repo}/security/code-scanning"
        print(f"\033[4;36müîó View code scanning results on GHAS: {gh_link}\033[0m\n")
    elif not permission_error:
        print("\n‚ö†Ô∏è No SARIF files were uploaded to GHAS.")

def main(args):
    if args.uninstall:
        uninstall_scancompare()

    if args.version:
        print(f"üì¶ scancompare version {get_current_version()}")
        sys.exit(0)

    if args.update:
        check_latest_version(only_check=True)
        sys.exit(0)

    if not args.image and not args.repo_url:
        print("Usage: scancompare <image> [--repo-url <repo>, --keep-data, --verbose, --version, --update, --uninstall]")
        print("\nFlags:")
        print("  --repo-url <url>   Build and scan a Docker image from a GitHub repo containing a Dockerfile")
        print("  --keep-data        Keep all output and temporary data (HTML, SARIF, JSON, image, cloned repo)")
        print("  --verbose          Show detailed output (git clone, docker build, scan logs)")
        print("  --version          Show scancompare version")
        print("  --update           Check and apply latest scancompare update")
        print("  --uninstall        Uninstall scancompare CLI")
        sys.exit(1)

    check_latest_version()
    
    # Validate image exists before starting any scan process
    if args.image and not validate_image_exists(args.image):
        explain_exit(f"Docker image '{args.image}' could not be found locally or remotely.")

    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)
    summary_log = scan_dir / "scan_summary.log"
    summary_lines = [
        "\n" + "="*40,
        f"Scan started: {datetime.now().isoformat()}"
    ]

    temp_dir = None
    image_tag = None
    shown_ghas_hint = False
    scancompare_version = get_current_version()
    summary_lines.append(f"üî¢ scancompare version: {scancompare_version}")

    if args.repo_url:
        temp_dir = tempfile.mkdtemp()

        # Clone the repository with progress
        tool_progress("üì•", f"Cloning {args.repo_url}")
        git_clone_cmd = ["git", "clone", args.repo_url, temp_dir]
        if args.verbose:
            subprocess.run(git_clone_cmd, check=True)
        else:
            subprocess.run(git_clone_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        tool_done()  # Mark cloning as done

        args.cloned_repo_path = temp_dir

        # Find all Dockerfiles in the repository
        dockerfile_paths = []
        for root, _, files in os.walk(temp_dir):
            for file in files:
                if file.lower().startswith("dockerfile"):
                    dockerfile_paths.append(os.path.join(root, file))

        if not dockerfile_paths:
            print("‚ùå No Dockerfile found in the provided repository. Exiting.")
            sys.exit(1)  # Exit if no Dockerfile is found

        repo_name = args.repo_url.split('/')[-1].replace('.git', '')
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

        # Build images for each Dockerfile found
        built_images = []
        for dockerfile_path in dockerfile_paths:
            # Create a unique image tag based on the directory containing the Dockerfile
            dockerfile_dir = os.path.dirname(dockerfile_path)
            dir_name = os.path.basename(dockerfile_dir)
            image_tag = f"{repo_name}-{dir_name}-{timestamp}"

            # Docker image build
            tool_progress("üê≥", f"Building Docker image '{image_tag}' from {os.path.relpath(dockerfile_path, temp_dir)}")
            docker_build_cmd = ["docker", "build", "-t", image_tag, dockerfile_dir]
            try:
                if args.verbose:
                    subprocess.run(docker_build_cmd, check=True)
                else:
                    subprocess.run(docker_build_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                tool_done()  # Mark Docker build as done
                built_images.append(image_tag)
                print(f"üõ°Ô∏è Starting vulnerability scans against image: {image_tag}")
                summary_lines.append(f"üõ°Ô∏è Scanning built image: {image_tag}")

                # Scan this image with Trivy
                trivy_path = scan_dir / f"original_trivy_{dir_name}.json"
                tool_progress("üîπ", f"Scanning {image_tag} with Trivy")
                try:
                    if not run_scan("trivy", image_tag, trivy_path, verbose=args.verbose):
                        print(f"\n‚ö†Ô∏è Trivy scan failed for {image_tag}")
                        print(f"   To debug, run: trivy image --format json -o {trivy_path} {image_tag}")
                        continue
                    tool_done()
                except Exception as e:
                    print(f"\n‚ö†Ô∏è Trivy scan failed for {image_tag}")
                    print(f"   Error: {str(e)}")
                    print(f"   To debug, run: trivy image --format json -o {trivy_path} {image_tag}")
                    continue
                trivy_version = get_version("trivy")
                print(f"   üî¢ Trivy version: {trivy_version}")
                summary_lines.append(f"üî¢ Trivy version: {trivy_version}")
                print(f"   ‚úÖ Trivy scan saved to {trivy_path}")

                # Scan this image with Grype
                grype_path = scan_dir / f"original_grype_{dir_name}.json"
                tool_progress("üîπ", f"Scanning {image_tag} with Grype")
                if not run_scan("grype", image_tag, grype_path, verbose=args.verbose):
                    print(f"‚ö†Ô∏è Grype scan failed for {image_tag}")
                    continue
                tool_done()
                grype_version = get_version("grype")
                print(f"   üî¢ Grype version: {grype_version}")
                summary_lines.append(f"üî¢ Grype version: {grype_version}")
                print(f"   ‚úÖ Grype scan saved to {grype_path}")

                # Process scan results for this image
                trivy_data = extract_cves_with_severity(trivy_path)
                grype_data = extract_cves_with_severity(grype_path)

                shared, only_trivy, only_grype = display_summary(trivy_data, grype_data)

                print_cves_by_severity(f"Unique to Grype ({image_tag})", {c: grype_data[c] for c in only_grype})
                print_cves_by_severity(f"Unique to Trivy ({image_tag})", {c: trivy_data[c] for c in only_trivy})
                print_cves_by_severity(f"Shared CVEs ({image_tag})", {c: trivy_data.get(c, grype_data.get(c, "Unknown")) for c in shared})

                # Try GHAS upload first if we have a repository
                if args.repo_url:
                    repo = get_repo_from_url(args.repo_url)
                    print(f"\nüõ†Ô∏è Attempting to upload scan results to GitHub Advanced Security...")
                    try:
                        upload_to_ghas(image_tag, trivy_path, grype_path, shared, only_trivy, only_grype, repo, temp_dir, verbose=args.verbose)
                    except Exception as e:
                        print(f"‚ö†Ô∏è GHAS upload failed: {e}")
                        print("‚ÑπÔ∏è Continuing with local report generation...")

                # Generate HTML report for this image
                generate_html_report(image_tag, trivy_data, grype_data, shared, only_trivy, only_grype, trivy_version, grype_version)

            except subprocess.CalledProcessError as e:
                tool_done()  # Mark Docker build as done with error
                print(f"\n‚ö†Ô∏è Docker build failed for {os.path.relpath(dockerfile_path, temp_dir)}")
                print(f"   Error: {e.stderr.decode() if e.stderr else 'Unknown error'}")
                print(f"   To debug, run: docker build -t {image_tag} {dockerfile_dir}")
                print("   Continuing with other Dockerfiles if available...\n")
                continue  # Continue with next Dockerfile instead of exiting

        if not built_images:
            explain_exit("No images were successfully built and scanned.")

        # Use the first built image for cleanup
        image = built_images[0]
    else:
        image = args.image
        print(f"üõ°Ô∏è Starting vulnerability scans against image: {image}")
        summary_lines.append(f"üõ°Ô∏è Scanning image: {image}")

        # Scan the single image
        trivy_path = scan_dir / "original_trivy.json"
        grype_path = scan_dir / "original_grype.json"

        # Trivy scan
        tool_progress("üîπ", "Scanning with Trivy")
        try:
            if not run_scan("trivy", image, trivy_path, verbose=args.verbose):
                print(f"\n‚ö†Ô∏è Trivy scan failed for {image}")
                print(f"   To debug, run: trivy image --format json -o {trivy_path} {image}")
                explain_exit("Trivy scan failed or image not found.")
            tool_done()
        except Exception as e:
            print(f"\n‚ö†Ô∏è Trivy scan failed for {image}")
            print(f"   Error: {str(e)}")
            print(f"   To debug, run: trivy image --format json -o {trivy_path} {image}")
            explain_exit("Trivy scan failed or image not found.")
        trivy_version = get_version("trivy")
        print(f"   üî¢ Trivy version: {trivy_version}")
        summary_lines.append(f"üî¢ Trivy version: {trivy_version}")
        print(f"   ‚úÖ Trivy scan saved to {trivy_path}")

        # Grype scan
        tool_progress("üîπ", "Scanning with Grype")
        if not run_scan("grype", image, grype_path, verbose=args.verbose):
            explain_exit("Grype scan failed or image not found.")
        tool_done()
        grype_version = get_version("grype")
        print(f"   üî¢ Grype version: {grype_version}")
        summary_lines.append(f"üî¢ Grype version: {grype_version}")
        print(f"   ‚úÖ Grype scan saved to {grype_path}")

        trivy_data = extract_cves_with_severity(trivy_path)
        grype_data = extract_cves_with_severity(grype_path)

        shared, only_trivy, only_grype = display_summary(trivy_data, grype_data)

        print_cves_by_severity("Unique to Grype", {c: grype_data[c] for c in only_grype})
        print_cves_by_severity("Unique to Trivy", {c: trivy_data[c] for c in only_trivy})
        print_cves_by_severity("Shared CVEs", {c: trivy_data.get(c, grype_data.get(c, "Unknown")) for c in shared})

        # Try GHAS upload first if we have a repository
        if args.repo_url:
            repo = get_repo_from_url(args.repo_url)
            print(f"\nüõ†Ô∏è Attempting to upload scan results to GitHub Advanced Security...")
            try:
                upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, temp_dir, verbose=args.verbose)
            except Exception as e:
                print(f"‚ö†Ô∏è GHAS upload failed: {e}")
                print("‚ÑπÔ∏è Continuing with local report generation...")

        # Generate HTML report
        generate_html_report(image, trivy_data, grype_data, shared, only_trivy, only_grype, trivy_version, grype_version)

    # Cleanup: Show progress
    print("üßπ Cleaning up old scan artifacts...")
    summary_lines.append("üßπ Cleaned up old scan artifacts.")
    for path in scan_dir.glob("*.*"):
        if not path.name.endswith(".html"):
            path.unlink()

    if args.keep_data:
        print("üõë Cleanup skipped due to --keep-data. Keeping Docker images, cloned repo, and scan artifacts.")
        summary_lines.append("üõë Cleanup skipped due to --keep-data. Keeping Docker images, cloned repo, and scan artifacts.")
    else:
        # Clean up Docker images
        if args.repo_url and built_images:
            for image_tag in built_images:
                print(f"üßº Removing temporary Docker image '{image_tag}'...")
                subprocess.run(["docker", "rmi", "-f", image_tag], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        elif image:
            print(f"üßº Removing temporary Docker image '{image}'...")
            subprocess.run(["docker", "rmi", "-f", image], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # Clean up cloned repository if it exists
        if temp_dir and os.path.isdir(temp_dir):
            print(f"üßπ Removing temporary cloned repository from '{temp_dir}'...")
            shutil.rmtree(temp_dir)

    if summary_lines:
        with open(summary_log, "a") as log_file:
            for line in summary_lines:
                log_file.write(line + "\n")
        print(f"üìù Summary written to: {summary_log}")

if __name__ == "__main__":
    args = handle_cli_args()
    main(args)
