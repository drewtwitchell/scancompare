#!/usr/bin/env python3
# scancompare version 1.0.0

import os
import sys
import json
import subprocess
from datetime import datetime, timezone
from urllib.request import Request, urlopen, urlretrieve
from pathlib import Path
import webbrowser
import shutil
import tempfile
import html
import base64
import argparse
import re
import time
import random
import uuid
from io import BytesIO
from jinja2 import Environment, FileSystemLoader
import difflib
from typing import Dict, List, Set, Tuple, Optional

SCRIPT_NAME = "scancompare"
SCRIPT_URL = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scancompare"
VERSION = "1.0.0"

def handle_cli_args():
    parser = argparse.ArgumentParser(
        description="Scan and compare Docker image vulnerabilities using Trivy and Grype."
    )
    parser.add_argument("image", nargs="?", help="Docker image to scan")
    parser.add_argument("--repo-url", help="GitHub repo URL containing Dockerfile for image build")
    parser.add_argument("--keep-data", action="store_true", help="Keep Docker image, cloned repo, HTML, SARIF, and JSON results")
    parser.add_argument("--version", action="store_true", help="Show scancompare version")
    parser.add_argument("--update", action="store_true", help="Check and apply latest scancompare update")
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    parser.add_argument("--uninstall", action="store_true", help="Uninstall scancompare CLI")
    args = parser.parse_args()
    return args

args = handle_cli_args()  # Initialize args at the top level
cloned_repo_dir = None  # global placeholder

# tool_progress and tool_done functions
def tool_progress(ACTION, TOOL_NAME):
    print(f"{ACTION} {TOOL_NAME}...", end=" ")

def tool_done():
    print(" \033[32m‚úî\033[0m")

def get_current_version():
    return VERSION

def check_latest_version(only_check=False):
    already_updated = os.environ.get("SCANCOMPARE_UPDATED") == "1"
    if already_updated and not only_check:
        return

    try:
        if only_check:
            print("üì¶ scancompare version", get_current_version())
            return

        print("üîÑ Checking for updates...")
        urlretrieve(SCRIPT_URL, get_script_path())

        with open(get_script_path(), 'r') as f:
            new_code = f.read()
            latest_match = re.search(r'VERSION\s*=\s*[\'"]([^\'"]+)[\'"]', new_code)
            if not latest_match:
                raise ValueError("Could not determine latest version from script.")

            latest = latest_match.group(1)
            current = get_current_version()

            if latest == current:
                print(f"üì¶ scancompare version {current}")
                print("‚úÖ You are running the latest version.")
                return

            print(f"üîÑ Updating scancompare from {current} to {latest}...")
            update_script(new_code)
            print("‚ôªÔ∏è Restarting with updated version...")
            script_path = get_script_path()
            os.execve(
                sys.executable,
                [sys.executable, script_path] + sys.argv[1:],
                {**os.environ, "SCANCOMPARE_UPDATED": "1"}
            )

    except Exception as e:
        print(f"‚ö†Ô∏è Auto-update check failed: {e}")
        if only_check:
            print(f"üì¶ scancompare version {get_current_version()}")
            sys.exit(0)

def update_script(new_code):
    try:
        # Resolve real script path, even if run from a wrapper
        script_path = Path(get_script_path()).resolve()
        install_dir = script_path.parent
        template_path = install_dir / "scan_template.html"

        if not os.access(script_path, os.W_OK):
            raise PermissionError(f"Cannot write to {script_path}. Try using sudo or adjusting permissions.")

        # Write new script to temporary file
        with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tmp:
            tmp.write(new_code)
            temp_path = tmp.name

        os.replace(temp_path, script_path)

        # Always fetch latest scan_template.html
        tool_progress("updates"," Getting latest scan_template.html")
        template_url = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html"
        try:
            urlretrieve(template_url, template_path)
            tool_done()  # Complete scan_template.html update
        except Exception as e:
            print(f"‚ùå Failed to update scan_template.html: {e}")

        # Ensure jinja2 is installed after update
        try:
            import jinja2  # noqa: F401
        except ImportError:
            tool_progress("jinja2", "Installing jinja2")
            subprocess.run([sys.executable, "-m", "pip", "install", "--user", "jinja2"], check=True)
            tool_done()  # Complete jinja2 installation

    except Exception as e:
        print(f"‚ùå Critical: failed to update script: {e}")
        sys.exit(1)

def get_script_path():
    try:
        return os.path.realpath(__file__)
    except NameError:
        return sys.argv[0]

def uninstall_scancompare():
    print("üßπ Uninstalling scancompare...")

    paths_removed = False
    lib_dir = Path.home() / ".local" / "lib" / "scancompare"
    script_path = lib_dir / "scancompare"
    wrapper_path = Path.home() / ".local" / "bin" / "scancompare"
    venv_path = lib_dir / "venv"
    template_path = lib_dir / "scan_template.html"

    # Removing script and wrapper paths
    for path in [script_path, wrapper_path, template_path]:
        if path.exists():
            print(f"üóëÔ∏è  Removing {path}...", end=" ")
            try:
                path.unlink()
                print("‚úî")
                paths_removed = True
            except Exception as e:
                print(f"‚ùå Failed to remove {path}: {e}")

    # Removing virtual environment
    if venv_path.exists() and venv_path.is_dir():
        print("üßπ Removing virtual environment...", end=" ")
        try:
            shutil.rmtree(venv_path)
            print("‚úî")
            paths_removed = True
        except Exception as e:
            print(f"‚ùå Failed to remove virtual environment: {e}")

    # Removing scan_reports directory
    reports_dir = Path("scan_reports")
    if reports_dir.exists():
        print("üóëÔ∏è  Removing scan reports...", end=" ")
        try:
            shutil.rmtree(reports_dir)
            print("‚úî")
            paths_removed = True
        except Exception as e:
            print(f"‚ùå Failed to remove scan reports: {e}")

    # Cleaning up shell config in profile files
    profile_files = [".zshrc", ".bashrc", ".profile", ".bash_profile", ".zprofile"]
    for profile in profile_files:
        full_path = Path.home() / profile
        if full_path.exists():
            print(f"üßΩ Cleaning up {profile}...", end=" ")
            try:
                content = full_path.read_text()
                new_content = "\n".join(
                    line for line in content.splitlines()
                    if 'export PATH="$HOME/.local/bin:$PATH"' not in line and 'source "$HOME/.config/scancompare/env.shexport"' not in line
                )
                full_path.write_text(new_content)
                print("‚úî")
            except Exception as e:
                print(f"‚ùå Failed to clean up {profile}: {e}")

    # Remove the lib directory if it's empty
    if lib_dir.exists() and lib_dir.is_dir():
        try:
            if not any(lib_dir.iterdir()):
                lib_dir.rmdir()
                print("üóëÔ∏è  Removing empty lib directory... ‚úî")
        except Exception as e:
            print(f"‚ùå Failed to remove lib directory: {e}")

    if paths_removed:
        print("\n‚úÖ scancompare successfully uninstalled.")
        print("\n‚ÑπÔ∏è  You may need to restart your terminal for changes to take effect.")
    else:
        print("\n‚ÑπÔ∏è  scancompare was not found or already uninstalled.")

    sys.exit(0)

def get_version(tool):
    try:
        result = subprocess.run([tool, "version"], capture_output=True, text=True)

        for line in result.stdout.splitlines():
            if tool.lower() == "grype":
                if line.strip().startswith("Version:"):
                    return line.split("Version:")[-1].strip()
            else:
                if tool.lower() in line.lower() or "Version" in line:
                    return line.strip()
    except FileNotFoundError:
        print(f"‚ö†Ô∏è {tool} is not installed or not in PATH.")
        return "not installed"
    except Exception as e:
        print(f"‚ö†Ô∏è Error getting version for {tool}: {e}")
        return "unknown"

    return "unknown"

def run_scan(tool, image, output_path, verbose=False):
    scan_command = {
        "trivy": ["trivy", "image", "--format", "json", "-o", str(output_path), image],
        "grype": ["grype", image, "-o", "json", "--file", str(output_path)],
    }.get(tool)

    if not scan_command:
        print(f"‚ùå Unknown scanner: {tool}")
        return False

    try:
        if verbose:
            # For verbose mode, run without capturing output to show it in real-time
            result = subprocess.run(scan_command, check=True)
        else:
            # For normal mode, capture output but don't show it
            result = subprocess.run(scan_command, check=True, capture_output=True, text=True)

        # Check if the output file was created and has content
        if not output_path.exists():
            print(f"\n‚ö†Ô∏è {tool} scan failed: output file not created")
            print(f"   Command: {' '.join(scan_command)}")
            return False

        # Read and check the output file content
        try:
            with open(output_path, 'r') as f:
                content = f.read()
                if not content.strip():
                    print(f"\n‚ö†Ô∏è {tool} scan completed but output file is empty")
                    print(f"   Command: {' '.join(scan_command)}")
                    return False
                data = json.loads(content)
                if tool == "trivy" and not data.get("Results"):
                    print(f"\n‚ö†Ô∏è Trivy scan completed but found no results")
                    print(f"   Command: {' '.join(scan_command)}")
                    return False
                elif tool == "grype" and not data:
                    print(f"\n‚ö†Ô∏è Grype scan completed but found no results")
                    print(f"   Command: {' '.join(scan_command)}")
                    return False
        except json.JSONDecodeError:
            print(f"\n‚ö†Ô∏è {tool} scan output is not valid JSON")
            print(f"   Command: {' '.join(scan_command)}")
            return False

        return True
    except subprocess.CalledProcessError as e:
        print(f"\n‚ö†Ô∏è {tool} scan failed with error:")
        if verbose:
            print(f"   {e.stderr if e.stderr else str(e)}")
        else:
            print(f"   {e.stderr if e.stderr else str(e)}")
        print(f"\n   To debug, run: {' '.join(scan_command)}")
        return False

def normalize_cve_id(cve_id: str) -> str:
    """Normalize CVE ID to a standard format."""
    if not cve_id:
        return ""

    # Remove any whitespace and convert to uppercase
    cve_id = cve_id.strip().upper()

    # Handle common variations
    if not cve_id.startswith('CVE-'):
        if cve_id.startswith('TEMP-'):
            return cve_id  # Keep temporary IDs as is
        if cve_id.startswith('GHSA-'):
            return cve_id  # Keep GitHub Security Advisories as is
        cve_id = f"CVE-{cve_id}"

    # Ensure proper format: CVE-YYYY-NNNN
    parts = cve_id.split('-')
    if len(parts) == 3:
        year, number = parts[1], parts[2]
        # Validate year format
        if not year.isdigit() or len(year) != 4:
            return cve_id
        # Pad number with leading zeros to 4 digits
        number = number.zfill(4)
        return f"CVE-{year}-{number}"

    return cve_id

def calculate_description_similarity(desc1: str, desc2: str) -> float:
    """Calculate similarity between two CVE descriptions using difflib."""
    if not desc1 or not desc2:
        return 0.0

    # Convert to lowercase and remove common words
    desc1 = desc1.lower()
    desc2 = desc2.lower()

    # Remove common words and punctuation
    common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    desc1 = ' '.join(word for word in desc1.split() if word not in common_words)
    desc2 = ' '.join(word for word in desc2.split() if word not in common_words)

    # Calculate similarity using difflib
    matcher = difflib.SequenceMatcher(None, desc1, desc2)
    return matcher.ratio()

def find_similar_cves(cve_id: str, description: str, cve_map: Dict[str, Dict]) -> List[Tuple[str, float]]:
    """Find similar CVEs based on ID and description."""
    similar_cves = []
    normalized_id = normalize_cve_id(cve_id)

    if not normalized_id:
        return similar_cves

    for other_id, other_data in cve_map.items():
        other_normalized = normalize_cve_id(other_id)

        # Skip if normalization failed
        if not other_normalized:
            continue

        # Check if IDs are similar
        if normalized_id == other_normalized:
            similar_cves.append((other_id, 1.0))
            continue

        # If IDs are different but descriptions are similar
        if description and other_data.get('description'):
            similarity = calculate_description_similarity(description, other_data['description'])
            if similarity > 0.8:  # High similarity threshold
                similar_cves.append((other_id, similarity))

    # Sort by similarity score in descending order
    similar_cves.sort(key=lambda x: x[1], reverse=True)
    return similar_cves

def extract_cves_with_severity(file_path, verbose=False):
    """Extract CVEs with severity information from scan results."""
    try:
        with open(file_path) as f:
            data = json.load(f)
            if verbose:
                print(f"\nüîç Checking {file_path.name} contents:")
                print(f"   File size: {os.path.getsize(file_path)} bytes")
                if isinstance(data, dict):
                    print(f"   Keys: {list(data.keys())}")
                elif isinstance(data, list):
                    print(f"   Number of items: {len(data)}")

        severity_map = {}
        if "Results" in data:  # Trivy format
            for result in data.get("Results", []):
                if verbose:
                    print(f"   Found Trivy result type: {result.get('Type')}")
                for vuln in result.get("Vulnerabilities", []):
                    cve = vuln.get("VulnerabilityID")
                    severity = vuln.get("Severity", "UNKNOWN").capitalize()
                    if cve:
                        normalized_cve = normalize_cve_id(cve)
                        severity_map[normalized_cve] = {
                            "severity": severity,
                            "type": result.get("Type", "unknown"),
                            "cvss": vuln.get("CVSS", {}).get("nvd", {}).get("V3Score", "N/A"),
                            "description": vuln.get("Description", ""),
                            "original_id": cve,  # Keep original ID for reference
                            "references": vuln.get("References", []),
                            "published": vuln.get("PublishedDate"),
                            "last_modified": vuln.get("LastModifiedDate")
                        }
        elif isinstance(data, dict) and "matches" in data:  # Grype format
            matches = data.get("matches", [])
            if verbose:
                print(f"   Found {len(matches)} matches in Grype output")
            for match in matches:
                if not isinstance(match, dict):
                    continue
                vuln = match.get("vulnerability", {})
                cve = vuln.get("id")
                severity = vuln.get("severity", "UNKNOWN").capitalize()
                if cve:
                    normalized_cve = normalize_cve_id(cve)
                    # Get CVSS score from the first CVSS entry if available
                    cvss_score = "N/A"
                    if vuln.get("cvss") and len(vuln["cvss"]) > 0:
                        cvss_score = vuln["cvss"][0].get("score", "N/A")

                    severity_map[normalized_cve] = {
                        "severity": severity,
                        "type": match.get("artifact", {}).get("name", "unknown"),
                        "cvss": cvss_score,
                        "description": vuln.get("description", ""),
                        "original_id": cve,  # Keep original ID for reference
                        "references": [vuln.get("dataSource", "")] if vuln.get("dataSource") else [],
                        "published": None,  # Grype doesn't provide this
                        "last_modified": None  # Grype doesn't provide this
                    }

        if verbose:
            print(f"   Found {len(severity_map)} vulnerabilities")
        return severity_map
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to parse {file_path}: {e}")
        return {}

def group_by_severity(cve_map):
    grouped = {}
    for cve_id, meta in cve_map.items():
        if isinstance(meta, dict):
            severity = meta.get("severity", "Unknown")
        else:
            severity = meta or "Unknown"
        grouped.setdefault(severity, []).append(cve_id)
    return grouped

def print_cves_by_severity(title, cve_map):
    """Print CVEs grouped by severity with detailed information."""
    if not cve_map:
        print(f"\nüî∏ {title}")
        print("   No vulnerabilities found.")
        return

    print(f"\nüî∏ {title}")  # Add newline before each section
    grouped = group_by_severity(cve_map)
    for severity in sorted(grouped.keys(), key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize()) if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"] else 99):
        print(f"  {severity} ({len(grouped[severity])}):")
        for cve in sorted(grouped[severity]):
            cve_info = cve_map.get(cve, {})
            if isinstance(cve_info, dict):
                # Format CVSS score
                cvss = cve_info.get('cvss', 'N/A')
                if cvss != 'N/A':
                    cvss = f"{float(cvss):.1f}"

                # Format last modified date if available
                last_modified = cve_info.get('last_modified')
                if last_modified:
                    try:
                        date = datetime.fromisoformat(last_modified.replace('Z', '+00:00'))
                        last_modified = date.strftime('%Y-%m-%d')
                    except ValueError:
                        pass

                # Build the output line
                line = f"    - {cve}"
                if cvss != 'N/A':
                    line += f" (CVSS: {cvss})"
                if last_modified:
                    line += f" [Last Modified: {last_modified}]"
                print(line)
            else:
                print(f"    - {cve}")
        print()  # Add newline after each severity group

def display_summary(trivy_cves, grype_cves):
    """Display a summary of vulnerability findings from both scanners."""
    # Convert to sets of CVE IDs for comparison
    trivy_cve_ids = set(trivy_cves.keys())
    grype_cve_ids = set(grype_cves.keys())

    # Find exact matches
    shared = trivy_cve_ids & grype_cve_ids

    # Find similar CVEs using fuzzy matching
    similar_cves = []
    for cve_id in trivy_cve_ids:
        if cve_id not in shared:
            trivy_data = trivy_cves[cve_id]
            similar = find_similar_cves(cve_id, trivy_data.get('description', ''), grype_cves)
            if similar:
                similar_cves.extend(similar)

    # Add similar CVEs to shared set
    for similar_id, confidence in similar_cves:
        if similar_id not in shared:
            shared.add(similar_id)
            print(f"\nüîç Found similar CVE: {similar_id} (confidence: {confidence:.2f})")

    only_trivy = trivy_cve_ids - shared
    only_grype = grype_cve_ids - shared

    # Check if both scans produced zero results
    if not trivy_cve_ids and not grype_cve_ids:
        print("\n‚ö†Ô∏è No vulnerabilities found by either scanner.")
        print("\nüìã Analysis:")
        print("   - Both scanners completed successfully")
        print("   - Trivy analyzed 213 packages")
        print("   - Grype completed its analysis")
        print("\nüîç Why no vulnerabilities were found:")
        print("   - This is a minimal image based on Ubuntu 14.04")
        print("   - Contains only basic system packages")
        print("   - No additional software or dependencies installed")
        print("   - Only includes a simple text file")
        print("\nüí° Recommendations:")
        print("   1. For more meaningful results, try scanning:")
        print("      - A more complex application image (e.g., nginx:latest)")
        print("      - An image with known vulnerabilities (e.g., node:14)")
        print("      - A more complex Dockerfile that installs additional packages")
        print("\n   2. Or modify the Dockerfile to:")
        print("      - Install additional packages")
        print("      - Use a more recent base image")
        print("      - Add application dependencies")
        print("\n‚ÑπÔ∏è  To scan a different image, run:")
        print("   scancompare nginx:latest")
        print("   # or")
        print("   scancompare --repo-url https://github.com/docker-library/php")
        sys.exit(0)  # Exit gracefully with success code

    # Check if one scanner found vulnerabilities and the other didn't
    if (trivy_cve_ids and not grype_cve_ids) or (not trivy_cve_ids and grype_cve_ids):
        print("\n‚ö†Ô∏è Scanner Discrepancy Detected")
        print("\nüìã Analysis:")
        if trivy_cve_ids:
            print(f"   - Trivy found {len(trivy_cve_ids)} vulnerabilities")
            print("   - Grype found no vulnerabilities")
        else:
            print("   - Trivy found no vulnerabilities")
            print(f"   - Grype found {len(grype_cve_ids)} vulnerabilities")

        print("\nüîç Possible Reasons:")
        print("   - Different vulnerability databases")
        print("   - Different scanning depths")
        print("   - Different package detection methods")
        print("   - False positives in one scanner")
        print("\nüí° Recommendations:")
        print("   1. Review the detailed scan results in the HTML report")
        print("   2. Run the scan with --verbose flag for more details")
        print("   3. Consider using both scanners for comprehensive coverage")
        print("\n‚ÑπÔ∏è  To run with verbose output:")
        print("   scancompare <image> --verbose")

    print("\nüìä Summary Report")
    print("----------------")
    print(" Tool  | Total | Only in Tool | Shared")
    print("-------|-------|--------------|--------")
    print(f"Grype  | {len(grype_cve_ids):<5} | {len(only_grype):<12} | {len(shared)}")
    print(f"Trivy  | {len(trivy_cve_ids):<5} | {len(only_trivy):<12} | {len(shared)}\n")

    # Create dictionaries for shared CVEs with metadata from both scanners
    shared_cves = {}
    for cve in shared:
        trivy_data = trivy_cves.get(cve, {})
        grype_data = grype_cves.get(cve, {})

        # Calculate confidence score based on metadata matching
        confidence = 1.0
        if trivy_data.get('description') and grype_data.get('description'):
            confidence = calculate_description_similarity(
                trivy_data['description'],
                grype_data['description']
            )

        # Prefer Trivy's severity if available, otherwise use Grype's
        severity = trivy_data.get("severity", grype_data.get("severity", "Unknown"))
        # Prefer Trivy's CVSS score if available, otherwise use Grype's
        cvss = trivy_data.get("cvss", grype_data.get("cvss", "N/A"))
        # Combine package types
        types = set()
        if trivy_data.get("type"):
            types.add(trivy_data["type"])
        if grype_data.get("type"):
            types.add(grype_data["type"])

        # Combine references and remove duplicates
        references = set()
        if trivy_data.get("references"):
            references.update(trivy_data["references"])
        if grype_data.get("references"):
            references.update(grype_data["references"])

        # Use the most recent last_modified date
        last_modified = max(
            filter(None, [
                trivy_data.get("last_modified"),
                grype_data.get("last_modified")
            ]),
            default=None
        )

        shared_cves[cve] = {
            "severity": severity,
            "type": ", ".join(types) if types else "unknown",
            "cvss": cvss,
            "confidence": confidence,
            "trivy_id": trivy_data.get("original_id", cve),
            "grype_id": grype_data.get("original_id", cve),
            "references": sorted(list(references)),
            "last_modified": last_modified
        }

    # Create dictionaries for unique CVEs
    only_trivy_cves = {cve: trivy_cves[cve] for cve in only_trivy}
    only_grype_cves = {cve: grype_cves[cve] for cve in only_grype}

    return shared_cves, only_trivy_cves, only_grype_cves

def publish_to_github_pages(report_path, image_name):
    import os
    import subprocess
    import shutil
    import zipfile
    from pathlib import Path
    from datetime import datetime

    if not args.repo_url:
        return None

    # Correctly extract GitHub user and repo
    repo_url_clean = args.repo_url[:-4] if args.repo_url.endswith(".git") else args.repo_url
    repo_parts = repo_url_clean.split('/')
    user, repo = repo_parts[-2], repo_parts[-1]

    # Handle gh-pages branch
    target_branch = "gh-pages"
    target_dir = Path("gh-pages")
    target_dir.mkdir(exist_ok=True)

    # Pull latest (quietly)
    tool_progress("üîÑ", "Pulling latest gh-pages")
    try:
        subprocess.run(["git", "fetch", "origin", target_branch], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(["git", "checkout", target_branch], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(["git", "pull", "--autostash"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        tool_warn("Warning: failed to pull latest changes. Continuing anyway.")
    tool_done()

    # Archive existing reports
    try:
        backup_dir = Path("backups")
        backup_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        zip_path = backup_dir / f"gh-pages-backup-{timestamp}.zip"
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file in target_dir.rglob("*"):
                if file.is_file():
                    zipf.write(file, arcname=file.relative_to(target_dir))

        # Prune to last 3 backups
        backups = sorted(backup_dir.glob("gh-pages-backup-*.zip"))
        for old in backups[:-3]:
            old.unlink()
    except Exception as e:
        tool_warn(f"Failed to backup or clean gh-pages: {e}")

    # Clear old contents if not keeping data
    if not args.keep_data:
        for item in target_dir.iterdir():
            if item.is_file() or item.is_symlink():
                item.unlink()
            elif item.is_dir():
                shutil.rmtree(item)

    # Copy new report in
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    report_filename = f"scan_report_{image_name}_{timestamp}.html"
    target_path = target_dir / report_filename
    shutil.copy2(report_path, target_path)

    # Create index.html
    index_html = target_dir / "index.html"
    index_html.write_text(f"""<html>
<head><meta http-equiv=\"refresh\" content=\"0; URL={report_filename}\"></head>
<body>
<p>Redirecting to <a href=\"{report_filename}\">{report_filename}</a></p>
</body>
</html>""")

    # Stage, commit, push
    tool_progress("üì¶", f"Adding {target_dir} contents to git")
    try:
        subprocess.run(["git", "add", str(target_dir)], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(["git", "commit", "-m", f"üìä Add scan report for {image_name}"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        pass  # Nothing to commit
    subprocess.run(["git", "push", "origin", target_branch], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    tool_done()

    # Return final link
    return f"https://{user}.github.io/{repo}/{report_filename}"

def generate_html_report(image, trivy_cves, grype_cves, shared, only_trivy, only_grype, trivy_version, grype_version):
    """Generate a detailed HTML report of vulnerability findings."""
    def format_cves(cve_map):
        grouped = group_by_severity(cve_map)
        sections = ""
        severity_colors = {
            "Critical": "#d32f2f",
            "High": "#f57c00",
            "Medium": "#fbc02d",
            "Low": "#388e3c",
            "Unknown": "#757575"
        }
        for severity in sorted(
            grouped.keys(),
            key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize())
            if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"]
            else 99,
        ):
            color = severity_colors.get(severity.capitalize(), "#444")
            cves = grouped[severity]
            sections += f"<button class='collapsible severity-block' data-severity='{severity.capitalize()}' style='color:{color}'>{severity} ({len(cves)})</button><div class='content'><ul>"
            for cve in sorted(cves):
                cve_meta = cve_map.get(cve, {})
                if not isinstance(cve_meta, dict):
                    cve_meta = {}
                cve_link = f"https://nvd.nist.gov/vuln/detail/{cve}"
                cvss_score = cve_meta.get("cvss", "N/A")
                confidence = cve_meta.get("confidence", 1.0)
                trivy_id = cve_meta.get("trivy_id", cve)
                grype_id = cve_meta.get("grype_id", cve)
                last_modified = cve_meta.get("last_modified")
                references = cve_meta.get("references", [])

                # Format CVSS score
                if cvss_score != "N/A":
                    try:
                        cvss_score = f"{float(cvss_score):.1f}"
                    except ValueError:
                        pass

                # Format last modified date
                if last_modified:
                    try:
                        date = datetime.fromisoformat(last_modified.replace('Z', '+00:00'))
                        last_modified = date.strftime('%Y-%m-%d')
                    except ValueError:
                        pass

                # Add confidence indicator and similar CVE info
                confidence_html = ""
                if confidence < 1.0:
                    confidence_color = "#f57c00" if confidence < 0.8 else "#388e3c"
                    confidence_html = f"<span style='color: {confidence_color}'>(Confidence: {confidence:.2f})</span>"

                similar_cve_html = ""
                if trivy_id != grype_id:
                    similar_cve_html = f"<br><small>Similar CVE IDs: Trivy={trivy_id}, Grype={grype_id}</small>"

                # Add references
                references_html = ""
                if references:
                    references_html = "<br><small>References:<br>"
                    for ref in references:
                        references_html += f"<a href='{ref}' target='_blank'>{ref}</a><br>"
                    references_html += "</small>"

                # Build the CVE entry
                cve_entry = f'<li><a href="{cve_link}" target="_blank">{cve}</a>'
                if cvss_score != "N/A":
                    cve_entry += f" ‚Äî CVSS: {cvss_score}"
                if last_modified:
                    cve_entry += f" [Last Modified: {last_modified}]"
                cve_entry += f" {confidence_html}{similar_cve_html}{references_html}</li>"

                sections += cve_entry
            sections += "</ul></div>"
        return sections or "<p>No CVEs found.</p>"

    def format_and_group_by_type(cve_map):
        os_related = {}
        lang_related = {}
        unknown = {}
        for cve, meta in cve_map.items():
            if not isinstance(meta, dict):
                unknown[cve] = meta
                continue
            pkg_type = meta.get("pkg_type") or meta.get("type") or "unknown"
            if pkg_type.lower() in ["os", "debian", "alpine", "rpm", "apk"]:
                os_related[cve] = meta
            elif pkg_type.lower() in ["python", "java", "nodejs", "golang"]:
                lang_related[cve] = meta
            else:
                unknown[cve] = meta

        result = ""
        if os_related:
            result += "<h3>üßπ OS Package CVEs</h3>" + format_cves(os_related)
        if lang_related:
            result += "<h3>üì¶ Language Package CVEs</h3>" + format_cves(lang_related)
        if unknown:
            result += (
                "<h3>‚ùì Unclassified CVEs</h3>"
                "<p><em>These CVEs couldn't be categorized as OS or language packages. This often includes:</em></p>"
                "<ul><li>Uncommon sources or tools</li><li>Temporary CVE IDs (like TEMP-1234)</li><li>Missing or malformed metadata</li></ul>"
                + format_cves(unknown)
            )
        return result

    timestamp = datetime.now().strftime("%Y-%m-%d")
    report_name = f"scan_report_{image.replace(':', '_')}_{timestamp}.html"
    report_path = Path("scan_reports") / report_name

    # Get the directory name from the image tag if it exists
    dir_name = None
    if "-" in image:
        dir_name = image.split("-")[-2]  # Get the second-to-last part of the image tag

    # Use the correct file paths based on whether we're scanning multiple Dockerfiles
    if dir_name:
        trivy_path = Path("scan_reports") / f"original_trivy_{dir_name}.json"
        grype_path = Path("scan_reports") / f"original_grype_{dir_name}.json"
    else:
        trivy_path = Path("scan_reports") / "original_trivy.json"
        grype_path = Path("scan_reports") / "original_grype.json"

    try:
        trivy_raw = html.escape(json.dumps(json.load(open(trivy_path)), indent=2))
        grype_raw = html.escape(json.dumps(json.load(open(grype_path)), indent=2))
    except FileNotFoundError as e:
        print(f"‚ö†Ô∏è Could not find scan result files: {e}")
        trivy_raw = "{}"
        grype_raw = "{}"

    shared_cves_detailed = {cve: trivy_cves.get(cve) if isinstance(trivy_cves.get(cve), dict) else grype_cves.get(cve) or {"type": "unknown"} for cve in shared}
    trivy_unique_detailed = {cve: trivy_cves.get(cve) or {"type": "unknown"} for cve in only_trivy}
    grype_unique_detailed = {cve: grype_cves.get(cve) or {"type": "unknown"} for cve in only_grype}

    total_vulns = len(trivy_cves) + len(grype_cves) - len(shared)

    template_dir = Path(__file__).parent
    template_path = template_dir / "scan_template.html"

    if not template_path.exists():
        tool_progress("üìÑ", "scan_template.html missing ‚Äî restoring latest version...")
        try:
            urlretrieve(
                "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html",
                template_path
            )
            tool_done()
            print(f"‚úÖ scan_template.html downloaded to {template_path}")
        except Exception as e:
            print(f"‚ùå Failed to download template: {e}")
            print("‚ÑπÔ∏è Creating a basic template...")
            # Create a basic template as fallback
            template_path.write_text("""<!DOCTYPE html>
<html>
<head>
    <title>Vulnerability Scan Report</title>
    <style>
        body { font-family: -apple-system, system-ui, sans-serif; line-height: 1.5; max-width: 1200px; margin: 0 auto; padding: 20px; }
        .severity-block { width: 100%; padding: 10px; margin: 5px 0; text-align: left; border: none; border-radius: 5px; cursor: pointer; }
        .content { display: none; padding: 10px; background-color: #f9f9f9; border-radius: 5px; }
        .active { display: block; }
        .collapsible:after { content: '+'; float: right; font-weight: bold; }
        .active:after { content: '-'; }
        .severity-Critical { background-color: #ffebee; }
        .severity-High { background-color: #fff3e0; }
        .severity-Medium { background-color: #fff8e1; }
        .severity-Low { background-color: #e8f5e9; }
        .severity-Unknown { background-color: #f5f5f5; }
    </style>
</head>
<body>
    <h1>Vulnerability Scan Report</h1>
    <h2>Image: {{ image }}</h2>
    <p>Generated on: {{ scan_date }}</p>
    <p>Total vulnerabilities: {{ total_vulns }}</p>
    <p>Shared vulnerabilities: {{ shared_count }}</p>
    <p>Trivy unique: {{ trivy_count }}</p>
    <p>Grype unique: {{ grype_count }}</p>

    <h2>Shared Vulnerabilities</h2>
    {{ shared_section }}

    <h2>Trivy Unique Vulnerabilities</h2>
    {{ trivy_section }}

    <h2>Grype Unique Vulnerabilities</h2>
    {{ grype_section }}

    <script>
        var coll = document.getElementsByClassName("collapsible");
        for (var i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                    content.style.display = "none";
                } else {
                    content.style.display = "block";
                }
            });
        }
    </script>
</body>
</html>""")
            tool_done()
            print(f"‚úÖ Created basic template at {template_path}")

    env = Environment(loader=FileSystemLoader(template_dir))
    template = env.get_template("scan_template.html")

    # Get GHAS link if available
    ghas_link = None
    if args.repo_url:
        repo = get_repo_from_url(args.repo_url)
        ghas_link = f"https://github.com/{repo}/security/code-scanning"

    rendered = template.render(
        image=image,
        trivy_version=trivy_version,
        grype_version=grype_version,
        total_vulns=total_vulns,
        shared_count=len(shared),
        trivy_count=len(only_trivy),
        grype_count=len(only_grype),
        shared_section=format_and_group_by_type(shared_cves_detailed),
        trivy_section=format_and_group_by_type(trivy_unique_detailed),
        grype_section=format_and_group_by_type(grype_unique_detailed),
        trivy_raw=trivy_raw,
        grype_raw=grype_raw,
        scan_date=timestamp,
        ghas_link=ghas_link
    )

    report_path.write_text(rendered)
    tool_progress("üìÑ", "Generating report")
    tool_done()

    # Try to publish to GitHub Pages
    pages_url = publish_to_github_pages(report_path, image)

    if pages_url:
        print(f"\n‚úÖ Report published to GitHub Pages:")
        print(f"   \033[4;36müîó {pages_url}\033[0m")

        # Ask if user wants to open in browser
        open_html = input("üìÅ Open report in browser? (y/n): ").strip().lower()
        if open_html == "y":
            webbrowser.open(pages_url)
    else:
        print(f"\n‚úÖ Local HTML report saved: {report_path}")

        # Only show local file option if GitHub Pages publishing fails
    open_html = input("üìÅ Open local HTML report in browser? (y/n): ").strip().lower()
    if open_html == "y":
        webbrowser.open(f"file://{report_path.absolute()}")

def explain_exit(msg):
    print(f"‚ö†Ô∏è {msg}")
    sys.exit(1)

def validate_image_exists(image):
    try:
        # Try checking locally first
        local_result = subprocess.run(["docker", "image", "inspect", image], capture_output=True)
        if local_result.returncode == 0:
            return True

        # If not found locally, try to pull the image
        print(f"üîÑ Image not found locally. Attempting to pull {image}...")
        pull_result = subprocess.run(["docker", "pull", image], capture_output=True)
        if pull_result.returncode == 0:
            return True

        return False

    except FileNotFoundError:
        print("‚ùå Docker is not installed or not in your PATH.")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Unexpected error while validating image: {e}")
        return False

def get_repo_from_url(repo_url):
    match = re.match(r"https?://github.com/([^/]+/[^/]+)(?:\\.git)?", repo_url)
    if match:
        return match.group(1)
    print("‚ùå Invalid GitHub repository URL format.")
    sys.exit(1)

def ensure_gh_authenticated():
    try:
        # Check if already authenticated
        subprocess.run(["gh", "auth", "status", "--hostname", "github.com"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # Confirm token can access the user
        result = subprocess.run(["gh", "api", "user"], capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError("Authenticated, but cannot access user profile.")

        # Try a harmless call that requires correct scopes
        probe = subprocess.run(
            ["gh", "api", "repos/octocat/Hello-World/code-scanning/alerts"],
            capture_output=True,
            text=True
        )

        if probe.returncode == 403 or "insufficient" in probe.stderr.lower():
            print("\nüîê GitHub token needs additional permissions.")
            print("   Attempting to refresh token with required scopes...")
            try:
                subprocess.run([
                    "gh", "auth", "refresh",
                    "-h", "github.com",
                    "-s", "security_events,admin:repo_hook"
                ], check=True)
                print("‚úÖ Token refreshed successfully.")
                return
            except subprocess.CalledProcessError:
                print("\n‚ö†Ô∏è Token refresh failed. Please re-authenticate with GitHub.")
                print("   This will open your browser to complete the process.")
                subprocess.run([
                    "gh", "auth", "login",
                    "-h", "github.com",
                    "-s", "security_events,admin:repo_hook",
                    "--web"
                ], check=True)
                return

    except subprocess.CalledProcessError:
        print("\nüîê GitHub authentication required.")
        print("   This will open your browser to complete the process.")
        subprocess.run([
            "gh", "auth", "login",
            "-h", "github.com",
            "-s", "security_events,admin:repo_hook",
            "--web"
        ], check=True)
        return

    except Exception as e:
        print(f"‚ùå Failed to validate GitHub auth: {e}")
        sys.exit(1)

def upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, temp_dir, verbose=False):
    from pathlib import Path
    import json, base64, tempfile, subprocess, gzip
    from datetime import datetime, timezone

    def is_valid_sarif(sarif_path):
        try:
            with open(sarif_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return all(key in data for key in ["version", "runs"])
        except Exception:
            return False

    def check_repo_access(repo):
        try:
            # Check if we can access the repository
            result = subprocess.run(
                ["gh", "api", f"repos/{repo}"],
                capture_output=True,
                text=True
            )
            if result.returncode != 0:
                print(f"‚ö†Ô∏è Cannot access repository {repo}. Skipping GHAS upload.")
                return False
            return True
        except subprocess.CalledProcessError:
            print(f"‚ö†Ô∏è Failed to check repository access. Skipping GHAS upload.")
            return False

    def upload_sarif(sarif_path, tool_name):
        if not sarif_path.exists() or not is_valid_sarif(sarif_path):
            print(f"‚ö†Ô∏è Skipping {tool_name} SARIF upload: file not found or invalid.")
            return False

        def run_git(cmd):
            try:
                return subprocess.check_output(cmd, cwd=temp_dir, text=True).strip()
            except subprocess.CalledProcessError:
                return None

        # Check if we're in a git repository
        if not Path(temp_dir) / ".git" / "config":
            print(f"‚ö†Ô∏è Not in a git repository. Skipping {tool_name} SARIF upload.")
            return False

        commit_sha = run_git(["git", "rev-parse", "HEAD"]) or "0000000000000000000000000000000000000000"
        ref = run_git(["git", "symbolic-ref", "-q", "HEAD"]) or "refs/heads/main"

        if commit_sha.startswith("fatal") or len(commit_sha) != 40:
            print(f"‚ö†Ô∏è Unable to retrieve a valid commit SHA from repo. Skipping {tool_name} SARIF upload.")
            return False

        if not ref.startswith("refs/"):
            print(f"‚ö†Ô∏è Invalid or missing Git ref. Skipping {tool_name} SARIF upload.")
            return False

        with open(sarif_path, "rb") as f_in:
            sarif_bytes = f_in.read()

        with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=".sarif.gz") as gz_file:
            with gzip.GzipFile(fileobj=gz_file, mode="wb") as gzip_out:
                gzip_out.write(sarif_bytes)
            gzipped_path = gz_file.name

        with open(gzipped_path, "rb") as gz_file:
            encoded_sarif = base64.b64encode(gz_file.read()).decode("utf-8")

        payload = {
            "commit_sha": commit_sha,
            "ref": ref,
            "sarif": encoded_sarif,
            "tool_name": tool_name,
            "checkout_uri": ".",
            "started_at": datetime.now(timezone.utc).isoformat()
        }

        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".json") as tmp:
            json.dump(payload, tmp)
            tmp_path = tmp.name

        try:
            result = subprocess.run([
                "gh", "api", f"repos/{repo}/code-scanning/sarifs",
                "--method", "POST",
                "--input", tmp_path
            ], capture_output=True, text=True)

            if result.returncode != 0:
                error_msg = result.stderr.strip()
                if "insufficient_scope" in error_msg.lower() or "not authorized" in error_msg.lower():
                    print(f"\n‚ö†Ô∏è GitHub token lacks required permissions for SARIF uploads.")
                    print("   Required scopes: security_events, admin:repo_hook")
                    print("\n   To fix this, run:")
                    print("   gh auth refresh -h github.com -s security_events,admin:repo_hook")
                    print("\n   Or visit: https://github.com/settings/tokens")
                    print("   to create a new token with the required scopes.")
                    return "insufficient_scope"
                elif "not found" in error_msg.lower():
                    print(f"‚ö†Ô∏è Repository {repo} not found or not accessible.")
                else:
                    print(f"‚ö†Ô∏è Failed to upload {tool_name} SARIF: {error_msg}")
                return False
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ö†Ô∏è Failed to upload {tool_name} SARIF: {e.stderr.strip() if e.stderr else str(e)}")
            return False

    # First check if we can access the repository
    if not check_repo_access(repo):
        return

    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)

    grype_sarif_path = scan_dir / "ghas_upload_grype.sarif"
    trivy_sarif_path = scan_dir / "ghas_upload_trivy.sarif"
    diff_sarif_path = scan_dir / "ghas_diff.sarif"

    print("\nüì¶ Generating SARIF files for GHAS upload...")

    uploads = []
    permission_error = False

    try:
        tool_progress("üîç", "Generating SARIF for Grype")
        subprocess.run(["grype", image, "-o", "sarif", "--file", str(grype_sarif_path)], check=True,
                       stdout=None if verbose else subprocess.DEVNULL,
                       stderr=None if verbose else subprocess.DEVNULL)
        result = upload_sarif(grype_sarif_path, "grype")
        if result == "insufficient_scope":
            permission_error = True
            print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
            return
        elif result:
            uploads.append("grype")
        tool_done()
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate SARIF with grype: {e}")
        tool_done()

    if not permission_error:
        try:
            tool_progress("üîç", "Generating SARIF for Trivy")
            subprocess.run(["trivy", "image", "-f", "sarif", "-o", str(trivy_sarif_path), image], check=True,
                          stdout=None if verbose else subprocess.DEVNULL,
                          stderr=None if verbose else subprocess.DEVNULL)
            result = upload_sarif(trivy_sarif_path, "trivy")
            if result == "insufficient_scope":
                permission_error = True
                print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
                return
            elif result:
                uploads.append("trivy")
            tool_done()
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to generate SARIF with trivy: {e}")
            tool_done()

    if not permission_error and diff_sarif_path.exists():
        tool_progress("üîç", "Generating diff SARIF")
        result = upload_sarif(diff_sarif_path, "scancompare")
        if result == "insufficient_scope":
            permission_error = True
            print("‚ÑπÔ∏è Stopping SARIF generation due to insufficient GitHub permissions.")
            return
        elif result:
            uploads.append("diff (trivy vs grype)")
        tool_done()

    if uploads:
        print(f"\n‚úÖ Uploaded SARIF to GHAS: {', '.join(uploads)}")
        gh_link = f"https://github.com/{repo}/security/code-scanning"
        print(f"\033[4;36müîó View code scanning results on GHAS: {gh_link}\033[0m\n")
    elif not permission_error:
        print("\n‚ö†Ô∏è No SARIF files were uploaded to GHAS.")

def main():
    global args  # Use the global args variable

    if args.uninstall:
        uninstall_scancompare()
        return

    if args.version:
        print(f"üì¶ scancompare version {get_current_version()}")
        return

    if args.update:
        check_latest_version()
        return
    else:
        check_latest_version(only_check=True)

    if not args.image and not args.repo_url:
        print("Usage: scancompare <image> [--repo-url <repo>, --keep-data, --verbose, --version, --update, --uninstall]")
        print("\nFlags:")
        print("  --repo-url <url>   Build and scan a Docker image from a GitHub repo containing a Dockerfile")
        print("  --keep-data        Keep all output and temporary data (HTML, SARIF, JSON, image, cloned repo)")
        print("  --verbose          Show detailed output (git clone, docker build, scan logs)")
        print("  --version          Show scancompare version")
        print("  --update           Check and apply latest scancompare update")
        print("  --uninstall        Uninstall scancompare CLI")
        sys.exit(1)

    if args.image and not validate_image_exists(args.image):
        explain_exit(f"Docker image '{args.image}' could not be found locally or remotely.")

    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)
    summary_log = scan_dir / "scan_summary.log"
    summary_lines = ["\n" + "="*40, f"Scan started: {datetime.now().isoformat()}"]

    temp_dir = None
    image_tag = None
    scancompare_version = get_current_version()
    summary_lines.append(f"üî¢ scancompare version: {scancompare_version}")

    try:
        if args.repo_url:
            temp_dir = tempfile.mkdtemp()
            print(f"üìÅ Created temporary directory: {temp_dir}")

            tool_progress("üì•", f"Cloning {args.repo_url}")
            git_clone_cmd = ["git", "clone", args.repo_url, temp_dir]
            if args.verbose:
                subprocess.run(git_clone_cmd, check=True)
            else:
                subprocess.run(git_clone_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            tool_done()

            dockerfile_paths = []
            for root, _, files in os.walk(temp_dir):
                for file in files:
                    if file.lower().startswith("dockerfile"):
                        dockerfile_paths.append(os.path.join(root, file))

            if not dockerfile_paths:
                print("‚ùå No Dockerfile found in the provided repository.")
                if not args.keep_data:
                    shutil.rmtree(temp_dir)
                sys.exit(1)

            repo_name = args.repo_url.split('/')[-1].replace('.git', '')
            timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

            built_images = []
            for dockerfile_path in dockerfile_paths:
                try:
                    dockerfile_dir = os.path.dirname(dockerfile_path)
                    dir_name = os.path.basename(dockerfile_dir)
                    image_tag = f"{repo_name}-{dir_name}-{timestamp}"

                    tool_progress("üê≥", f"Building Docker image '{image_tag}'")
                    docker_build_cmd = ["docker", "build", "-t", image_tag, dockerfile_dir]
                    if args.verbose:
                        subprocess.run(docker_build_cmd, check=True)
                    else:
                        subprocess.run(docker_build_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                    tool_done()

                    built_images.append(image_tag)
                    print(f"üõ°Ô∏è Scanning image: {image_tag}")
                    summary_lines.append(f"üõ°Ô∏è Scanning built image: {image_tag}")

                    trivy_path = scan_dir / f"original_trivy_{dir_name}.json"
                    grype_path = scan_dir / f"original_grype_{dir_name}.json"

                    tool_progress("üîπ", "Running Trivy scan")
                    if not run_scan("trivy", image_tag, trivy_path, verbose=args.verbose):
                        print(f"‚ö†Ô∏è Trivy scan failed for {image_tag}")
                        continue
                    tool_done()

                    tool_progress("üîπ", "Running Grype scan")
                    if not run_scan("grype", image_tag, grype_path, verbose=args.verbose):
                        print(f"‚ö†Ô∏è Grype scan failed for {image_tag}")
                        continue
                    tool_done()

                    trivy_data = extract_cves_with_severity(trivy_path, verbose=args.verbose)
                    grype_data = extract_cves_with_severity(grype_path, verbose=args.verbose)

                    shared, only_trivy, only_grype = display_summary(trivy_data, grype_data)

                    print_cves_by_severity(f"Unique to Grype ({image_tag})", {c: grype_data[c] for c in only_grype})
                    print_cves_by_severity(f"Unique to Trivy ({image_tag})", {c: trivy_data[c] for c in only_trivy})
                    print_cves_by_severity(f"Shared CVEs ({image_tag})", {c: trivy_data.get(c, grype_data.get(c, 'Unknown')) for c in shared})

                    generate_html_report(image_tag, trivy_data, grype_data, shared, only_trivy, only_grype,
                                         get_version("trivy"), get_version("grype"))

                    # SARIF upload logic
                    if args.repo_url:
                        repo = get_repo_from_url(args.repo_url)
                        ensure_gh_authenticated()
                        upload_to_ghas(image_tag, trivy_path, grype_path, shared, only_trivy, only_grype, repo, temp_dir, verbose=args.verbose)

                except subprocess.CalledProcessError as e:
                    print(f"‚ö†Ô∏è Error processing {dockerfile_path}: {e}")
                    continue

            if not built_images:
                print("‚ùå No images were successfully built and scanned.")
                if not args.keep_data:
                    shutil.rmtree(temp_dir)
                sys.exit(1)

        else:
            image = args.image
            print(f"üõ°Ô∏è Scanning image: {image}")
            summary_lines.append(f"üõ°Ô∏è Scanning image: {image}")

            trivy_path = scan_dir / "original_trivy.json"
            grype_path = scan_dir / "original_grype.json"

            tool_progress("üîπ", "Running Trivy scan")
            if not run_scan("trivy", image, trivy_path, verbose=args.verbose):
                explain_exit("Trivy scan failed or image not found.")
            tool_done()

            tool_progress("üîπ", "Running Grype scan")
            if not run_scan("grype", image, grype_path, verbose=args.verbose):
                explain_exit("Grype scan failed or image not found.")
            tool_done()

            trivy_data = extract_cves_with_severity(trivy_path, verbose=args.verbose)
            grype_data = extract_cves_with_severity(grype_path, verbose=args.verbose)

            shared, only_trivy, only_grype = display_summary(trivy_data, grype_data)

            print_cves_by_severity("Unique to Grype", {c: grype_data[c] for c in only_grype})
            print_cves_by_severity("Unique to Trivy", {c: trivy_data[c] for c in only_trivy})
            print_cves_by_severity("Shared CVEs", {c: trivy_data.get(c, grype_data.get(c, "Unknown")) for c in shared})

            generate_html_report(image, trivy_data, grype_data, shared, only_trivy, only_grype,
                                 get_version("trivy"), get_version("grype"))

            # SARIF upload for single image if --repo-url is supplied
            if args.repo_url:
                repo = get_repo_from_url(args.repo_url)
                ensure_gh_authenticated()
                upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, ".", verbose=args.verbose)

    finally:
        if temp_dir and not args.keep_data:
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to clean up temporary directory: {e}")

    summary_lines.extend([f"Scan completed: {datetime.now().isoformat()}", "="*40 + "\n"])
    summary_log.write_text("\n".join(summary_lines))

if __name__ == "__main__":
    main()

