#!/usr/bin/env python3
# scancompare version 0.0.2 

import os
import sys
import json
import subprocess
from datetime import datetime, timezone
from urllib.request import Request, urlopen
from pathlib import Path
import webbrowser
import shutil
import tempfile
import html
import base64
import argparse
import re
import time
import random
import uuid
from io import BytesIO  
from jinja2 import Environment, FileSystemLoader

SCRIPT_NAME = "scancompare"
SCRIPT_URL = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scancompare"
VERSION = "0.0.2"

args = None  # global placeholder
cloned_repo_dir = None # global placeholder


def get_current_version():
    return VERSION

def check_latest_version(only_check=False):
    already_updated = os.environ.get("SCANCOMPARE_UPDATED") == "1"
    if already_updated and not only_check:
        return

    if not already_updated:
        print("üîé Checking for updates...")

    try:
        api_url = "https://api.github.com/repos/drewtwitchell/scancompare/contents/scancompare"
        req = Request(api_url, headers={"User-Agent": "scancompare-updater"})
        with urlopen(req) as response:
            metadata = json.load(response)

        if "content" not in metadata or "encoding" not in metadata or metadata["encoding"] != "base64":
            raise ValueError("Could not decode GitHub API content")

        new_code = base64.b64decode(metadata["content"]).decode("utf-8")
        latest_match = re.search(r'VERSION\s*=\s*[\'"]([^\'"]+)[\'"]', new_code)
        if not latest_match:
            raise ValueError("Could not determine latest version from script.")

        latest = latest_match.group(1)
        current = get_current_version()

        if latest == current:
            if only_check:
                print(f"üì¶ scancompare version {current}")
                sys.exit(0)
            else:
                print(f"üì¶ Already using latest version ({current})")
                return

        print(f"üîÑ New version detected. Updating scancompare script...")
        update_script(new_code)
        print(f"‚úÖ scancompare updated to version {latest}")
        print("‚ôªÔ∏è Restarting with updated version...")

        os.execve(
            sys.executable,
            [sys.executable, str(get_script_path())] + sys.argv[1:],
            {**os.environ, "SCANCOMPARE_UPDATED": "1"}
        )

    except Exception as e:
        print(f"‚ö†Ô∏è Auto-update check failed: {e}")
        if only_check:
            print(f"üì¶ scancompare version {get_current_version()}")
            sys.exit(0)

def update_script(new_code):
    try:
        script_path = Path(get_script_path())
        install_dir = script_path.parent
        template_path = install_dir / "scan_template.html"

        if not os.access(script_path, os.W_OK):
            raise PermissionError(f"Cannot write to {script_path}. Try using sudo or adjusting permissions.")

        # Write new script to temporary file
        with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tmp:
            tmp.write(new_code)
            temp_path = tmp.name

        os.replace(temp_path, script_path)

        # Ensure template is present
        if not template_path.exists():
            print("üìÑ scan_template.html missing ‚Äî restoring latest version...")
            template_url = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html"
            try:
                import urllib.request
                urllib.request.urlretrieve(template_url, template_path)
                print(f"‚úÖ scan_template.html downloaded to {template_path}")
            except Exception as e:
                print(f"‚ùå Failed to download template: {e}")

        # Ensure jinja2 is installed after update
        try:
            import jinja2  # noqa: F401
        except ImportError:
            print("üì¶ Installing required Python module: jinja2")
            subprocess.run([sys.executable, "-m", "pip", "install", "--user", "jinja2"], check=True)

    except Exception as e:
        print(f"‚ùå Critical: failed to update script: {e}")
        sys.exit(1)

def get_script_path():
    try:
        return os.path.realpath(__file__)
    except NameError:
        return sys.argv[0]
    
def uninstall_scancompare():
    print("üßπ Uninstalling scancompare...")
    paths_removed = False

    lib_dir = Path.home() / ".local" / "lib" / "scancompare"
    script_path = lib_dir / "scancompare"
    wrapper_path = Path.home() / ".local" / "bin" / "scancompare"
    venv_path = lib_dir / "venv"

    for path in [script_path, wrapper_path]:
        if path.exists():
            path.unlink()
            print(f"üóëÔ∏è Removed {path}")
            paths_removed = True

    if venv_path.exists() and venv_path.is_dir():
        shutil.rmtree(venv_path)
        print(f"üßπ Removed Python virtual environment: {venv_path}")

    reports_dir = Path("scan_reports")
    if reports_dir.exists():
        shutil.rmtree(reports_dir)
        print("üóëÔ∏è Removed scan_reports directory")

    profile_files = [".zshrc", ".bashrc", ".profile", ".bash_profile", ".zprofile"]
    for profile in profile_files:
        full_path = Path.home() / profile
        if full_path.exists():
            content = full_path.read_text()
            new_content = "\n".join(
                line for line in content.splitlines()
                if 'export PATH="$HOME/.local/bin:$PATH"' not in line and 'source "$HOME/.config/scancompare/env.shexport"' not in line
            )
            full_path.write_text(new_content)
            print(f"üßΩ Cleaned up shell config in {full_path}")

    if paths_removed:
        print("‚úÖ scancompare successfully uninstalled.")
    else:
        print("‚ÑπÔ∏è scancompare was not found or already uninstalled.")
    sys.exit(0)

def handle_cli_args():
    parser = argparse.ArgumentParser(
        description="Scan and compare Docker image vulnerabilities using Trivy and Grype."
    )
    parser.add_argument("image", nargs="?", help="Docker image to scan")
    parser.add_argument("--repo-url", help="GitHub repo URL containing Dockerfile for image build")
    parser.add_argument("--keep-data", action="store_true", help="Keep Docker image, cloned repo, HTML, SARIF, and JSON results")
    parser.add_argument("--version", action="store_true", help="Show scancompare version")
    parser.add_argument("--update", action="store_true", help="Check and apply latest scancompare update")
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    parser.add_argument("--uninstall", action="store_true", help="Uninstall scancompare CLI")
    return parser.parse_args()

def get_version(tool):
    try:
        result = subprocess.run([tool, "version"], capture_output=True, text=True)

        for line in result.stdout.splitlines():
            if tool.lower() == "grype":
                if line.strip().startswith("Version:"):
                    return line.split("Version:")[-1].strip()
            else:
                if tool.lower() in line.lower() or "Version" in line:
                    return line.strip()
    except FileNotFoundError:
        print(f"‚ö†Ô∏è {tool} is not installed or not in PATH.")
        return "not installed"
    except Exception as e:
        print(f"‚ö†Ô∏è Error getting version for {tool}: {e}")
        return "unknown"

    return "unknown"

def run_scan(tool, image, output_path, verbose=False):
    scan_command = {
        "trivy": ["trivy", "image", "--format", "json", "-o", str(output_path), image],
        "grype": ["grype", image, "-o", "json", "--file", str(output_path)],
    }.get(tool)

    if not scan_command:
        print(f"‚ùå Unknown scanner: {tool}")
        return False

    try:
        if verbose:
            subprocess.run(scan_command, check=True)
        else:
            subprocess.run(scan_command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        return True
    except subprocess.CalledProcessError:
        return False

def extract_cves_with_severity(file_path):
    try:
        with open(file_path) as f:
            data = json.load(f)

        severity_map = {}
        if "Results" in data:  # Trivy format
            for result in data.get("Results", []):
                for vuln in result.get("Vulnerabilities", []):
                    cve = vuln.get("VulnerabilityID")
                    severity = vuln.get("Severity", "UNKNOWN").capitalize()
                    if cve:
                        severity_map[cve] = severity
        elif "matches" in data:  # Grype format
            for match in data.get("matches", []):
                vuln = match.get("vulnerability", {})
                cve = vuln.get("id")
                severity = vuln.get("severity", "UNKNOWN").capitalize()
                if cve:
                    severity_map[cve] = severity

        return severity_map
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to parse {file_path}: {e}")
        return {}


def group_by_severity(cve_map):
    grouped = {}
    for cve_id, meta in cve_map.items():
        if isinstance(meta, dict):
            severity = meta.get("severity", "Unknown")
        else:
            severity = meta or "Unknown"
        grouped.setdefault(severity, []).append(cve_id)
    return grouped

def print_cves_by_severity(title, cve_map):
    print(f"üî∏ {title}")
    grouped = group_by_severity(cve_map)
    for severity in sorted(grouped.keys(), key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize()) if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"] else 99):
        print(f"  {severity} ({len(grouped[severity])}):")
        for cve in sorted(grouped[severity]):
            print(f"    - {cve}")

def display_summary(trivy_cves, grype_cves):
    shared = set(trivy_cves) & set(grype_cves)
    only_trivy = set(trivy_cves) - set(grype_cves)
    only_grype = set(grype_cves) - set(trivy_cves)

    print("\nüìä CLI Summary Report\n")
    print("Tool       | Total | Only in Tool | Shared")
    print("-----------|-------|---------------|--------")
    print(f"Grype      | {len(grype_cves)}   | {len(only_grype)}             | {len(shared)}")
    print(f"Trivy      | {len(trivy_cves)}   | {len(only_trivy)}            | {len(shared)}")

    return shared, only_trivy, only_grype

def generate_html_report(image, trivy_cves, grype_cves, shared, only_trivy, only_grype, trivy_version, grype_version):
    def format_cves(cve_map):
        grouped = group_by_severity(cve_map)
        sections = ""
        severity_colors = {
            "Critical": "#d32f2f",
            "High": "#f57c00",
            "Medium": "#fbc02d",
            "Low": "#388e3c",
            "Unknown": "#757575"
        }
        for severity in sorted(
            grouped.keys(),
            key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize())
            if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"]
            else 99,
        ):
            color = severity_colors.get(severity.capitalize(), "#444")
            cves = grouped[severity]
            sections += f"<button class='collapsible severity-block' data-severity='{severity.capitalize()}' style='color:{color}'>{severity} ({len(cves)})</button><div class='content'><ul>"
            for cve in sorted(cves):
                cve_meta = cve_map.get(cve, {})
                if not isinstance(cve_meta, dict):
                    cve_meta = {}
                cve_link = f"https://nvd.nist.gov/vuln/detail/{cve}"
                cvss_score = cve_meta.get("cvss", "N/A")
                sections += f'<li><a href="{cve_link}" target="_blank">{cve}</a> ‚Äî CVSS: {cvss_score}</li>'
            sections += "</ul></div>"
        return sections or "<p>No CVEs found.</p>"

    def format_and_group_by_type(cve_map):
        os_related = {}
        lang_related = {}
        unknown = {}
        for cve, meta in cve_map.items():
            if not isinstance(meta, dict):
                unknown[cve] = meta
                continue
            pkg_type = meta.get("pkg_type") or meta.get("type") or "unknown"
            if pkg_type.lower() in ["os", "debian", "alpine", "rpm", "apk"]:
                os_related[cve] = meta
            elif pkg_type.lower() in ["python", "java", "nodejs", "golang"]:
                lang_related[cve] = meta
            else:
                unknown[cve] = meta

        result = ""
        if os_related:
            result += "<h3>üßπ OS Package CVEs</h3>" + format_cves(os_related)
        if lang_related:
            result += "<h3>üì¶ Language Package CVEs</h3>" + format_cves(lang_related)
        if unknown:
            result += (
                "<h3>‚ùì Unclassified CVEs</h3>"
                "<p><em>These CVEs couldn‚Äôt be categorized as OS or language packages. This often includes:</em></p>"
                "<ul><li>Uncommon sources or tools</li><li>Temporary CVE IDs (like TEMP-1234)</li><li>Missing or malformed metadata</li></ul>"
                + format_cves(unknown)
            )
        return result

    timestamp = datetime.now().strftime("%Y-%m-%d")
    report_name = f"scan_report_{image.replace(':', '_')}_{timestamp}.html"
    report_path = Path("scan_reports") / report_name

    trivy_raw = html.escape(json.dumps(json.load(open("scan_reports/original_trivy.json")), indent=2))
    grype_raw = html.escape(json.dumps(json.load(open("scan_reports/original_grype.json")), indent=2))

    shared_cves_detailed = {cve: trivy_cves.get(cve) if isinstance(trivy_cves.get(cve), dict) else grype_cves.get(cve) or {"type": "unknown"} for cve in shared}
    trivy_unique_detailed = {cve: trivy_cves.get(cve) or {"type": "unknown"} for cve in only_trivy}
    grype_unique_detailed = {cve: grype_cves.get(cve) or {"type": "unknown"} for cve in only_grype}

    total_vulns = len(trivy_cves) + len(grype_cves) - len(shared)

    template_dir = Path(__file__).parent
    template_path = template_dir / "scan_template.html"

    if not template_path.exists():
        print("üìÑ scan_template.html missing ‚Äî restoring latest version...")
        try:
            urllib.request.urlretrieve(
                "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html",
                template_path
            )
            print(f"‚úÖ scan_template.html downloaded to {template_path}")
        except Exception as e:
            print(f"‚ùå Failed to download template: {e}")

    env = Environment(loader=FileSystemLoader(template_dir))
    template = env.get_template("scan_template.html")

    rendered = template.render(
        image=image,
        trivy_version=trivy_version,
        grype_version=grype_version,
        total_vulns=total_vulns,
        shared_count=len(shared),
        trivy_count=len(only_trivy),
        grype_count=len(only_grype),
        shared_section=format_and_group_by_type(shared_cves_detailed),
        trivy_section=format_and_group_by_type(trivy_unique_detailed),
        grype_section=format_and_group_by_type(grype_unique_detailed),
        trivy_raw=trivy_raw,
        grype_raw=grype_raw
    )

    report_path.write_text(rendered)
    print(f"‚úÖ Local HTML report saved: {report_path}")

    open_html = input("üìÅ Open local HTML report in browser? (y/n): ").strip().lower()
    if open_html == "y":
        webbrowser.open(f"file://{report_path.absolute()}")

def explain_exit(msg):
    print(f"‚ö†Ô∏è {msg}")
    sys.exit(1)

def validate_image_exists(image):
    try:
        # Try checking locally first
        local_result = subprocess.run(["docker", "image", "inspect", image], capture_output=True)
        if local_result.returncode == 0:
            return True

        # If not found locally, check remote manifest
        remote_result = subprocess.run(["docker", "manifest", "inspect", image], capture_output=True)
        if remote_result.returncode == 0:
            return True

        return False

    except FileNotFoundError:
        print("‚ùå Docker is not installed or not in your PATH.")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Unexpected error while validating image: {e}")
        return False

def get_repo_from_url(repo_url):
    match = re.match(r"https?://github.com/([^/]+/[^/]+)(?:\\.git)?", repo_url)
    if match:
        return match.group(1)
    print("‚ùå Invalid GitHub repository URL format.")
    sys.exit(1)

def ensure_gh_installed():
    if shutil.which("gh"):
        return
    print("üîß GitHub CLI (gh) not found. Attempting to install...")
    if shutil.which("brew"):
        subprocess.run(["brew", "install", "gh"], check=True)
    else:
        subprocess.run(["curl", "-fsSL", "https://cli.github.com/install.sh"], check=True)
    print("‚úÖ GitHub CLI installed.")

def ensure_gh_authenticated():
    try:
        # Check if already authenticated
        subprocess.run(["gh", "auth", "status", "--hostname", "github.com"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # Confirm token can access the user
        result = subprocess.run(["gh", "api", "user"], capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError("Authenticated, but cannot access user profile.")

        # Try a harmless call that requires correct scopes
        probe = subprocess.run(
            ["gh", "api", "repos/octocat/Hello-World/code-scanning/alerts"],
            capture_output=True,
            text=True
        )

        if probe.returncode == 403 or "insufficient" in probe.stderr.lower():
            print("‚ö†Ô∏è Token may not have access to required installations or private repositories.")
            print("üîÅ Attempting to refresh token with necessary scopes...")
            subprocess.run([
                "gh", "auth", "refresh",
                "-h", "github.com",
                "-s", "repo,admin:repo_hook,security_events"
            ], check=True)

    except subprocess.CalledProcessError:
        print("üîê GitHub authentication required.")
        subprocess.run(["gh", "auth", "login"], check=True)

    except Exception as e:
        print(f"‚ùå Failed to validate GitHub auth: {e}")
        sys.exit(1)

def upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, verbose=False):
    from pathlib import Path
    import json, base64, tempfile, subprocess, gzip
    from datetime import datetime, timezone

    def is_valid_sarif(sarif_path):
        try:
            with open(sarif_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return all(key in data for key in ["version", "runs"])
        except Exception:
            return False

    def upload_sarif(sarif_path, tool_name):
        if not sarif_path.exists() or not is_valid_sarif(sarif_path):
            print(f"‚ö†Ô∏è Skipping {tool_name} SARIF upload: file not found or invalid.")
            return False

        temp_repo_dir = Path("scan_reports").resolve()

        if hasattr(args, "cloned_repo_path"):
            temp_repo_dir = Path(args.cloned_repo_path)
            print(f"üìÅ Using cloned repo at {temp_repo_dir} for Git commit info.")
        elif args.repo_url:
            repo_name = args.repo_url.split("/")[-1].replace(".git", "")
            guessed_path = Path.home() / ".scancompare_clones" / repo_name
            if guessed_path.exists():
                temp_repo_dir = guessed_path
                print(f"üìÅ Using guessed repo path: {temp_repo_dir}")
            else:
                print(f"‚ö†Ô∏è Could not find local clone directory at {guessed_path}, using fallback for commit info.")

        def run_git(cmd):
            try:
                return subprocess.check_output(cmd, cwd=temp_repo_dir, text=True).strip()
            except subprocess.CalledProcessError:
                return None

        commit_sha = run_git(["git", "rev-parse", "HEAD"]) or "0000000000000000000000000000000000000000"
        ref = run_git(["git", "symbolic-ref", "-q", "HEAD"]) or "refs/heads/main"

        if commit_sha.startswith("fatal") or len(commit_sha) != 40:
            print("‚ö†Ô∏è Unable to retrieve a valid commit SHA from repo, using fallback.")
            commit_sha = "0000000000000000000000000000000000000000"

        if not ref.startswith("refs/"):
            print("‚ö†Ô∏è Invalid or missing Git ref, using default 'refs/heads/main'.")
            ref = "refs/heads/main"

        with open(sarif_path, "rb") as f_in:
            sarif_bytes = f_in.read()

        with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=".sarif.gz") as gz_file:
            with gzip.GzipFile(fileobj=gz_file, mode="wb") as gzip_out:
                gzip_out.write(sarif_bytes)
            gzipped_path = gz_file.name

        with open(gzipped_path, "rb") as gz_file:
            encoded_sarif = base64.b64encode(gz_file.read()).decode("utf-8")

        payload = {
            "commit_sha": commit_sha,
            "ref": ref,
            "sarif": encoded_sarif,
            "tool_name": tool_name,
            "checkout_uri": ".",
            "started_at": datetime.now(timezone.utc).isoformat()
        }

        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".json") as tmp:
            json.dump(payload, tmp)
            tmp_path = tmp.name

        try:
            subprocess.run([
                "gh", "api", f"repos/{repo}/code-scanning/sarifs",
                "--method", "POST",
                "--input", tmp_path
            ], check=True,
               stdout=None if verbose else subprocess.DEVNULL,
               stderr=None if verbose else subprocess.DEVNULL)
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå GH CLI failed for {tool_name}: {e}")
            return False

    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)

    grype_sarif_path = scan_dir / "ghas_upload_grype.sarif"
    trivy_sarif_path = scan_dir / "ghas_upload_trivy.sarif"
    diff_sarif_path = scan_dir / "ghas_diff.sarif"

    print("üì¶ Generating SARIF files for GHAS upload...")

    uploads = []

    try:
        subprocess.run(["grype", image, "-o", "sarif", "--file", str(grype_sarif_path)], check=True,
                       stdout=None if verbose else subprocess.DEVNULL,
                       stderr=None if verbose else subprocess.DEVNULL)
        if upload_sarif(grype_sarif_path, "grype"):
            uploads.append("grype")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate SARIF with grype: {e}")

    try:
        subprocess.run(["trivy", "image", "-f", "sarif", "-o", str(trivy_sarif_path), image], check=True,
                       stdout=None if verbose else subprocess.DEVNULL,
                       stderr=None if verbose else subprocess.DEVNULL)
        if upload_sarif(trivy_sarif_path, "trivy"):
            uploads.append("trivy")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate SARIF with trivy: {e}")

    if diff_sarif_path.exists():
        if upload_sarif(diff_sarif_path, "scancompare"):
            uploads.append("diff")

    if uploads:
        print(f"\n‚úÖ Uploaded SARIF to GHAS: {', '.join(uploads)}")
        gh_link = f"https://github.com/{repo}/security/code-scanning"
        print(f"\033[4;36müîó View code scanning results on GHAS: {gh_link}\033[0m\n")

def main(args):
    if not args.image and not args.repo_url:
        print("Usage: scancompare <image> [--repo-url <repo>, --keep-data, --verbose, --version, --update, --uninstall]")
        print("\nFlags:")
        print("  --repo-url <url>   Build and scan a Docker image from a GitHub repo containing a Dockerfile")
        print("  --keep-data        Keep all output and temporary data (HTML, SARIF, JSON, image, cloned repo)")
        print("  --verbose          Show detailed output (git clone, docker build, scan logs)")
        print("  --version          Show scancompare version")
        print("  --update           Check and apply latest scancompare update")
        print("  --uninstall        Uninstall scancompare CLI")
        sys.exit(1)

    check_latest_version()
    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)
    summary_log = scan_dir / "scan_summary.log"
    summary_lines = [
        "\n" + "="*40,
        f"Scan started: {datetime.now().isoformat()}"
    ]

    temp_dir = None
    image_tag = None
    shown_ghas_hint = False
    scancompare_version = get_current_version()
    summary_lines.append(f"üî¢ scancompare version: {scancompare_version}")

    if args.repo_url:
        temp_dir = tempfile.mkdtemp()
        print(f"üì• Cloning {args.repo_url} into temporary directory...")
        git_clone_cmd = ["git", "clone", args.repo_url, temp_dir]
        if args.verbose:
            subprocess.run(git_clone_cmd, check=True)
        else:
            subprocess.run(git_clone_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        args.cloned_repo_path = temp_dir

        dockerfile_path = None
        for root, _, files in os.walk(temp_dir):
            for file in files:
                if file.lower().startswith("dockerfile"):
                    dockerfile_path = os.path.join(root, file)
                    break
            if dockerfile_path:
                break

        if not dockerfile_path:
            explain_exit("No Dockerfile found in the provided repository.")

        repo_name = args.repo_url.split('/')[-1].replace('.git', '')
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        image_tag = f"{repo_name}-{timestamp}"

        print(f"üê≥ Building Docker image '{image_tag}' from {dockerfile_path}...")
        docker_build_cmd = ["docker", "build", "-t", image_tag, os.path.dirname(dockerfile_path)]
        try:
            if args.verbose:
                subprocess.run(docker_build_cmd, check=True)
            else:
                subprocess.run(docker_build_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            print(f"‚úÖ Docker image '{image_tag}' built successfully.")
            print(f"üõ°Ô∏è Starting vulnerability scans against image: {image_tag}")
            summary_lines.append(f"üõ°Ô∏è Scanning built image: {image_tag}")
        except subprocess.CalledProcessError:
            explain_exit(f"Docker build failed. Please check the Dockerfile and try building manually with 'docker build -t {image_tag} .' to debug.")

        image = image_tag
        repo = get_repo_from_url(args.repo_url)
    else:
        image = args.image
        repo = None
        print(f"\nüõ°Ô∏è Starting vulnerability scans against image: {image}")
        summary_lines.append(f"üõ°Ô∏è Scanning image: {image}")

    if not validate_image_exists(image):
        explain_exit(f"Docker image '{image}' could not be found locally or remotely.")

    trivy_path = scan_dir / "original_trivy.json"
    grype_path = scan_dir / "original_grype.json"

    print(f"\nüîπ Scanning with Trivy...")
    trivy_version = get_version("trivy")
    print(f"   üî¢ Trivy version: {trivy_version}")
    summary_lines.append(f"üî¢ Trivy version: {trivy_version}")
    if not run_scan("trivy", image, trivy_path, verbose=args.verbose):
        explain_exit("Trivy scan failed or image not found.")
    print(f"    ‚úî Trivy scan saved to {trivy_path}")

    print(f"\nüîπ Scanning with Grype...")
    grype_version = get_version("grype")
    print(f"   üî¢ Grype version: {grype_version}")
    summary_lines.append(f"üî¢ Grype version: {grype_version}")
    if not run_scan("grype", image, grype_path, verbose=args.verbose):
        explain_exit("Grype scan failed or image not found.")
    print(f"    ‚úî Grype scan saved to {grype_path}")

    trivy_data = extract_cves_with_severity(trivy_path)
    grype_data = extract_cves_with_severity(grype_path)

    shared, only_trivy, only_grype = display_summary(trivy_data, grype_data)

    print_cves_by_severity("Unique to Grype", {c: grype_data[c] for c in only_grype})
    print_cves_by_severity("Unique to Trivy", {c: trivy_data[c] for c in only_trivy})
    print_cves_by_severity("Shared CVEs", {c: trivy_data.get(c, grype_data.get(c, "Unknown")) for c in shared})

    if repo:
        print(f"üõ†Ô∏è Trivy and Grype scans complete. Preparing upload to GitHub Advanced Security (GHAS)...")
        upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, verbose=args.verbose)
        generate_html_report(image, trivy_data, grype_data, shared, only_trivy, only_grype, trivy_version, grype_version)

        if args.keep_data:
            print("üõë Cleanup skipped due to --keep-data. Keeping Docker image, cloned repo, and scan artifacts.")
            summary_lines.append("üõë Cleanup skipped due to --keep-data. Keeping Docker image, cloned repo, and scan artifacts.")
        else:
            print(f"\nüßº Removing temporary Docker image '{image_tag}'...")
            subprocess.run(["docker", "rmi", "-f", image_tag], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            print("‚úÖ Cleaned up temporary Docker image.")
            summary_lines.append(f"‚úÖ Removed temporary Docker image '{image_tag}'")

            if temp_dir and os.path.isdir(temp_dir):
                print(f"\nüßπ Removing temporary cloned repository from '{temp_dir}' (cloned from {args.repo_url})...")
                shutil.rmtree(temp_dir)
                print("‚úÖ Cleaned up temporary cloned repository.")
                summary_lines.append(f"‚úÖ Removed temporary cloned repository from '{temp_dir}'")
    else:
        print(f"üìÑ Trivy and Grype scans complete. Generating local HTML report...")
        generate_html_report(image, trivy_data, grype_data, shared, only_trivy, only_grype, trivy_version, grype_version)
        if not shown_ghas_hint:
            hint = "üí° To enable GitHub Advanced Security upload, rerun with --repo-url <your-repo-url>"
            print(hint)
            summary_lines.append(hint)
            shown_ghas_hint = True

        if args.keep_data:
            print("üõë Cleanup skipped due to --keep-data. Keeping scan artifacts.")
            summary_lines.append("üõë Cleanup skipped due to --keep-data. Keeping scan artifacts.")
        else:
            print("üßπ Cleaning previous HTML, SARIF, and JSON scan files... Keeping only the latest report.")
            summary_lines.append("üßπ Cleaned up old scan artifacts, kept only latest.")
            for path in scan_dir.glob("*.*"):
                if path.name not in {trivy_path.name, grype_path.name} and not path.name.endswith(".html"):
                    path.unlink()

    if summary_lines:
        with open(summary_log, "a") as log_file:
            for line in summary_lines:
                log_file.write(line + "\n")
        print(f"üìù Summary written to: {summary_log}")

if __name__ == "__main__":
    args = handle_cli_args()
    main(args)
