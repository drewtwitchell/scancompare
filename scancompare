#!/usr/bin/env python3
# scancompare version 0.0.7 

import os
import sys
import json
import subprocess
from datetime import datetime, timezone
from urllib.request import Request, urlopen
from pathlib import Path
import webbrowser
import shutil
import tempfile
import html
import base64
import argparse
import re
import time
import random
import uuid
from io import BytesIO  
from jinja2 import Environment, FileSystemLoader

SCRIPT_NAME = "scancompare"
SCRIPT_URL = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scancompare"
VERSION = "0.0.7"

args = None  # global placeholder
cloned_repo_dir = None # global placeholder

# tool_progress and tool_done functions
def tool_progress(ACTION, TOOL_NAME):
    print(f"{ACTION} {TOOL_NAME}...", end=" ")

def tool_done():
    print(" \033[32m‚úî\033[0m")

def get_current_version():
    return VERSION

def check_latest_version(only_check=False):
    already_updated = os.environ.get("SCANCOMPARE_UPDATED") == "1"
    if already_updated and not only_check:
        return

    try:
        api_url = "https://api.github.com/repos/drewtwitchell/scancompare/contents/scancompare"
        req = Request(api_url, headers={"User-Agent": "scancompare-updater"})
        with urlopen(req) as response:
            metadata = json.load(response)

        if "content" not in metadata or "encoding" not in metadata or metadata["encoding"] != "base64":
            raise ValueError("Could not decode GitHub API content")

        new_code = base64.b64decode(metadata["content"]).decode("utf-8")
        latest_match = re.search(r'VERSION\s*=\s*[\'"]([^\'"]+)[\'"]', new_code)
        if not latest_match:
            raise ValueError("Could not determine latest version from script.")

        latest = latest_match.group(1)
        current = get_current_version()

        if latest == current:
            if only_check:
                print(f"üì¶ scancompare version {current}")
                sys.exit(0)
            else:
                print(f"üì¶ scancompare version {current}")
                return

        print(f"üîÑ Updating scancompare from {current} to {latest}...")
        update_script(new_code)  # This will handle the update, with its own progress steps

        print(f"‚úÖ scancompare updated to version {latest}")
        print("‚ôªÔ∏è Restarting with updated version...")

        os.execve(
            sys.executable,
            [sys.executable, str(get_script_path())] + sys.argv[1:],
            {**os.environ, "SCANCOMPARE_UPDATED": "1"}
        )

    except Exception as e:
        print(f"‚ö†Ô∏è Auto-update check failed: {e}")
        if only_check:
            print(f"üì¶ scancompare version {get_current_version()}")
            sys.exit(0)

def update_script(new_code):
    try:
        import urllib.request

        # Resolve real script path, even if run from a wrapper
        script_path = Path(get_script_path()).resolve()
        install_dir = script_path.parent
        template_path = install_dir / "scan_template.html"

        if not os.access(script_path, os.W_OK):
            raise PermissionError(f"Cannot write to {script_path}. Try using sudo or adjusting permissions.")

        # Write new script to temporary file
        with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tmp:
            tmp.write(new_code)
            temp_path = tmp.name

        os.replace(temp_path, script_path)

        # Always fetch latest scan_template.html
        tool_progress("updates"," Getting latest scan_template.html")
        template_url = "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html"
        try:
            with urllib.request.urlopen(template_url) as response:
                template_html = response.read().decode("utf-8")
                template_path.write_text(template_html)
            tool_done()  # Complete scan_template.html update
        except Exception as e:
            print(f"‚ùå Failed to update scan_template.html: {e}")

        # Ensure jinja2 is installed after update
        try:
            import jinja2  # noqa: F401
        except ImportError:
            tool_progress("jinja2", "Installing jinja2")
            subprocess.run([sys.executable, "-m", "pip", "install", "--user", "jinja2"], check=True)
            tool_done()  # Complete jinja2 installation

    except Exception as e:
        print(f"‚ùå Critical: failed to update script: {e}")
        sys.exit(1)

def get_script_path():
    try:
        return os.path.realpath(__file__)
    except NameError:
        return sys.argv[0]
    
def uninstall_scancompare():
    print("üßπ Uninstalling scancompare...")

    paths_removed = False
    lib_dir = Path.home() / ".local" / "lib" / "scancompare"
    script_path = lib_dir / "scancompare"
    wrapper_path = Path.home() / ".local" / "bin" / "scancompare"
    venv_path = lib_dir / "venv"

    # Removing script and wrapper paths
    for path in [script_path, wrapper_path]:
        if path.exists():
            print(f"üóëÔ∏è  Removing {path}...", end=" ")
            path.unlink()
            print("‚úî")
            paths_removed = True

    # Removing virtual environment
    if venv_path.exists() and venv_path.is_dir():
        print("üßπ Removing virtual environment...", end=" ")
        shutil.rmtree(venv_path)
        print("‚úî")

    # Removing scan_reports directory
    reports_dir = Path("scan_reports")
    if reports_dir.exists():
        print("üóëÔ∏è  Removing scan reports...", end=" ")
        shutil.rmtree(reports_dir)
        print("‚úî")

    # Cleaning up shell config in profile files
    profile_files = [".zshrc", ".bashrc", ".profile", ".bash_profile", ".zprofile"]
    for profile in profile_files:
        full_path = Path.home() / profile
        if full_path.exists():
            print(f"üßΩ Cleaning up {profile}...", end=" ")
            content = full_path.read_text()
            new_content = "\n".join(
                line for line in content.splitlines()
                if 'export PATH="$HOME/.local/bin:$PATH"' not in line and 'source "$HOME/.config/scancompare/env.shexport"' not in line
            )
            full_path.write_text(new_content)
            print("‚úî")

    if paths_removed:
        print("\n‚úÖ scancompare successfully uninstalled.")
    else:
        print("\n‚ÑπÔ∏è  scancompare was not found or already uninstalled.")
    
    sys.exit(0)

def handle_cli_args():
    parser = argparse.ArgumentParser(
        description="Scan and compare Docker image vulnerabilities using Trivy and Grype."
    )
    parser.add_argument("image", nargs="?", help="Docker image to scan")
    parser.add_argument("--repo-url", help="GitHub repo URL containing Dockerfile for image build")
    parser.add_argument("--keep-data", action="store_true", help="Keep Docker image, cloned repo, HTML, SARIF, and JSON results")
    parser.add_argument("--version", action="store_true", help="Show scancompare version")
    parser.add_argument("--update", action="store_true", help="Check and apply latest scancompare update")
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    parser.add_argument("--uninstall", action="store_true", help="Uninstall scancompare CLI")
    return parser.parse_args()

def get_version(tool):
    try:
        result = subprocess.run([tool, "version"], capture_output=True, text=True)

        for line in result.stdout.splitlines():
            if tool.lower() == "grype":
                if line.strip().startswith("Version:"):
                    return line.split("Version:")[-1].strip()
            else:
                if tool.lower() in line.lower() or "Version" in line:
                    return line.strip()
    except FileNotFoundError:
        print(f"‚ö†Ô∏è {tool} is not installed or not in PATH.")
        return "not installed"
    except Exception as e:
        print(f"‚ö†Ô∏è Error getting version for {tool}: {e}")
        return "unknown"

    return "unknown"

def run_scan(tool, image, output_path, verbose=False):
    scan_command = {
        "trivy": ["trivy", "image", "--format", "json", "-o", str(output_path), image],
        "grype": ["grype", image, "-o", "json", "--file", str(output_path)],
    }.get(tool)

    if not scan_command:
        print(f"‚ùå Unknown scanner: {tool}")
        return False

    try:
        if verbose:
            subprocess.run(scan_command, check=True)
        else:
            subprocess.run(scan_command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        return True
    except subprocess.CalledProcessError:
        return False

def extract_cves_with_severity(file_path):
    try:
        with open(file_path) as f:
            data = json.load(f)

        severity_map = {}
        if "Results" in data:  # Trivy format
            for result in data.get("Results", []):
                for vuln in result.get("Vulnerabilities", []):
                    cve = vuln.get("VulnerabilityID")
                    severity = vuln.get("Severity", "UNKNOWN").capitalize()
                    if cve:
                        severity_map[cve] = severity
        elif "matches" in data:  # Grype format
            for match in data.get("matches", []):
                vuln = match.get("vulnerability", {})
                cve = vuln.get("id")
                severity = vuln.get("severity", "UNKNOWN").capitalize()
                if cve:
                    severity_map[cve] = severity

        return severity_map
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to parse {file_path}: {e}")
        return {}

def group_by_severity(cve_map):
    grouped = {}
    for cve_id, meta in cve_map.items():
        if isinstance(meta, dict):
            severity = meta.get("severity", "Unknown")
        else:
            severity = meta or "Unknown"
        grouped.setdefault(severity, []).append(cve_id)
    return grouped

def print_cves_by_severity(title, cve_map):
    print(f"üî∏ {title}")
    grouped = group_by_severity(cve_map)
    for severity in sorted(grouped.keys(), key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize()) if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"] else 99):
        print(f"  {severity} ({len(grouped[severity])}):")
        for cve in sorted(grouped[severity]):
            print(f"    - {cve}")

def display_summary(trivy_cves, grype_cves):
    shared = set(trivy_cves) & set(grype_cves)
    only_trivy = set(trivy_cves) - set(grype_cves)
    only_grype = set(grype_cves) - set(trivy_cves)

    # Indicating the start of the summary report generation
    tool_progress("summary",f"üìä Generating CLI Summary Report...")

    # The actual summary output
    print("\n Tool  | Total | Only in Tool | Shared")
    print("-----------|-------|---------------|--------")
    print(f"Grype  | {len(grype_cves)}   | {len(only_grype)}             | {len(shared)}")
    print(f"Trivy  | {len(trivy_cves)}   | {len(only_trivy)}            | {len(shared)}")

    # Marking the completion of the summary report
    tool_done()  # Mark the summary generation as done

    return shared, only_trivy, only_grype

def generate_html_report(image, trivy_cves, grype_cves, shared, only_trivy, only_grype, trivy_version, grype_version):
    def format_cves(cve_map):
        grouped = group_by_severity(cve_map)
        sections = ""
        severity_colors = {
            "Critical": "#d32f2f",
            "High": "#f57c00",
            "Medium": "#fbc02d",
            "Low": "#388e3c",
            "Unknown": "#757575"
        }
        for severity in sorted(
            grouped.keys(),
            key=lambda s: ["Critical", "High", "Medium", "Low", "Unknown"].index(s.capitalize())
            if s.capitalize() in ["Critical", "High", "Medium", "Low", "Unknown"]
            else 99,
        ):
            color = severity_colors.get(severity.capitalize(), "#444")
            cves = grouped[severity]
            sections += f"<button class='collapsible severity-block' data-severity='{severity.capitalize()}' style='color:{color}'>{severity} ({len(cves)})</button><div class='content'><ul>"
            for cve in sorted(cves):
                cve_meta = cve_map.get(cve, {})
                if not isinstance(cve_meta, dict):
                    cve_meta = {}
                cve_link = f"https://nvd.nist.gov/vuln/detail/{cve}"
                cvss_score = cve_meta.get("cvss", "N/A")
                sections += f'<li><a href="{cve_link}" target="_blank">{cve}</a> ‚Äî CVSS: {cvss_score}</li>'
            sections += "</ul></div>"
        return sections or "<p>No CVEs found.</p>"

    def format_and_group_by_type(cve_map):
        os_related = {}
        lang_related = {}
        unknown = {}
        for cve, meta in cve_map.items():
            if not isinstance(meta, dict):
                unknown[cve] = meta
                continue
            pkg_type = meta.get("pkg_type") or meta.get("type") or "unknown"
            if pkg_type.lower() in ["os", "debian", "alpine", "rpm", "apk"]:
                os_related[cve] = meta
            elif pkg_type.lower() in ["python", "java", "nodejs", "golang"]:
                lang_related[cve] = meta
            else:
                unknown[cve] = meta

        result = ""
        if os_related:
            result += "<h3>üßπ OS Package CVEs</h3>" + format_cves(os_related)
        if lang_related:
            result += "<h3>üì¶ Language Package CVEs</h3>" + format_cves(lang_related)
        if unknown:
            result += (
                "<h3>‚ùì Unclassified CVEs</h3>"
                "<p><em>These CVEs couldn't be categorized as OS or language packages. This often includes:</em></p>"
                "<ul><li>Uncommon sources or tools</li><li>Temporary CVE IDs (like TEMP-1234)</li><li>Missing or malformed metadata</li></ul>"
                + format_cves(unknown)
            )
        return result

    timestamp = datetime.now().strftime("%Y-%m-%d")
    report_name = f"scan_report_{image.replace(':', '_')}_{timestamp}.html"
    report_path = Path("scan_reports") / report_name

    trivy_raw = html.escape(json.dumps(json.load(open("scan_reports/original_trivy.json")), indent=2))
    grype_raw = html.escape(json.dumps(json.load(open("scan_reports/original_grype.json")), indent=2))

    shared_cves_detailed = {cve: trivy_cves.get(cve) if isinstance(trivy_cves.get(cve), dict) else grype_cves.get(cve) or {"type": "unknown"} for cve in shared}
    trivy_unique_detailed = {cve: trivy_cves.get(cve) or {"type": "unknown"} for cve in only_trivy}
    grype_unique_detailed = {cve: grype_cves.get(cve) or {"type": "unknown"} for cve in only_grype}

    total_vulns = len(trivy_cves) + len(grype_cves) - len(shared)

    template_dir = Path(__file__).parent
    template_path = template_dir / "scan_template.html"

    if not template_path.exists():
        tool_progress("updates_html","üìÑ scan_template.html missing ‚Äî restoring latest version...")
        try:
            urllib.request.urlretrieve(
                "https://raw.githubusercontent.com/drewtwitchell/scancompare/main/scan_template.html",
                template_path
            )
            tool_done()
            print(f"‚úÖ scan_template.html downloaded to {template_path}")
        except Exception as e:
            print(f"‚ùå Failed to download template: {e}")
    else:
        tool_done()
    env = Environment(loader=FileSystemLoader(template_dir))
    template = env.get_template("scan_template.html")

    rendered = template.render(
        image=image,
        trivy_version=trivy_version,
        grype_version=grype_version,
        total_vulns=total_vulns,
        shared_count=len(shared),
        trivy_count=len(only_trivy),
        grype_count=len(only_grype),
        shared_section=format_and_group_by_type(shared_cves_detailed),
        trivy_section=format_and_group_by_type(trivy_unique_detailed),
        grype_section=format_and_group_by_type(grype_unique_detailed),
        trivy_raw=trivy_raw,
        grype_raw=grype_raw
    )

    report_path.write_text(rendered)
    print(f"‚úÖ Local HTML report saved: {report_path}")

    open_html = input("üìÅ Open local HTML report in browser? (y/n): ").strip().lower()
    if open_html == "y":
        webbrowser.open(f"file://{report_path.absolute()}")

def explain_exit(msg):
    print(f"‚ö†Ô∏è {msg}")
    sys.exit(1)

def validate_image_exists(image):
    try:
        # Try checking locally first
        local_result = subprocess.run(["docker", "image", "inspect", image], capture_output=True)
        if local_result.returncode == 0:
            return True

        # If not found locally, try to pull the image
        print(f"üîÑ Image not found locally. Attempting to pull {image}...")
        pull_result = subprocess.run(["docker", "pull", image], capture_output=True)
        if pull_result.returncode == 0:
            return True

        return False

    except FileNotFoundError:
        print("‚ùå Docker is not installed or not in your PATH.")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Unexpected error while validating image: {e}")
        return False

def get_repo_from_url(repo_url):
    match = re.match(r"https?://github.com/([^/]+/[^/]+)(?:\\.git)?", repo_url)
    if match:
        return match.group(1)
    print("‚ùå Invalid GitHub repository URL format.")
    sys.exit(1)

def ensure_gh_installed():
    if shutil.which("gh"):
        return
    print("üîß GitHub CLI (gh) not found. Attempting to install...")
    if shutil.which("brew"):
        subprocess.run(["brew", "install", "gh"], check=True)
    else:
        subprocess.run(["curl", "-fsSL", "https://cli.github.com/install.sh"], check=True)
    print("‚úÖ GitHub CLI installed.")

def ensure_gh_authenticated():
    try:
        # Check if already authenticated
        subprocess.run(["gh", "auth", "status", "--hostname", "github.com"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        # Confirm token can access the user
        result = subprocess.run(["gh", "api", "user"], capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError("Authenticated, but cannot access user profile.")

        # Try a harmless call that requires correct scopes
        probe = subprocess.run(
            ["gh", "api", "repos/octocat/Hello-World/code-scanning/alerts"],
            capture_output=True,
            text=True
        )

        if probe.returncode == 403 or "insufficient" in probe.stderr.lower():
            print("‚ö†Ô∏è Token may not have access to required installations or private repositories.")
            print("üîÅ Attempting to refresh token with necessary scopes...")
            subprocess.run([
                "gh", "auth", "refresh",
                "-h", "github.com",
                "-s", "repo,admin:repo_hook,security_events"
            ], check=True)

    except subprocess.CalledProcessError:
        print("üîê GitHub authentication required.")
        subprocess.run(["gh", "auth", "login"], check=True)

    except Exception as e:
        print(f"‚ùå Failed to validate GitHub auth: {e}")
        sys.exit(1)

def upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, verbose=False):
    from pathlib import Path
    import json, base64, tempfile, subprocess, gzip
    from datetime import datetime, timezone

    def is_valid_sarif(sarif_path):
        try:
            with open(sarif_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return all(key in data for key in ["version", "runs"])
        except Exception:
            return False

    def upload_sarif(sarif_path, tool_name):
        if not sarif_path.exists() or not is_valid_sarif(sarif_path):
            print(f"‚ö†Ô∏è Skipping {tool_name} SARIF upload: file not found or invalid.")
            return False

        temp_repo_dir = Path("scan_reports").resolve()

        def run_git(cmd):
            try:
                return subprocess.check_output(cmd, cwd=temp_repo_dir, text=True).strip()
            except subprocess.CalledProcessError:
                return None

        commit_sha = run_git(["git", "rev-parse", "HEAD"]) or "0000000000000000000000000000000000000000"
        ref = run_git(["git", "symbolic-ref", "-q", "HEAD"]) or "refs/heads/main"

        if commit_sha.startswith("fatal") or len(commit_sha) != 40:
            print("‚ö†Ô∏è Unable to retrieve a valid commit SHA from repo, using fallback.")
            commit_sha = "0000000000000000000000000000000000000000"

        if not ref.startswith("refs/"):
            print("‚ö†Ô∏è Invalid or missing Git ref, using default 'refs/heads/main'.")
            ref = "refs/heads/main"

        with open(sarif_path, "rb") as f_in:
            sarif_bytes = f_in.read()

        with tempfile.NamedTemporaryFile(mode="wb", delete=False, suffix=".sarif.gz") as gz_file:
            with gzip.GzipFile(fileobj=gz_file, mode="wb") as gzip_out:
                gzip_out.write(sarif_bytes)
            gzipped_path = gz_file.name

        with open(gzipped_path, "rb") as gz_file:
            encoded_sarif = base64.b64encode(gz_file.read()).decode("utf-8")

        payload = {
            "commit_sha": commit_sha,
            "ref": ref,
            "sarif": encoded_sarif,
            "tool_name": tool_name,
            "checkout_uri": ".",
            "started_at": datetime.now(timezone.utc).isoformat()
        }

        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".json") as tmp:
            json.dump(payload, tmp)
            tmp_path = tmp.name

        try:
            subprocess.run([
                "gh", "api", f"repos/{repo}/code-scanning/sarifs",
                "--method", "POST",
                "--input", tmp_path
            ], check=True,
               stdout=None if verbose else subprocess.DEVNULL,
               stderr=None if verbose else subprocess.DEVNULL)
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå GH CLI failed for {tool_name}: {e}")
            return False

    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)

    grype_sarif_path = scan_dir / "ghas_upload_grype.sarif"
    trivy_sarif_path = scan_dir / "ghas_upload_trivy.sarif"
    diff_sarif_path = scan_dir / "ghas_diff.sarif"

    print("üì¶ Generating SARIF files for GHAS upload...")

    uploads = []

    try:
        tool_progress("grype", "Generating SARIF for grype...")
        subprocess.run(["grype", image, "-o", "sarif", "--file", str(grype_sarif_path)], check=True,
                       stdout=None if verbose else subprocess.DEVNULL,
                       stderr=None if verbose else subprocess.DEVNULL)
        if upload_sarif(grype_sarif_path, "grype"):
            uploads.append("grype")
        tool_done()  # Completed grype SARIF generation
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate SARIF with grype: {e}")

    try:
        tool_progress("trivy", "Generating SARIF for trivy...")
        subprocess.run(["trivy", "image", "-f", "sarif", "-o", str(trivy_sarif_path), image], check=True,
                       stdout=None if verbose else subprocess.DEVNULL,
                       stderr=None if verbose else subprocess.DEVNULL)
        if upload_sarif(trivy_sarif_path, "trivy"):
            uploads.append("trivy")
        tool_done()  # Completed trivy SARIF generation
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to generate SARIF with trivy: {e}")

    if diff_sarif_path.exists():
        tool_progress("scancompare", "Generating diff SARIF (trivy vs grype)...")
        if upload_sarif(diff_sarif_path, "scancompare"):
            uploads.append("diff (trivy vs grype)")
        tool_done()  # Completed diff SARIF generation

    if uploads:
        print(f"\n‚úÖ Uploaded SARIF to GHAS: {', '.join(uploads)}")
        gh_link = f"https://github.com/{repo}/security/code-scanning"
        print(f"\033[4;36müîó View code scanning results on GHAS: {gh_link}\033[0m\n")

def main(args):
    if args.uninstall:
        uninstall_scancompare()

    if args.version:
        print(f"üì¶ scancompare version {get_current_version()}")
        sys.exit(0)

    if args.update:
        check_latest_version(only_check=True)
        sys.exit(0)

    if not args.image and not args.repo_url:
        print("Usage: scancompare <image> [--repo-url <repo>, --keep-data, --verbose, --version, --update, --uninstall]")
        print("\nFlags:")
        print("  --repo-url <url>   Build and scan a Docker image from a GitHub repo containing a Dockerfile")
        print("  --keep-data        Keep all output and temporary data (HTML, SARIF, JSON, image, cloned repo)")
        print("  --verbose          Show detailed output (git clone, docker build, scan logs)")
        print("  --version          Show scancompare version")
        print("  --update           Check and apply latest scancompare update")
        print("  --uninstall        Uninstall scancompare CLI")
        sys.exit(1)

    check_latest_version()
    
    # Validate image exists before starting any scan process
    if args.image and not validate_image_exists(args.image):
        explain_exit(f"Docker image '{args.image}' could not be found locally or remotely.")

    scan_dir = Path("scan_reports")
    scan_dir.mkdir(exist_ok=True)
    summary_log = scan_dir / "scan_summary.log"
    summary_lines = [
        "\n" + "="*40,
        f"Scan started: {datetime.now().isoformat()}"
    ]

    temp_dir = None
    image_tag = None
    shown_ghas_hint = False
    scancompare_version = get_current_version()
    summary_lines.append(f"üî¢ scancompare version: {scancompare_version}")

    if args.repo_url:
        temp_dir = tempfile.mkdtemp()

        # Clone the repository with progress
        tool_progress("cloning",f"üì• Cloning {args.repo_url} into temporary directory...")
        git_clone_cmd = ["git", "clone", args.repo_url, temp_dir]
        if args.verbose:
            subprocess.run(git_clone_cmd, check=True)
        else:
            subprocess.run(git_clone_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        tool_done()  # Mark cloning as done

        args.cloned_repo_path = temp_dir

        dockerfile_path = None
        for root, _, files in os.walk(temp_dir):
            for file in files:
                if file.lower().startswith("dockerfile"):
                    dockerfile_path = os.path.join(root, file)
                    break
            if dockerfile_path:
                break

        if not dockerfile_path:
            print("‚ùå No Dockerfile found in the provided repository. Exiting.")
            sys.exit(1)  # Exit if Dockerfile is not found

        repo_name = args.repo_url.split('/')[-1].replace('.git', '')
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        image_tag = f"{repo_name}-{timestamp}"

        # Docker image build as a sub-process of cloning the repo (Indented)
        tool_progress("docker",f"    üê≥ Building Docker image '{image_tag}' from {dockerfile_path}...")
        docker_build_cmd = ["docker", "build", "-t", image_tag, os.path.dirname(dockerfile_path)]
        try:
            if args.verbose:
                subprocess.run(docker_build_cmd, check=True)
            else:
                subprocess.run(docker_build_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            tool_done()  # Mark Docker build as done
            tool_progress("scans",f"üõ°Ô∏è Starting vulnerability scans against image: {image_tag}")
            summary_lines.append(f"üõ°Ô∏è Scanning built image: {image_tag}")
        except subprocess.CalledProcessError:
            explain_exit(f"Docker build failed. Please check the Dockerfile and try building manually with 'docker build -t {image_tag} .' to debug.")

        image = image_tag
        repo = get_repo_from_url(args.repo_url)
    else:
        image = args.image
        repo = None
        print(f"üõ°Ô∏è Starting vulnerability scans against image: {image}")
        summary_lines.append(f"üõ°Ô∏è Scanning image: {image}")

    trivy_path = scan_dir / "original_trivy.json"
    grype_path = scan_dir / "original_grype.json"

    # Trivy scan as a sub-process of "Starting vulnerability scans"
    tool_progress("scans", "üîπ Scanning with Trivy...")
    trivy_version = get_version("trivy")
    print(f"       üî¢ Trivy version: {trivy_version}")
    summary_lines.append(f"üî¢ Trivy version: {trivy_version}")
    if not run_scan("trivy", image, trivy_path, verbose=args.verbose):
        explain_exit("Trivy scan failed or image not found.")
    print(f"       ‚úî Trivy scan saved to {trivy_path}")
    tool_done()  # Mark Trivy scan as done

    # Grype scan as a sub-process of "Starting vulnerability scans"
    tool_progress("scans", "üîπ Scanning with Grype...")
    grype_version = get_version("grype")
    print(f"       üî¢ Grype version: {grype_version}")
    summary_lines.append(f"üî¢ Grype version: {grype_version}")
    if not run_scan("grype", image, grype_path, verbose=args.verbose):
        explain_exit("Grype scan failed or image not found.")
    print(f"       ‚úî Grype scan saved to {grype_path}")
    tool_done()  # Mark Grype scan as done

    trivy_data = extract_cves_with_severity(trivy_path)
    grype_data = extract_cves_with_severity(grype_path)

    shared, only_trivy, only_grype = display_summary(trivy_data, grype_data)

    print_cves_by_severity("Unique to Grype", {c: grype_data[c] for c in only_grype})
    print_cves_by_severity("Unique to Trivy", {c: trivy_data[c] for c in only_trivy})
    print_cves_by_severity("Shared CVEs", {c: trivy_data.get(c, grype_data.get(c, "Unknown")) for c in shared})

    if repo:
        print(f"üõ†Ô∏è Trivy and Grype scans complete. Preparing upload to GitHub Advanced Security (GHAS)...")
        upload_to_ghas(image, trivy_path, grype_path, shared, only_trivy, only_grype, repo, verbose=args.verbose)

        # Generate HTML report with progress
        tool_progress("generate",f"üìÑ Generating HTML report...")
        generate_html_report(image, trivy_data, grype_data, shared, only_trivy, only_grype, trivy_version, grype_version)
        tool_done()  # Mark HTML report generation as done

        # Cleanup: Show progress with tool_progress
        tool_progress("clean",f"üßπ Cleaning up old scan artifacts...")  # Replacing print with tool_progress
        summary_lines.append("üßπ Cleaned up old scan artifacts.")
        for path in scan_dir.glob("*.*"):
            if path.name not in {trivy_path.name, grype_path.name} and not path.name.endswith(".html"):
                path.unlink()
        tool_done()  # Mark cleanup as done

        if args.keep_data:
            print("üõë Cleanup skipped due to --keep-data. Keeping Docker image, cloned repo, and scan artifacts.")
            summary_lines.append("üõë Cleanup skipped due to --keep-data. Keeping Docker image, cloned repo, and scan artifacts.")
        else:
            # Now, cleaning up Docker image and repo as part of cleanup sub-process
            tool_progress("clean",f"    üßº Removing temporary Docker image '{image_tag}'...")
            subprocess.run(["docker", "rmi", "-f", image_tag], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            tool_done()  # Mark Docker image removal as done

            if temp_dir and os.path.isdir(temp_dir):
                tool_progress("clean",f"    üßπ Removing temporary cloned repository from '{temp_dir}' (cloned from {args.repo_url})...")
                shutil.rmtree(temp_dir)
                tool_done()  # Mark repository removal as done
    else:
        tool_progress("generate",f"üìÑ Generating local HTML report...")  # Show progress for HTML report generation
        generate_html_report(image, trivy_data, grype_data, shared, only_trivy, only_grype, trivy_version, grype_version)
        tool_done()  # Mark HTML report generation as done

        if not shown_ghas_hint:
            hint = "üí° To enable GitHub Advanced Security upload, rerun with --repo-url <your-repo-url>"
            print(hint)
            summary_lines.append(hint)
            shown_ghas_hint = True

        if args.keep_data:
            print("üõë Cleanup skipped due to --keep-data. Keeping scan artifacts.")
            summary_lines.append("üõë Cleanup skipped due to --keep-data. Keeping scan artifacts.")
        else:
            print("üßπ Cleaning previous HTML, SARIF, and JSON scan files... Keeping only the latest report.")
            summary_lines.append("üßπ Cleaned up old scan artifacts, kept only latest.")
            for path in scan_dir.glob("*.*"):
                if path.name not in {trivy_path.name, grype_path.name} and not path.name.endswith(".html"):
                    path.unlink()

    if summary_lines:
        with open(summary_log, "a") as log_file:
            for line in summary_lines:
                log_file.write(line + "\n")
        print(f"üìù Summary written to: {summary_log}")

if __name__ == "__main__":
    args = handle_cli_args()
    main(args)
