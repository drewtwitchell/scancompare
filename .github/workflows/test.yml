name: Scancompare Comprehensive Test Suite

on:
  push:
    branches: [ main, develop, 'release-*' ]
  pull_request:
    branches: [ main, develop, 'release-*' ]

jobs:
  test:
    runs-on: ubuntu-latest
    env:
      # Set this to prevent the script from trying to update itself
      SCANCOMPARE_UPDATED: "1"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.8
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      # Install dependencies directly
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y expect
          pip install jinja2 requests

      # Install Trivy and Grype
      - name: Install Trivy and Grype
        run: |
          # Install Trivy
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin v0.42.0
          trivy --version
          
          # Install Grype
          curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin
          grype --version

      - name: Set up Docker
        run: |
          curl -fsSL https://get.docker.com -o get-docker.sh
          sudo sh get-docker.sh
          sudo systemctl start docker

      # Create necessary directories that the script expects
      - name: Set up directory structure
        run: |
          mkdir -p ~/ScanCompare/scan_reports
          mkdir -p ~/ScanCompare/temp
          mkdir -p ~/ScanCompare/backups
          mkdir -p ~/ScanCompare/temp/gh-pages
          mkdir -p ~/ScanCompare/temp/docker
          
          # Create test results directory
          mkdir -p test_results

      # Create the test results file
      - name: Initialize test report
        run: |
          echo '# Scancompare Automation Flags Test Results' > test_results/report.md
          echo '' >> test_results/report.md
          echo '| Test Case | Status | Details |' >> test_results/report.md
          echo '|-----------|--------|---------|' >> test_results/report.md

      # Verify scancompare has automation flags
      - name: Verify scancompare script
        id: verify
        run: |
          # Make script executable and verify
          chmod +x scancompare
          ls -la scancompare
          
          # Verify flag help display
          if ./scancompare --help | grep -E -- "--auto|--mock-yes|--mock-no"; then
            echo "| Script Help Check | ✅ PASS | Automation flags found in help text |" >> test_results/report.md
            echo "Automation flags found in help text"
          else
            echo "| Script Help Check | ❌ FAIL | Automation flags NOT found in help text |" >> test_results/report.md
            echo "Automation flags NOT found in help text"
            exit 1
          fi

      # Test simple direct python calls to verify arguments work
      - name: Test argument parsing
        id: parser
        run: |
          # Make the test parser script executable
          chmod +x scripts/test_parser.py
          
          # Run the test script
          python scripts/test_parser.py || echo "Argument parser tests failed"
          
          # Append parser test results to main report
          cat test_results/parser_results.md >> test_results/report.md

      # For actual script testing, we'll use expect scripts for interactive prompts
      - name: Run expect scripts for automated testing
        id: expect
        run: |
          echo "Running automated tests..."
          
          # Create test_scripts directory
          mkdir -p test_scripts
          
          # Copy expect scripts from repository to test_scripts
          cp scripts/expect/*.exp test_scripts/
          
          # Make all scripts executable
          chmod +x test_scripts/*.exp
          
          # Run all expect scripts and record results
          for script in test_scripts/*.exp; do
            echo "Running test: $(basename $script)"
            $script || echo "Test execution error: $(basename $script)"
          done
          
          # Add results to main report
          echo "" >> test_results/report.md
          echo "## Interactive Test Results" >> test_results/report.md
          echo "" >> test_results/report.md
          echo "| Test Case | Status | Details |" >> test_results/report.md
          echo "|-----------|--------|---------|" >> test_results/report.md
          
          # Process auto mode test
          if [ -f "test_results/basic_auto_result.txt" ]; then
            result=$(cat test_results/basic_auto_result.txt)
            if [ "$result" == "PASS" ]; then
              echo "| Auto Mode Test | ✅ PASS | Auto mode correctly handled interactive flow |" >> test_results/report.md
            elif [ "$result" == "INCONCLUSIVE" ]; then
              echo "| Auto Mode Test | ⚠️ INCONCLUSIVE | Script completed but auto behavior unclear |" >> test_results/report.md
            else
              echo "| Auto Mode Test | ❌ FAIL | Auto mode failed to handle interactive flow |" >> test_results/report.md
            fi
          else
            echo "| Auto Mode Test | ❌ FAIL | Test did not complete |" >> test_results/report.md
          fi
          
          # Process mock-yes mode test
          if [ -f "test_results/basic_yes_result.txt" ]; then
            result=$(cat test_results/basic_yes_result.txt)
            if [ "$result" == "PASS" ]; then
              echo "| Mock-Yes Mode Test | ✅ PASS | Mock-yes mode correctly handled interactive flow |" >> test_results/report.md
            elif [ "$result" == "INCONCLUSIVE" ]; then
              echo "| Mock-Yes Mode Test | ⚠️ INCONCLUSIVE | Script completed but mock-yes behavior unclear |" >> test_results/report.md
            else
              echo "| Mock-Yes Mode Test | ❌ FAIL | Mock-yes mode failed to handle interactive flow |" >> test_results/report.md
            fi
          else
            echo "| Mock-Yes Mode Test | ❌ FAIL | Test did not complete |" >> test_results/report.md
          fi
          
          # Process mock-no mode test
          if [ -f "test_results/basic_no_result.txt" ]; then
            result=$(cat test_results/basic_no_result.txt)
            if [ "$result" == "PASS" ]; then
              echo "| Mock-No Mode Test | ✅ PASS | Mock-no mode correctly handled interactive flow |" >> test_results/report.md
            elif [ "$result" == "INCONCLUSIVE" ]; then
              echo "| Mock-No Mode Test | ⚠️ INCONCLUSIVE | Script completed but mock-no behavior unclear |" >> test_results/report.md
            else
              echo "| Mock-No Mode Test | ❌ FAIL | Mock-no mode failed to handle interactive flow |" >> test_results/report.md
            fi
          else
            echo "| Mock-No Mode Test | ❌ FAIL | Test did not complete |" >> test_results/report.md
          fi
          
          # Process multiple flags test
          if [ -f "test_results/multiple_flags_result.txt" ]; then
            result=$(cat test_results/multiple_flags_result.txt)
            if [ "$result" == "PASS" ]; then
              echo "| Multiple Flags Test | ✅ PASS | Multiple automation flags correctly rejected |" >> test_results/report.md
            else
              echo "| Multiple Flags Test | ❌ FAIL | Multiple automation flags not properly rejected |" >> test_results/report.md
            fi
          else
            echo "| Multiple Flags Test | ❌ FAIL | Test did not complete |" >> test_results/report.md
          fi
          
      # Test utility commands
      - name: Test utility commands
        id: utilities
        run: |
          echo "## Utility Command Tests" >> test_results/report.md
          echo "" >> test_results/report.md
          echo "| Test Case | Status | Details |" >> test_results/report.md
          echo "|-----------|--------|---------|" >> test_results/report.md
          
          # Test version command
          if python scancompare --version > test_results/version.log 2>&1; then
            echo "| Version Command | ✅ PASS | Successfully displayed version |" >> test_results/report.md
          else
            echo "| Version Command | ❌ FAIL | Failed to display version |" >> test_results/report.md
          fi
          
          # Test version with auto flag
          if python scancompare --version --auto > test_results/version_auto.log 2>&1; then
            echo "| Version + Auto | ✅ PASS | Successfully handled version with auto flag |" >> test_results/report.md
          else
            echo "| Version + Auto | ❌ FAIL | Failed to handle version with auto flag |" >> test_results/report.md
          fi
          
      # Generate a summary of all test results
      - name: Generate test summary
        run: |
          # Count results
          TOTAL_TESTS=$(grep -c "^|" test_results/report.md | awk '{print $1-2}')
          PASSED_TESTS=$(grep -c "✅ PASS" test_results/report.md)
          INCONCLUSIVE_TESTS=$(grep -c "⚠️ INCONCLUSIVE" test_results/report.md)
          FAILED_TESTS=$((TOTAL_TESTS - PASSED_TESTS - INCONCLUSIVE_TESTS))
          
          # Add summary to report
          echo "" >> test_results/report.md
          echo "## Test Summary" >> test_results/report.md
          echo "" >> test_results/report.md
          echo "- Total Tests: $TOTAL_TESTS" >> test_results/report.md
          echo "- Passed: $PASSED_TESTS" >> test_results/report.md
          echo "- Inconclusive: $INCONCLUSIVE_TESTS" >> test_results/report.md
          echo "- Failed: $FAILED_TESTS" >> test_results/report.md
          
          if [ $FAILED_TESTS -gt 0 ]; then
            echo "" >> test_results/report.md
            echo "### Failed Tests" >> test_results/report.md
            echo "" >> test_results/report.md
            grep "❌ FAIL" test_results/report.md >> test_results/report.md
          fi
          
          if [ $INCONCLUSIVE_TESTS -gt 0 ]; then
            echo "" >> test_results/report.md
            echo "### Inconclusive Tests" >> test_results/report.md
            echo "" >> test_results/report.md
            grep "⚠️ INCONCLUSIVE" test_results/report.md >> test_results/report.md
          fi
          
          echo "" >> test_results/report.md
          echo "## Detailed Logs" >> test_results/report.md
          echo "" >> test_results/report.md
          echo "Detailed logs for each test are available in the test_results directory." >> test_results/report.md
          
          # Save success/failure status for final exit
          if [ $FAILED_TESTS -eq 0 ]; then
            echo "result=success" >> $GITHUB_ENV
          else
            echo "result=failure" >> $GITHUB_ENV
          fi

      # Save test results to file that will be saved as an artifact in GitHub Actions
      - name: Save test results
        run: |
          # Create a directory to store test results
          mkdir -p $GITHUB_WORKSPACE/test-artifacts
          
          # Copy all test results to the artifact directory
          cp -r test_results/* $GITHUB_WORKSPACE/test-artifacts/
          
          # Indicate this is the test-artifact directory
          echo "Test artifacts are stored in $GITHUB_WORKSPACE/test-artifacts" > $GITHUB_WORKSPACE/test-artifacts/README.txt
          
          # Make all files readable
          chmod -R a+r $GITHUB_WORKSPACE/test-artifacts
          
          # Indicate where artifacts can be found
          echo "Artifact Directory: $GITHUB_WORKSPACE/test-artifacts" | tee -a $GITHUB_STEP_SUMMARY

      # Output test report to GitHub step summary
      - name: Output test summary
        run: |
          cat test_results/report.md >> $GITHUB_STEP_SUMMARY
          
      # Exit with appropriate status
      - name: Report test status
        run: |
          if [[ "${{ env.result }}" == "success" ]]; then
            echo "All tests passed successfully!"
            exit 0
          else
            echo "Some tests failed. See test report for details."
            cat test_results/report.md
            exit 1
          fi